{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzeNPmLdvyjhg28whv7k9f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/Thesis/blob/main/H_vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfeKjc3gt7mH",
        "outputId": "801e844a-3099-446a-927d-1da020a640ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hvit'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "/bin/bash: line 1: cd: hvit: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/your-username/hvit.git\n",
        "!cd hvit"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6k71oE54uBMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "g3570FimvN9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import monai\n",
        "import torch\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class OASIS_Dataset(Dataset):\n",
        "    def __init__(self, input_dim, data_path, num_steps=1000, is_pair: bool = False, ext=\"pkl\"):\n",
        "        self.paths = glob.glob(os.path.join(data_path, f\"*.{ext}\"))\n",
        "        self.num_steps = num_steps\n",
        "        self.input_dim = input_dim\n",
        "        self.is_pair = is_pair\n",
        "\n",
        "        self.transforms_mask = monai.transforms.Compose([\n",
        "            monai.transforms.Resize(spatial_size=input_dim, mode=\"nearest\")\n",
        "        ])\n",
        "        self.transforms_image = monai.transforms.Compose([\n",
        "            monai.transforms.Resize(spatial_size=input_dim)\n",
        "        ])\n",
        "\n",
        "    def _pkload(self, filename: str) -> tuple:\n",
        "        \"\"\"\n",
        "        Load a pickled file and return its contents.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The path to the pickled file.\n",
        "\n",
        "        Returns:\n",
        "            tuple: The unpickled contents of the file.\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: If the file does not exist.\n",
        "            pickle.UnpicklingError: If there's an error during unpickling.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(filename, 'rb') as file:\n",
        "                return pickle.load(file) #np.ascontiguousarray(pickle.load(file))\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"The file {filename} was not found.\")\n",
        "        except pickle.UnpicklingError:\n",
        "            raise pickle.UnpicklingError(f\"Error unpickling the file {filename}.\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_pair:\n",
        "            src, tgt, src_lbl, tgt_lbl = self._pkload(self.paths[index])\n",
        "        else:\n",
        "            selected_items = random.sample(list(self.paths), 2)\n",
        "            src, src_lbl = self._pkload(selected_items[0])\n",
        "            tgt, tgt_lbl = self._pkload(selected_items[1])\n",
        "\n",
        "        src = torch.from_numpy(src).float().unsqueeze(0)\n",
        "        src_lbl = torch.from_numpy(src_lbl).long().unsqueeze(0)\n",
        "        tgt = torch.from_numpy(tgt).float().unsqueeze(0)\n",
        "        tgt_lbl = torch.from_numpy(tgt_lbl).long().unsqueeze(0)\n",
        "\n",
        "        src = self.transforms_image(src)\n",
        "        tgt = self.transforms_image(tgt)\n",
        "        src_lbl = self.transforms_mask(src_lbl)\n",
        "        tgt_lbl = self.transforms_mask(tgt_lbl)\n",
        "\n",
        "        return src, tgt, src_lbl, tgt_lbl\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_steps if not self.is_pair else len(self.paths)\n",
        "\n",
        "def get_dataloader(data_path, input_dim, batch_size, shuffle: bool = True, is_pair: bool = False):\n",
        "    ds = OASIS_Dataset(input_dim = input_dim, data_path = data_path, is_pair=is_pair)\n",
        "    dataloader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=4)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "Rw2-IS6PvPET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blocks\n"
      ],
      "metadata": {
        "id": "lMT0zfA_ude1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch import Tensor\n",
        "from einops import rearrange\n",
        "from functools import reduce\n",
        "\n",
        "ndims = 3 # H,W,D\n",
        "\n",
        "\n",
        "# these functions are adopted from timm.\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
        "    if keep_prob > 0.0 and scale_by_keep:\n",
        "        random_tensor.div_(keep_prob)\n",
        "    return x * random_tensor\n",
        "\n",
        "\n",
        "class timm_DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
        "        super(timm_DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.scale_by_keep = scale_by_keep\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
        "\n",
        "\n",
        "def _trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    # Values are generated by using a truncated uniform distribution and\n",
        "    # then using the inverse CDF for the normal distribution.\n",
        "    # Get upper and lower cdf values\n",
        "    l = norm_cdf((a - mean) / std)\n",
        "    u = norm_cdf((b - mean) / std)\n",
        "\n",
        "    # Uniformly fill tensor with values from [l, u], then translate to\n",
        "    # [2l-1, 2u-1].\n",
        "    tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "    # Use inverse cdf transform for normal distribution to get truncated\n",
        "    # standard normal\n",
        "    tensor.erfinv_()\n",
        "\n",
        "    # Transform to proper mean, std\n",
        "    tensor.mul_(std * math.sqrt(2.))\n",
        "    tensor.add_(mean)\n",
        "\n",
        "    # Clamp to ensure it's in the proper range\n",
        "    tensor.clamp_(min=a, max=b)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "\n",
        "def timm_trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
        "    normal distribution. The values are effectively drawn from the\n",
        "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
        "    with values outside :math:`[a, b]` redrawn until they are within\n",
        "    the bounds. The method used for generating the random values works\n",
        "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
        "    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are\n",
        "    applied while sampling the normal with mean/std applied, therefore a, b args\n",
        "    should be adjusted to match the range of mean, std args.\n",
        "    Args:\n",
        "        tensor: an n-dimensional `torch.Tensor`\n",
        "        mean: the mean of the normal distribution\n",
        "        std: the standard deviation of the normal distribution\n",
        "        a: the minimum cutoff value\n",
        "        b: the maximum cutoff value\n",
        "    Examples:\n",
        "        >>> w = torch.empty(3, 5)\n",
        "        >>> nn.init.trunc_normal_(w)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        return _trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "\n",
        "\n",
        "class Conv3dReLU(nn.Sequential):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            padding=0,\n",
        "            stride=1,\n",
        "            use_batchnorm=True,\n",
        "    ):\n",
        "        conv = nn.Conv3d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            bias=False,\n",
        "        )\n",
        "        relu = nn.LeakyReLU(inplace=True)\n",
        "        if not use_batchnorm:\n",
        "            nm = nn.InstanceNorm3d(out_channels)\n",
        "        else:\n",
        "            nm = nn.BatchNorm3d(out_channels)\n",
        "        super(Conv3dReLU, self).__init__(conv, nm, relu)\n",
        "\n",
        "\n",
        "def get_norm(name, **kwargs):\n",
        "    if name.lower() == 'BatchNorm2d'.lower():\n",
        "        BatchNorm = getattr(nn, 'BatchNorm%dd' % ndims)\n",
        "        return BatchNorm(**kwargs)\n",
        "    elif name.lower() == 'instance':\n",
        "        InstanceNorm = getattr(nn, 'InstanceNorm%dd' % ndims)\n",
        "        return InstanceNorm(**kwargs)\n",
        "    elif name.lower() == 'None'.lower():\n",
        "        return nn.Identity()\n",
        "    else:\n",
        "        return NotImplementedError\n",
        "\n",
        "\n",
        "def get_activation(name, **kwargs):\n",
        "    if name.lower() == 'ReLU'.lower():\n",
        "        return nn.ReLU()\n",
        "    elif name.lower() == 'GELU'.lower():\n",
        "        return nn.GELU()\n",
        "    elif name.lower() == 'None'.lower():\n",
        "        return nn.Identity()\n",
        "    else:\n",
        "        return NotImplementedError\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def prod_func(Vec):\n",
        "  return reduce( lambda x, y: x*y, Vec ) #  math.prod()\n",
        "\n",
        "def downsampler_fn(img, out_size):\n",
        "    \"\"\"\n",
        "    input sahep: B,C,H,W,D\n",
        "    output sahep: B,C,H,W,D\n",
        "    \"\"\"\n",
        "    return nn.functional.interpolate(img,\n",
        "                                     size=out_size,\n",
        "                                     mode='nearest',\n",
        "                                     align_corners=None,\n",
        "                                     recompute_scale_factor=None,\n",
        "                                     #antialias=False\n",
        "                                     )\n",
        "\n",
        "\n",
        "def get_norm(name, **kwargs):\n",
        "    if name.lower() == 'BatchNorm'.lower():\n",
        "        BatchNorm = getattr(nn, 'BatchNorm%dd' % ndims)\n",
        "        return BatchNorm(**kwargs)\n",
        "    elif name.lower() in ['instance', 'InstanceNorm'.lower()]:\n",
        "        InstanceNorm = getattr(nn, 'InstanceNorm%dd' % ndims)\n",
        "        return InstanceNorm(**kwargs)\n",
        "    elif name.lower() == 'None'.lower():\n",
        "        return nn.Identity()\n",
        "    else:\n",
        "        return NotImplementedError\n",
        "\n",
        "\n",
        "def get_activation(name, **kwargs):\n",
        "    if name.lower() == 'ReLU'.lower():\n",
        "        return nn.ReLU()\n",
        "    elif name.lower() == 'GELU'.lower():\n",
        "        return nn.GELU()\n",
        "    elif name.lower() == 'None'.lower():\n",
        "        return nn.Identity()\n",
        "    else:\n",
        "        return NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "def downsampler_fn(data, out_size):\n",
        "    \"\"\"\n",
        "    input sahep: B,Ci,Hi,Wi,Di\n",
        "    output sahep: B,C,H,W,D\n",
        "\n",
        "    \"\"\"\n",
        "    out = nn.functional.interpolate(data,\n",
        "                                     size=out_size,\n",
        "                                     mode='trilinear',\n",
        "                                     align_corners=None,\n",
        "                                     recompute_scale_factor=None,\n",
        "                                     #antialias=False\n",
        "    )\n",
        "    return out.to(data.device)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                in_feats,\n",
        "                MLP_type=\"basic\", # scmlp conv basic\n",
        "                hid_feats=None,\n",
        "                out_feats=None,\n",
        "                kernel_size=3,\n",
        "                act_name=\"GELU\",\n",
        "                drop=0.,\n",
        "                bias=False,\n",
        "            )->None:\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        out_feats = out_feats or in_feats\n",
        "        hid_feats = hid_feats or in_feats\n",
        "\n",
        "        if MLP_type.lower()==\"scmlp\":\n",
        "            # improved MLP : 3x3conv (spatial) -> eca (channel) -> mlp\n",
        "            self.net = nn.Sequential(*[\n",
        "                rearrange('B h w c -> B c h w'),\n",
        "                nn.Conv3d(in_feats, out_feats, kernel_size=1, bias=bias),\n",
        "                nn.BatchNorm3d(out_feats),\n",
        "                get_activation(act_name),\n",
        "                rearrange('B c h w d -> B h w d c'),\n",
        "                nn.Linear(out_feats, out_feats),\n",
        "                get_activation(act_name),\n",
        "                nn.Dropout(drop),\n",
        "                nn.Linear(out_feats, out_feats),\n",
        "                get_activation(act_name),\n",
        "                nn.Dropout(drop),\n",
        "            ])\n",
        "\n",
        "\n",
        "        elif MLP_type.lower()==\"conv\":\n",
        "            # improved MLP # RVT cvpr2022\n",
        "            self.net = nn.Sequential(*[\n",
        "                rearrange('B h w c -> B c h w'),\n",
        "                nn.Conv3d(in_feats, hid_feats, kernel_size=1, bias=bias),\n",
        "                nn.BatchNorm2d(hid_feats),\n",
        "                get_activation(act_name),\n",
        "                nn.Dropout(drop),\n",
        "                nn.Conv3d(hid_feats, hid_feats, kernel_size=kernel_size,\n",
        "                    padding=int(kernel_size//2), groups=hid_feats, bias=bias),\n",
        "                nn.BatchNorm2d(hid_feats),\n",
        "                get_activation(act_name),\n",
        "                nn.Conv3d(hid_feats, out_feats, kernel_size=1, bias=bias),\n",
        "                nn.BatchNorm2d(out_feats),\n",
        "                nn.Dropout(drop),\n",
        "                rearrange('B c h w-> B h w c'),\n",
        "            ])\n",
        "\n",
        "\n",
        "        elif MLP_type.lower()==\"basic\":\n",
        "            self.net = nn.Sequential(*[\n",
        "                    nn.Linear(in_feats, hid_feats),\n",
        "                    get_activation(act_name),\n",
        "                    nn.Dropout(drop),\n",
        "                    nn.Linear(hid_feats, out_feats),\n",
        "                    nn.Dropout(drop)\n",
        "            ])\n",
        "\n",
        "    def forward(self, x)-> torch.Tensor:\n",
        "        x = self.net(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EKXfxRG1ufMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation"
      ],
      "metadata": {
        "id": "YyogEY4pupxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    N-D Spatial Transformer\n",
        "    Obtained from https://github.com/voxelmorph/voxelmorph\n",
        "    \"\"\"\n",
        "    def __init__(self, size, mode='bilinear'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        # create sampling grid\n",
        "        vectors = [torch.arange(0, s) for s in size]\n",
        "        grids = torch.meshgrid(vectors)\n",
        "        grid = torch.stack(grids)\n",
        "        grid = torch.unsqueeze(grid, 0)\n",
        "        grid = grid.type(torch.FloatTensor)\n",
        "\n",
        "        # registering the grid as a buffer cleanly moves it to the GPU, but it also\n",
        "        # adds it to the state dict. this is annoying since everything in the state dict\n",
        "        # is included when saving weights to disk, so the model files are way bigger\n",
        "        # than they need to be. so far, there does not appear to be an elegant solution.\n",
        "        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict\n",
        "        self.register_buffer('grid', grid)\n",
        "\n",
        "    def forward(self, src, flow):\n",
        "        # new locations\n",
        "        new_locs = self.grid + flow\n",
        "        shape = flow.shape[2:]\n",
        "\n",
        "        # need to normalize grid values to [-1, 1] for resampler\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n",
        "\n",
        "        # move channels dim to last position\n",
        "        # also not sure why, but the channels need to be reversed\n",
        "        if len(shape) == 2:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 1)\n",
        "            new_locs = new_locs[..., [1, 0]]\n",
        "        elif len(shape) == 3:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 4, 1)\n",
        "            new_locs = new_locs[..., [2, 1, 0]]\n",
        "\n",
        "        return F.grid_sample(src, new_locs, align_corners=False, mode=self.mode)\n",
        "\n",
        "\n",
        "def normalize_displacement(displacement: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Spatially normalize the displacement vector field to the [-1, 1] coordinate system utilized by PyTorch's `grid_sample()` function.\n",
        "\n",
        "    This function assumes that the displacement field size is identical to the corresponding image dimensions.\n",
        "\n",
        "    Args:\n",
        "        displacement (Union[np.ndarray, torch.Tensor]): The displacement field with shape (N, ndim, *size).\n",
        "\n",
        "    Returns:\n",
        "        Union[np.ndarray, torch.Tensor]: The normalized displacement field.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input type is neither numpy.ndarray nor torch.Tensor.\n",
        "    \"\"\"\n",
        "    number_of_dimensions = displacement.ndim - 2\n",
        "\n",
        "    if isinstance(displacement, np.ndarray):\n",
        "        normalization_factors = 2.0 / np.array(displacement.shape[2:])\n",
        "        normalization_factors = normalization_factors.reshape(1, number_of_dimensions, *(1,) * number_of_dimensions)\n",
        "\n",
        "    elif isinstance(displacement, torch.Tensor):\n",
        "        normalization_factors = torch.tensor(2.0) / torch.tensor(\n",
        "            displacement.size()[2:], dtype=displacement.dtype, device=displacement.device)\n",
        "        normalization_factors = normalization_factors.view(1, number_of_dimensions, *(1,) * number_of_dimensions)\n",
        "\n",
        "    else:\n",
        "        raise TypeError(\"Input data type not recognized. Expected numpy.ndarray or torch.Tensor.\")\n",
        "\n",
        "    return displacement * normalization_factors\n"
      ],
      "metadata": {
        "id": "dMcTBFY3utu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# src/model/hvit_light.py"
      ],
      "metadata": {
        "id": "InMze5vAugIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "from typing import Tuple, Dict, Any, List, Set, Optional, Union, Callable, Type\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import Tensor\n",
        "from einops import rearrange\n",
        "import lightning as L\n",
        "\n",
        "from src.model.blocks import *\n",
        "from src.model.transformation import *\n",
        "\n",
        "\n",
        "WO_SELF_ATT = False # without self attention\n",
        "_NUM_CROSS_ATT = -1\n",
        "ndims = 3 # H,W,D\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention module for hierarchical vision transformer.\n",
        "\n",
        "    This module implements both local and global attention mechanisms.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        patch_size: Union[int, List[int]],\n",
        "        attention_type: str = \"local\",\n",
        "        qkv_bias: bool = True,\n",
        "        qk_scale: Optional[float] = None,\n",
        "        attn_drop: float = 0.,\n",
        "        proj_drop: float = 0.\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the Attention module.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Input dimension.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            patch_size (Union[int, List[int]]): Size of the patches.\n",
        "            attention_type (str): Type of attention mechanism (\"local\" or \"global\").\n",
        "            qkv_bias (bool): Whether to use bias in query, key, value projections.\n",
        "            qk_scale (Optional[float]): Scale factor for query-key dot product.\n",
        "            attn_drop (float): Dropout rate for attention matrix.\n",
        "            proj_drop (float): Dropout rate for output projection.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim: int = dim\n",
        "        self.num_heads: int = num_heads\n",
        "        self.patch_size: List[int] = [patch_size] * ndims if isinstance(patch_size, int) else patch_size\n",
        "        self.attention_type: str = attention_type\n",
        "\n",
        "        assert dim % num_heads == 0, \"Dimension must be divisible by number of heads\"\n",
        "        self.head_dim: int = dim // num_heads\n",
        "        self.scale: float = qk_scale or self.head_dim ** -0.5\n",
        "\n",
        "        # Skip initialization if using local attention without self-attention\n",
        "        if self.attention_type == \"local\" and WO_SELF_ATT:\n",
        "            return\n",
        "\n",
        "        # Initialize query, key, value projections based on attention type\n",
        "        if attention_type == \"local\":\n",
        "            self.qkv: nn.Linear = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        elif attention_type == \"global\":\n",
        "            self.qkv: nn.Linear = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "\n",
        "        self.attn_drop: nn.Dropout = nn.Dropout(attn_drop)\n",
        "        self.proj: nn.Linear = nn.Linear(dim, dim)\n",
        "        self.proj_drop: nn.Dropout = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x: Tensor, q_ms: Optional[Tensor] = None) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the Attention module.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor.\n",
        "            q_ms (Optional[Tensor]): Query tensor for global attention.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor after applying attention.\n",
        "        \"\"\"\n",
        "        B_, N, C = x.size()\n",
        "\n",
        "        # Return input if using local attention without self-attention\n",
        "        if self.attention_type == \"local\" and WO_SELF_ATT:\n",
        "            return x\n",
        "\n",
        "        if self.attention_type == \"local\":\n",
        "            qkv: Tensor = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "            q = q * self.scale\n",
        "        else:\n",
        "            B: int = q_ms.size()[0]\n",
        "            kv: Tensor = self.qkv(x).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "            k, v = kv[0], kv[1]\n",
        "            q: Tensor = self._process_global_query(q_ms, B, B_, N, C)\n",
        "\n",
        "        # Compute attention scores and apply attention\n",
        "        attn: Tensor = (q @ k.transpose(-2, -1))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x: Tensor = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _process_global_query(self, q_ms: Tensor, B: int, B_: int, N: int, C: int) -> Tensor:\n",
        "        \"\"\"\n",
        "        Process the global query tensor.\n",
        "\n",
        "        Args:\n",
        "            q_ms (Tensor): Global query tensor.\n",
        "            B (int): Batch size of q_ms.\n",
        "            B_ (int): Batch size of input tensor.\n",
        "            N (int): Number of patches.\n",
        "            C (int): Channel dimension.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Processed global query tensor.\n",
        "        \"\"\"\n",
        "        q_tmp: Tensor = q_ms.reshape(B, self.num_heads, N, C // self.num_heads)\n",
        "        div_, rem_ = divmod(B_, B)\n",
        "        q_tmp = q_tmp.repeat(div_, 1, 1, 1)\n",
        "        q_tmp = q_tmp.reshape(B * div_, self.num_heads, N, C // self.num_heads)\n",
        "\n",
        "        q: Tensor = torch.zeros(B_, self.num_heads, N, C // self.num_heads, device=q_ms.device)\n",
        "        q[:B*div_] = q_tmp\n",
        "        if rem_ > 0:\n",
        "            q[B*div_:] = q_tmp[:rem_]\n",
        "\n",
        "        return q * self.scale\n",
        "\n",
        "\n",
        "def get_patches(x: Tensor, patch_size: int) -> Tuple[Tensor, int, int, int]:\n",
        "    \"\"\"\n",
        "    Divide the input tensor into patches and reshape them for processing.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input tensor of shape (B, H, W, D, C).\n",
        "        patch_size (int): Size of each patch.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Tensor, int, int, int]: A tuple containing:\n",
        "            - windows: Reshaped tensor of patches.\n",
        "            - H, W, D: Updated dimensions of the input tensor.\n",
        "    \"\"\"\n",
        "    B, H, W, D, C = x.size()\n",
        "    nh: float = H / patch_size\n",
        "    nw: float = W / patch_size\n",
        "    nd: float = D / patch_size\n",
        "\n",
        "    # Check if downsampling is required\n",
        "    down_req: float = (nh - int(nh)) + (nw - int(nw)) + (nd - int(nd))\n",
        "    if down_req > 0:\n",
        "        # Downsample the input tensor to fit patch size\n",
        "        new_dims: List[int] = [int(nh) * patch_size, int(nw) * patch_size, int(nd) * patch_size]\n",
        "        x = downsampler_fn(x.permute(0, 4, 1, 2, 3), new_dims).permute(0, 2, 3, 4, 1)\n",
        "        B, H, W, D, C = x.size()\n",
        "\n",
        "    # Reshape the tensor into patches\n",
        "    x = x.view(B, H // patch_size, patch_size,\n",
        "               W // patch_size, patch_size,\n",
        "               D // patch_size, patch_size,\n",
        "               C)\n",
        "\n",
        "    # Rearrange dimensions and flatten patches\n",
        "    windows: Tensor = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, patch_size, patch_size, patch_size, C)\n",
        "\n",
        "    return windows, H, W, D\n",
        "\n",
        "\n",
        "def get_image(windows: Tensor, patch_size: int, Hatt: int, Watt: int, Datt: int, H: int, W: int, D: int) -> Tensor:\n",
        "    \"\"\"\n",
        "    Reconstruct the image from windows (patches).\n",
        "\n",
        "    Args:\n",
        "        windows (Tensor): Input tensor containing the windows.\n",
        "        patch_size (int): Size of each patch.\n",
        "        Hatt, Watt, Datt (int): Dimensions of the attention space.\n",
        "        H, W, D (int): Original dimensions of the image.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Reconstructed image.\n",
        "    \"\"\"\n",
        "    # Calculate batch size\n",
        "    B: int = int(windows.size(0) / ((Hatt * Watt * Datt) // (patch_size ** 3)))\n",
        "\n",
        "    # Reshape windows into image\n",
        "    x: Tensor = windows.view(B,\n",
        "                    Hatt // patch_size,\n",
        "                    Watt // patch_size,\n",
        "                    Datt // patch_size,\n",
        "                    patch_size, patch_size, patch_size, -1)\n",
        "    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, Hatt, Watt, Datt, -1)\n",
        "\n",
        "    # Downsample if necessary\n",
        "    if H != Hatt or W != Watt or D != Datt:\n",
        "        x = downsampler_fn(x.permute(0, 4, 1, 2, 3), [H, W, D]).permute(0, 2, 3, 4, 1)\n",
        "    return x\n",
        "\n",
        "class ViTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer Block.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 embed_dim: int,\n",
        "                 input_dims: List[int],\n",
        "                 num_heads: int,\n",
        "                 mlp_type: str,\n",
        "                 patch_size: int,\n",
        "                 mlp_ratio: float,\n",
        "                 qkv_bias: bool,\n",
        "                 qk_scale: Optional[float],\n",
        "                 drop: float,\n",
        "                 attn_drop: float,\n",
        "                 drop_path: float,\n",
        "                 act_layer: str,\n",
        "                 attention_type: str,\n",
        "                 norm_layer: Callable[..., nn.Module],\n",
        "                 layer_scale: Optional[float]):\n",
        "        super().__init__()\n",
        "        self.patch_size: int = patch_size\n",
        "        self.num_windows: int = prod_func([d // patch_size for d in input_dims])\n",
        "\n",
        "        self.norm1: nn.Module = norm_layer(embed_dim)\n",
        "        self.attn: nn.Module = Attention(\n",
        "            embed_dim,\n",
        "            attention_type=attention_type,\n",
        "            num_heads=num_heads,\n",
        "            patch_size=patch_size,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop,\n",
        "        )\n",
        "\n",
        "        self.drop_path: nn.Module = timm_DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2: nn.Module = norm_layer(embed_dim)\n",
        "        self.mlp: nn.Module = MLP(\n",
        "            in_feats=embed_dim,\n",
        "            hid_feats=int(embed_dim * mlp_ratio),\n",
        "            act_name=act_layer,\n",
        "            drop=drop,\n",
        "            MLP_type=mlp_type\n",
        "        )\n",
        "\n",
        "        self.layer_scale: bool = layer_scale is not None and isinstance(layer_scale, (int, float))\n",
        "        if self.layer_scale:\n",
        "            self.gamma1: nn.Parameter = nn.Parameter(layer_scale * torch.ones(embed_dim), requires_grad=True)\n",
        "            self.gamma2: nn.Parameter = nn.Parameter(layer_scale * torch.ones(embed_dim), requires_grad=True)\n",
        "        else:\n",
        "            self.gamma1: float = 1.0\n",
        "            self.gamma2: float = 1.0\n",
        "\n",
        "    def forward(self, x: Tensor, q_ms: Optional[Tensor]) -> Tensor:\n",
        "        B, H, W, D, C = x.size()\n",
        "        shortcut: Tensor = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        x_windows, Hatt, Watt, Datt = get_patches(x, self.patch_size)\n",
        "        x_windows = x_windows.view(-1, self.patch_size ** 3, C)\n",
        "\n",
        "        attn_windows: Tensor = self.attn(x_windows, q_ms)\n",
        "        x = get_image(attn_windows, self.patch_size, Hatt, Watt, Datt, H, W, D)\n",
        "        x = shortcut + self.drop_path(self.gamma1 * x)\n",
        "        x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch Embedding layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chans: int = 3, out_chans: int = 32,\n",
        "                 drop_rate: float = 0,\n",
        "                 kernel_size: int = 3,\n",
        "                 stride: int = 1, padding: int = 1,\n",
        "                 dilation: int = 1, groups: int = 1, bias: bool = False) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        Convnd: Type[nn.Module] = getattr(nn, f\"Conv{ndims}d\")\n",
        "        self.proj: nn.Module = Convnd(in_channels=in_chans, out_channels=out_chans,\n",
        "                              kernel_size=kernel_size,\n",
        "                              stride=stride, padding=padding,\n",
        "                              dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "        self.drop: nn.Module = nn.Dropout(p=drop_rate)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.drop(self.proj(x))\n",
        "        return x\n",
        "\n",
        "class ViTLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer Layer.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_type: str,\n",
        "        dim: int,\n",
        "        dim_out: int,\n",
        "        depth: int,\n",
        "        input_dims: List[int],\n",
        "        num_heads: int,\n",
        "        patch_size: int,\n",
        "        mlp_type: str,\n",
        "        mlp_ratio: float,\n",
        "        qkv_bias: bool,\n",
        "        qk_scale: Optional[float],\n",
        "        drop: float,\n",
        "        attn_drop: float,\n",
        "        drop_path: Union[float, List[float]],\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        norm_type: str,\n",
        "        layer_scale: Optional[float],\n",
        "        act_layer: str\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.patch_size: int = patch_size\n",
        "        self.embed_dim: int = dim\n",
        "        self.input_dims: List[int] = input_dims\n",
        "        self.blocks: nn.ModuleList = nn.ModuleList([\n",
        "            ViTBlock(\n",
        "                embed_dim=dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_type=mlp_type,\n",
        "                patch_size=patch_size,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                attention_type=attention_type,\n",
        "                drop=drop,\n",
        "                attn_drop=attn_drop,\n",
        "                drop_path=drop_path[k] if isinstance(drop_path, list) else drop_path,\n",
        "                act_layer=act_layer,\n",
        "                norm_layer=norm_layer,\n",
        "                layer_scale=layer_scale,\n",
        "                input_dims=input_dims\n",
        "            )\n",
        "            for k in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, inp: Tensor, q_ms: Optional[Tensor], CONCAT_ok: bool) -> Tensor:\n",
        "        x: Tensor = inp.clone()\n",
        "        x = rearrange(x, 'b c h w d -> b h w d c')\n",
        "\n",
        "        if q_ms is not None:\n",
        "            q_ms = rearrange(q_ms, 'b c h w d -> b h w d c')\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            if q_ms is None:\n",
        "                x = blk(x, None)\n",
        "            else:\n",
        "                q_ms_patches, _, _, _ = get_patches(q_ms, self.patch_size)\n",
        "                q_ms_patches = q_ms_patches.view(-1, self.patch_size ** ndims, x.size()[-1])\n",
        "                x = blk(x, q_ms_patches)\n",
        "\n",
        "        x = rearrange(x, 'b h w d c -> b c h w d')\n",
        "\n",
        "        if CONCAT_ok:\n",
        "            x = torch.cat((inp, x), dim=-1)\n",
        "        else:\n",
        "            x = inp + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) module for hierarchical feature processing.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 PYR_SCALES=None,\n",
        "                 feats_num=None,\n",
        "                 hid_dim=None,\n",
        "                 depths=None,\n",
        "                 patch_size=None,\n",
        "                 mlp_ratio=None,\n",
        "                 num_heads=None,\n",
        "                 mlp_type=None,\n",
        "                 norm_type=None,\n",
        "                 act_layer=None,\n",
        "                 drop_path_rate: float = 0.2,\n",
        "                 qkv_bias: bool = True,\n",
        "                 qk_scale: bool = None,\n",
        "                 drop_rate: float = 0.,\n",
        "                 attn_drop_rate: float = 0.,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 layer_scale=None,\n",
        "                 img_size=None,\n",
        "                 NUM_CROSS_ATT=-1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Determine the number of levels for processing\n",
        "        num_levels = len(feats_num)\n",
        "        num_levels = min(num_levels, NUM_CROSS_ATT) if NUM_CROSS_ATT > 0 else num_levels\n",
        "        if WO_SELF_ATT:\n",
        "            num_levels -= 1\n",
        "\n",
        "        # Ensure patch_size is a list\n",
        "        patch_size = patch_size if isinstance(patch_size, list) else [patch_size for _ in range(num_levels)]\n",
        "        hwd = img_size[-1]\n",
        "\n",
        "        # Create patch embedding layers\n",
        "        self.patch_embed = nn.ModuleList([\n",
        "            PatchEmbed(\n",
        "                in_chans=feats_num[i],\n",
        "                out_chans=hid_dim,\n",
        "                drop_rate=drop_rate\n",
        "            ) for i in range(num_levels)\n",
        "        ])\n",
        "\n",
        "        # Generate drop path rate for each layer\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        # Create ViT layers\n",
        "        self.levels = nn.ModuleList()\n",
        "        for i in range(num_levels):\n",
        "            level = ViTLayer(\n",
        "                dim=hid_dim,\n",
        "                dim_out=hid_dim,\n",
        "                depth=depths[i],\n",
        "                num_heads=num_heads[i],\n",
        "                patch_size=patch_size[i],\n",
        "                mlp_type=mlp_type,\n",
        "                attention_type=\"local\" if i == 0 else \"global\",\n",
        "                drop_path=dpr[sum(depths[:i]):sum(depths[:i+1])],\n",
        "                input_dims=img_size[i],\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                norm_layer=norm_layer,\n",
        "                layer_scale=layer_scale,\n",
        "                norm_type=norm_type,\n",
        "                act_layer=act_layer\n",
        "            )\n",
        "            self.levels.append(level)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        \"\"\"Initialize the weights of the module.\"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            timm_trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        \"\"\"Return keywords for no weight decay.\"\"\"\n",
        "        return {'rpb'}\n",
        "\n",
        "    def forward(self, KQs, CONCAT_ok: bool = False):\n",
        "        \"\"\"\n",
        "        Forward pass of the ViT module.\n",
        "\n",
        "        Args:\n",
        "            KQs (List[Tensor]): List of input tensors for each level.\n",
        "            CONCAT_ok (bool): Flag to determine if concatenation is allowed.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Processed output tensor.\n",
        "        \"\"\"\n",
        "        for i, (patch_embed_, level) in enumerate(zip(self.patch_embed, self.levels)):\n",
        "            if i == 0:\n",
        "                # First level: process input without cross-attention\n",
        "                Q = patch_embed_(KQs[i])\n",
        "                x = level(Q, None, CONCAT_ok=CONCAT_ok)\n",
        "                Q = patch_embed_(x)\n",
        "            else:\n",
        "                # Subsequent levels: process with cross-attention\n",
        "                K = patch_embed_(KQs[i])\n",
        "                x = level(Q, K, CONCAT_ok=CONCAT_ok)\n",
        "                Q = x.clone()\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderCnnBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional block for the encoder part of the network.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        stride,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "        affine=True,\n",
        "        eps=1e-05\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # First convolutional block\n",
        "        conv_block_1 = [\n",
        "            nn.Conv3d(\n",
        "                in_channels=in_channels, out_channels=out_channels,\n",
        "                kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                bias=bias\n",
        "            ),\n",
        "            nn.InstanceNorm3d(num_features=out_channels, affine=affine, eps=eps),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Second convolutional block\n",
        "        conv_block_2 = [\n",
        "            nn.Conv3d(\n",
        "                in_channels=out_channels, out_channels=out_channels,\n",
        "                kernel_size=kernel_size, stride=1, padding=padding,\n",
        "                bias=bias\n",
        "            ),\n",
        "            nn.InstanceNorm3d(num_features=out_channels, affine=affine, eps=eps),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Combine both blocks\n",
        "        self._block = nn.Sequential(\n",
        "            *conv_block_1,\n",
        "            *conv_block_2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of the EncoderCnnBlock.\"\"\"\n",
        "        return self._block(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self._num_stages: int = config['num_stages']\n",
        "        self.use_seg: bool = config['use_seg_loss']\n",
        "\n",
        "\n",
        "        # Determine channels of encoder feature maps\n",
        "        encoder_out_channels: torch.Tensor = torch.tensor([config['start_channels'] * 2**stage for stage in range(self._num_stages)])\n",
        "\n",
        "        # Estimate required stages\n",
        "        required_stages: Set[int] = set(int(fmap[-1]) for fmap in config['out_fmaps'])\n",
        "        self._required_stages: Set[int] = required_stages\n",
        "\n",
        "        earliest_required_stage: int = min(required_stages)\n",
        "\n",
        "        # Lateral connections\n",
        "        lateral_in_channels: torch.Tensor = encoder_out_channels[earliest_required_stage:]\n",
        "        lateral_out_channels: torch.Tensor = lateral_in_channels.clip(max=config['fpn_channels'])\n",
        "\n",
        "        self._lateral: nn.ModuleList = nn.ModuleList([\n",
        "            nn.Conv3d(in_channels=in_ch, out_channels=out_ch, kernel_size=1)\n",
        "            for in_ch, out_ch in zip(lateral_in_channels, lateral_out_channels)\n",
        "        ])\n",
        "        self._lateral_levels: int = len(self._lateral)\n",
        "\n",
        "        # Output layers\n",
        "        out_in_channels: List[int] = [lateral_out_channels[-self._num_stages + required_stage].item() for required_stage in required_stages]\n",
        "        out_out_channels: List[int] = [int(config['fpn_channels'])] * len(out_in_channels)\n",
        "        out_out_channels[0] = int(config['fpn_channels'])\n",
        "\n",
        "        self._out: nn.ModuleList = nn.ModuleList([\n",
        "            nn.Conv3d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1)\n",
        "            for in_ch, out_ch in zip(out_in_channels, out_out_channels)\n",
        "        ])\n",
        "\n",
        "        # Upsampling layers\n",
        "        self._up: nn.ModuleList = nn.ModuleList([\n",
        "            nn.ConvTranspose3d(\n",
        "                in_channels=list(reversed(lateral_out_channels))[level],\n",
        "                out_channels=list(reversed(lateral_out_channels))[level+1],\n",
        "                kernel_size=list(reversed(config['strides']))[level],\n",
        "                stride=list(reversed(config['strides']))[level]\n",
        "            )\n",
        "            for level in range(len(lateral_out_channels)-1)\n",
        "        ])\n",
        "\n",
        "        # Multi-scale attention\n",
        "        self.hierarchical_dec: nn.ModuleList = self._create_hierarchical_layers(config, out_out_channels)\n",
        "\n",
        "        if self.use_seg:\n",
        "            self._seg_head: nn.ModuleList = nn.ModuleList([\n",
        "                nn.Conv3d(out_ch, config['num_organs'] + 1, kernel_size=1, stride=1)\n",
        "                for out_ch in out_out_channels\n",
        "            ])\n",
        "\n",
        "    def _create_hierarchical_layers(self, config: Dict[str, Any], out_out_channels: List[int]) -> nn.ModuleList:\n",
        "        out: nn.ModuleList = nn.ModuleList()\n",
        "        img_size: List[List[int]] = []\n",
        "        feats_num: List[int] = []\n",
        "\n",
        "        for k, out_ch in enumerate(out_out_channels):\n",
        "            img_size.append([int(item/(2**(self._num_stages-k-1))) for item in config['data_size']])\n",
        "            feats_num.append(out_ch)\n",
        "            n: int = len(feats_num)\n",
        "\n",
        "            if k == 0:\n",
        "                out.append(nn.Identity())\n",
        "            else:\n",
        "                out.append(\n",
        "                    ViT(\n",
        "                        NUM_CROSS_ATT=config.get('NUM_CROSS_ATT', _NUM_CROSS_ATT),\n",
        "                        PYR_SCALES=[1.],\n",
        "                        feats_num=feats_num,\n",
        "                        hid_dim=int(config.get('fpn_channels', 64)),\n",
        "                        depths=[int(config.get('depths', 1))]*n,\n",
        "                        patch_size=[int(config.get('patch_size', 2))]*n,\n",
        "                        mlp_ratio=int(config.get('mlp_ratio', 2)),\n",
        "                        num_heads=[int(config.get('num_heads', 32))]*n,\n",
        "                        mlp_type='basic',\n",
        "                        norm_type='BatchNorm2d',\n",
        "                        act_layer='gelu',\n",
        "                        drop_path_rate=config.get('drop_path_rate', 0.2),\n",
        "                        qkv_bias=config.get('qkv_bias', True),\n",
        "                        qk_scale=None,\n",
        "                        drop_rate=config.get('drop_rate', 0.),\n",
        "                        attn_drop_rate=config.get('attn_drop_rate', 0.),\n",
        "                        norm_layer=nn.LayerNorm,\n",
        "                        layer_scale=1e-5,\n",
        "                        img_size=img_size\n",
        "                    )\n",
        "                )\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
        "        lateral_out: List[Tensor] = [lateral(fmap) for lateral, fmap in zip(self._lateral, list(x.values())[-self._lateral_levels:])]\n",
        "\n",
        "        up_out: List[Tensor] = []\n",
        "        for idx, x in enumerate(reversed(lateral_out)):\n",
        "            if idx != 0:\n",
        "                x = x + up\n",
        "\n",
        "            if idx < self._lateral_levels - 1:\n",
        "                up = self._up[idx](x)\n",
        "\n",
        "            up_out.append(x)\n",
        "\n",
        "        cnn_outputs: Dict[int, Tensor] = {stage: self._out[idx](fmap) for idx, (fmap, stage) in enumerate(zip(reversed(up_out), self._required_stages))}\n",
        "        return self._forward_hierarchical(cnn_outputs)\n",
        "\n",
        "    def _forward_hierarchical(self, cnn_outputs: Dict[int, Tensor]) -> Dict[str, Tensor]:\n",
        "        xs: List[Tensor] = [cnn_outputs[key].clone() for key in range(max(cnn_outputs.keys()), min(cnn_outputs.keys())-1, -1)]\n",
        "\n",
        "        out_dict: Dict[str, Tensor] = {}\n",
        "        for i, key in enumerate(range(max(cnn_outputs.keys()), min(cnn_outputs.keys())-1, -1)):\n",
        "            QK = xs[0:i+1]\n",
        "            QK.reverse()\n",
        "\n",
        "            if i == 0:\n",
        "                Pi = QK[0]\n",
        "            else:\n",
        "                Pi = self.hierarchical_dec[i](QK)\n",
        "            out_dict[f'P{key}'] = Pi\n",
        "\n",
        "            if self.use_seg:\n",
        "                Pi_seg = self._seg_head[i](Pi)\n",
        "                out_dict[f'S{key}'] = Pi_seg\n",
        "        return out_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class HierarchicalViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Hierarchical Vision Transformer (HViT) for image processing tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "\n",
        "        # Configuration parameters\n",
        "        self.backbone: str = config['backbone_net']\n",
        "        in_channels: int = 2 * config.get('in_channels', 1)  # source + target\n",
        "        kernel_size: int = config.get('kernel_size', 3)\n",
        "        emb_dim: int = config.get('start_channels', 32)\n",
        "        data_size: Tuple[int, ...] = config.get('data_size', [160, 192, 224])\n",
        "        self.out_fmaps: List[str] = config.get('out_fmaps', ['P4', 'P3', 'P2', 'P1'])\n",
        "\n",
        "        # Calculate number of stages\n",
        "        num_stages: int = min(int(math.log2(min(data_size))) - 1,\n",
        "                              max(int(fmap[-1]) for fmap in self.out_fmaps) + 1)\n",
        "\n",
        "        strides: List[int] = [1] + [2] * (num_stages - 1)\n",
        "        kernel_sizes: List[int] = [kernel_size] * num_stages\n",
        "\n",
        "        config['num_stages'] = num_stages\n",
        "        config['strides'] = strides\n",
        "\n",
        "        # Build encoder\n",
        "        self._encoder: nn.ModuleList = nn.ModuleList()\n",
        "        if self.backbone in ['fpn', 'FPN']:\n",
        "            for k in range(num_stages):\n",
        "                blk = EncoderCnnBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=emb_dim,\n",
        "                    kernel_size=kernel_sizes[k],\n",
        "                    stride=strides[k]\n",
        "                )\n",
        "                self._encoder.append(blk)\n",
        "\n",
        "                in_channels = emb_dim\n",
        "                emb_dim *= 2\n",
        "\n",
        "        # Build decoder\n",
        "        if self.backbone in ['fpn', 'FPN']:\n",
        "            self._decoder: Decoder = Decoder(config)\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Initialize model weights.\n",
        "        \"\"\"\n",
        "        # TODO: Implement weight initialization\n",
        "\n",
        "    def forward(self, x: Tensor, verbose: bool = False) -> Dict[str, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the HierarchicalViT model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor.\n",
        "            verbose (bool): If True, print shape information.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Tensor]: Output feature maps.\n",
        "        \"\"\"\n",
        "        down: Dict[str, Tensor] = {}\n",
        "        if self.backbone in ['fpn', 'FPN']:\n",
        "            for stage_id, module in enumerate(self._encoder):\n",
        "                x = module(x)\n",
        "                down[f'C{stage_id}'] = x\n",
        "            up = self._decoder(down)\n",
        "\n",
        "        if verbose:\n",
        "            for key, item in down.items():\n",
        "                print(f'down {key}', item.shape)\n",
        "            for key, item in up.items():\n",
        "                print(f'up {key}', item.shape)\n",
        "        return up\n",
        "\n",
        "\n",
        "class RegistrationHead(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Registration head for generating displacement fields.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
        "        super().__init__()\n",
        "        conv3d = nn.Conv3d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2\n",
        "        )\n",
        "        # Initialize weights with small random values\n",
        "        conv3d.weight = nn.Parameter(torch.zeros_like(conv3d.weight).normal_(0, 1e-5))\n",
        "        conv3d.bias = nn.Parameter(torch.zeros(conv3d.bias.shape))\n",
        "        self.add_module('conv3d', conv3d)\n",
        "\n",
        "\n",
        "class HViT_Light(nn.Module):\n",
        "    \"\"\"\n",
        "    Light Hierarchical Vision Transformer (HViT) model for image registration.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: dict):\n",
        "        super(HViT_Light, self).__init__()\n",
        "        self.upsample_df: bool = config.get('upsample_df', False)\n",
        "        self.upsample_scale_factor: int = config.get('upsample_scale_factor', 2)\n",
        "        self.scale_level_df: str = config.get('scale_level_df', 'P1')\n",
        "\n",
        "        self.deformable: HierarchicalViT = HierarchicalViT(config)\n",
        "        self.avg_pool: nn.AvgPool3d = nn.AvgPool3d(3, stride=2, padding=1)\n",
        "        self.spatial_trans: SpatialTransformer = SpatialTransformer(config['data_size'])\n",
        "        self.reg_head: RegistrationHead = RegistrationHead(\n",
        "            in_channels=config.get('fpn_channels', 64),\n",
        "            out_channels=ndims,\n",
        "            kernel_size=ndims,\n",
        "        )\n",
        "\n",
        "    def forward(self, source: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the HViT model.\n",
        "\n",
        "        Args:\n",
        "            source (Tensor): Source image tensor.\n",
        "            target (Tensor): Target image tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Tensor, Tensor]: Moved image and displacement field.\n",
        "        \"\"\"\n",
        "        x: Tensor = torch.cat((source, target), dim=1)\n",
        "        x_dec: Dict[str, Tensor] = self.deformable(x)\n",
        "\n",
        "        # Extract features at the specified scale level\n",
        "        x_dec: Tensor = x_dec[self.scale_level_df]\n",
        "        flow: Tensor = self.reg_head(x_dec)\n",
        "\n",
        "        if self.upsample_df:\n",
        "            flow = nn.Upsample(scale_factor=self.upsample_scale_factor,\n",
        "                               mode='trilinear',\n",
        "                               align_corners=False)(flow)\n",
        "\n",
        "        moved: Tensor = self.spatial_trans(source, flow)\n",
        "        return moved, flow\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the HViT model\n",
        "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
        "    B = 1\n",
        "    H, W, D = 160//2, 192//2, 224//2\n",
        "\n",
        "    for fpn_channels in [64]:\n",
        "        config = {\n",
        "            'NUM_CROSS_ATT': _NUM_CROSS_ATT,\n",
        "            'out_fmaps': ['P4', 'P3', 'P2', 'P1'],\n",
        "            'scale_level_df': 'P1',\n",
        "            'upsample_df': True,\n",
        "            'upsample_scale_factor': 2,\n",
        "            'fpn_channels': fpn_channels,\n",
        "            'start_channels': 32,\n",
        "            'patch_size': 2,\n",
        "            'bspl': False,\n",
        "\n",
        "            'backbone_net': 'fpn',\n",
        "            'in_channels': 1,\n",
        "            'data_size': [H, W, D],\n",
        "            'bias': True,\n",
        "            'norm_type': 'instance',\n",
        "            'cuda': 0,\n",
        "            'kernel_size': 3,\n",
        "            'depths': 1,\n",
        "            'mlp_ratio': 2,\n",
        "\n",
        "            'num_heads': 32,\n",
        "            'drop_path_rate': 0.,\n",
        "            'qkv_bias': True,\n",
        "            'drop_rate': 0.,\n",
        "            'attn_drop_rate': 0.,\n",
        "\n",
        "            'use_seg_loss': False,\n",
        "            'use_seg_proxy_loss': False,\n",
        "            'num_organs': -1\n",
        "        }\n",
        "\n",
        "        source = torch.rand([1, 1, H, W, D])\n",
        "        tgt = torch.rand([1, 1, H, W, D])\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        model = HViT_Light(config)\n",
        "        model.to(device)\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "            source = source.to(dtype=torch.float16).to(device)\n",
        "            tgt = tgt.to(dtype=torch.float16).to(device)\n",
        "\n",
        "            moved, flow = model(source, tgt)\n",
        "            print('\\n\\nmoved {} flow {}'.format(moved.shape, flow.shape))\n",
        "\n",
        "            max_mem_mb = torch.cuda.max_memory_allocated() / 1024**3\n",
        "            print(\"[+] Maximum memory:\\t{:.2f}GB: >>> \\t{:.0f} feats\".format(max_mem_mb, config['fpn_channels']) if max_mem_mb is not None else \"\")\n",
        "            print(\"[+] Required Total memory:\\t{:.2f}GB\".format(torch.cuda.get_device_properties(0).total_memory/1024**3))\n",
        "            print(\"[+] Trainable params:\\t{:.5f} m\".format(count_parameters(model)/1e6))\n"
      ],
      "metadata": {
        "id": "oJD30or9ui7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hvit"
      ],
      "metadata": {
        "id": "GZYshFAruWRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "from typing import Tuple, Dict, Any, List, Set, Optional, Union, Callable, Type\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import Tensor\n",
        "from einops import rearrange\n",
        "import lightning as L\n",
        "\n",
        "from src.model.blocks import *\n",
        "from src.model.transformation import *\n",
        "\n",
        "\n",
        "WO_SELF_ATT = False # without self attention\n",
        "_NUM_CROSS_ATT = -1\n",
        "ndims = 3 # H,W,D\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention module for hierarchical vision transformer.\n",
        "\n",
        "    This module implements both local and global attention mechanisms.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        patch_size: Union[int, List[int]],\n",
        "        attention_type: str = \"local\",\n",
        "        qkv_bias: bool = True,\n",
        "        qk_scale: Optional[float] = None,\n",
        "        attn_drop: float = 0.,\n",
        "        proj_drop: float = 0.\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the Attention module.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Input dimension.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            patch_size (Union[int, List[int]]): Size of the patches.\n",
        "            attention_type (str): Type of attention mechanism (\"local\" or \"global\").\n",
        "            qkv_bias (bool): Whether to use bias in query, key, value projections.\n",
        "            qk_scale (Optional[float]): Scale factor for query-key dot product.\n",
        "            attn_drop (float): Dropout rate for attention matrix.\n",
        "            proj_drop (float): Dropout rate for output projection.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim: int = dim\n",
        "        self.num_heads: int = num_heads\n",
        "        self.patch_size: List[int] = [patch_size] * ndims if isinstance(patch_size, int) else patch_size\n",
        "        self.attention_type: str = attention_type\n",
        "\n",
        "        assert dim % num_heads == 0, \"Dimension must be divisible by number of heads\"\n",
        "        self.head_dim: int = dim // num_heads\n",
        "        self.scale: float = qk_scale or self.head_dim ** -0.5\n",
        "\n",
        "        # Skip initialization if using local attention without self-attention\n",
        "        if self.attention_type == \"local\" and WO_SELF_ATT:\n",
        "            return\n",
        "\n",
        "        # Initialize query, key, value projections based on attention type\n",
        "        if attention_type == \"local\":\n",
        "            self.qkv: nn.Linear = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        elif attention_type == \"global\":\n",
        "            self.qkv: nn.Linear = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "\n",
        "        self.attn_drop: nn.Dropout = nn.Dropout(attn_drop)\n",
        "        self.proj: nn.Linear = nn.Linear(dim, dim)\n",
        "        self.proj_drop: nn.Dropout = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x: Tensor, q_ms: Optional[Tensor] = None) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the Attention module.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor.\n",
        "            q_ms (Optional[Tensor]): Query tensor for global attention.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor after applying attention.\n",
        "        \"\"\"\n",
        "        B_, N, C = x.size()\n",
        "\n",
        "        # Return input if using local attention without self-attention\n",
        "        if self.attention_type == \"local\" and WO_SELF_ATT:\n",
        "            return x\n",
        "\n",
        "        if self.attention_type == \"local\":\n",
        "            qkv: Tensor = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "            q = q * self.scale\n",
        "        else:\n",
        "            B: int = q_ms.size()[0]\n",
        "            kv: Tensor = self.qkv(x).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "            k, v = kv[0], kv[1]\n",
        "            q: Tensor = self._process_global_query(q_ms, B, B_, N, C)\n",
        "\n",
        "        # Compute attention scores and apply attention\n",
        "        attn: Tensor = (q @ k.transpose(-2, -1))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x: Tensor = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _process_global_query(self, q_ms: Tensor, B: int, B_: int, N: int, C: int) -> Tensor:\n",
        "        \"\"\"\n",
        "        Process the global query tensor.\n",
        "\n",
        "        Args:\n",
        "            q_ms (Tensor): Global query tensor.\n",
        "            B (int): Batch size of q_ms.\n",
        "            B_ (int): Batch size of input tensor.\n",
        "            N (int): Number of patches.\n",
        "            C (int): Channel dimension.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Processed global query tensor.\n",
        "        \"\"\"\n",
        "        q_tmp: Tensor = q_ms.reshape(B, self.num_heads, N, C // self.num_heads)\n",
        "        div_, rem_ = divmod(B_, B)\n",
        "        q_tmp = q_tmp.repeat(div_, 1, 1, 1)\n",
        "        q_tmp = q_tmp.reshape(B * div_, self.num_heads, N, C // self.num_heads)\n",
        "\n",
        "        q: Tensor = torch.zeros(B_, self.num_heads, N, C // self.num_heads, device=q_ms.device)\n",
        "        q[:B*div_] = q_tmp\n",
        "        if rem_ > 0:\n",
        "            q[B*div_:] = q_tmp[:rem_]\n",
        "\n",
        "        return q * self.scale\n",
        "\n",
        "\n",
        "def get_patches(x: Tensor, patch_size: int) -> Tuple[Tensor, int, int, int]:\n",
        "    \"\"\"\n",
        "    Divide the input tensor into patches and reshape them for processing.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input tensor of shape (B, H, W, D, C).\n",
        "        patch_size (int): Size of each patch.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Tensor, int, int, int]: A tuple containing:\n",
        "            - windows: Reshaped tensor of patches.\n",
        "            - H, W, D: Updated dimensions of the input tensor.\n",
        "    \"\"\"\n",
        "    B, H, W, D, C = x.size()\n",
        "    nh: float = H / patch_size\n",
        "    nw: float = W / patch_size\n",
        "    nd: float = D / patch_size\n",
        "\n",
        "    # Check if downsampling is required\n",
        "    down_req: float = (nh - int(nh)) + (nw - int(nw)) + (nd - int(nd))\n",
        "    if down_req > 0:\n",
        "        # Downsample the input tensor to fit patch size\n",
        "        new_dims: List[int] = [int(nh) * patch_size, int(nw) * patch_size, int(nd) * patch_size]\n",
        "        x = downsampler_fn(x.permute(0, 4, 1, 2, 3), new_dims).permute(0, 2, 3, 4, 1)\n",
        "        B, H, W, D, C = x.size()\n",
        "\n",
        "    # Reshape the tensor into patches\n",
        "    x = x.view(B, H // patch_size, patch_size,\n",
        "               W // patch_size, patch_size,\n",
        "               D // patch_size, patch_size,\n",
        "               C)\n",
        "\n",
        "    # Rearrange dimensions and flatten patches\n",
        "    windows: Tensor = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, patch_size, patch_size, patch_size, C)\n",
        "\n",
        "    return windows, H, W, D\n",
        "\n",
        "\n",
        "def get_image(windows: Tensor, patch_size: int, Hatt: int, Watt: int, Datt: int, H: int, W: int, D: int) -> Tensor:\n",
        "    \"\"\"\n",
        "    Reconstruct the image from windows (patches).\n",
        "\n",
        "    Args:\n",
        "        windows (Tensor): Input tensor containing the windows.\n",
        "        patch_size (int): Size of each patch.\n",
        "        Hatt, Watt, Datt (int): Dimensions of the attention space.\n",
        "        H, W, D (int): Original dimensions of the image.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Reconstructed image.\n",
        "    \"\"\"\n",
        "    # Calculate batch size\n",
        "    B: int = int(windows.size(0) / ((Hatt * Watt * Datt) // (patch_size ** 3)))\n",
        "\n",
        "    # Reshape windows into image\n",
        "    x: Tensor = windows.view(B,\n",
        "                    Hatt // patch_size,\n",
        "                    Watt // patch_size,\n",
        "                    Datt // patch_size,\n",
        "                    patch_size, patch_size, patch_size, -1)\n",
        "    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, Hatt, Watt, Datt, -1)\n",
        "\n",
        "    # Downsample if necessary\n",
        "    if H != Hatt or W != Watt or D != Datt:\n",
        "        x = downsampler_fn(x.permute(0, 4, 1, 2, 3), [H, W, D]).permute(0, 2, 3, 4, 1)\n",
        "    return x\n",
        "\n",
        "class ViTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer Block.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 embed_dim: int,\n",
        "                 input_dims: List[int],\n",
        "                 num_heads: int,\n",
        "                 mlp_type: str,\n",
        "                 patch_size: int,\n",
        "                 mlp_ratio: float,\n",
        "                 qkv_bias: bool,\n",
        "                 qk_scale: Optional[float],\n",
        "                 drop: float,\n",
        "                 attn_drop: float,\n",
        "                 drop_path: float,\n",
        "                 act_layer: str,\n",
        "                 attention_type: str,\n",
        "                 norm_layer: Callable[..., nn.Module],\n",
        "                 layer_scale: Optional[float]):\n",
        "        super().__init__()\n",
        "        self.patch_size: int = patch_size\n",
        "        self.num_windows: int = prod_func([d // patch_size for d in input_dims])\n",
        "\n",
        "        self.norm1: nn.Module = norm_layer(embed_dim)\n",
        "        self.attn: nn.Module = Attention(\n",
        "            embed_dim,\n",
        "            attention_type=attention_type,\n",
        "            num_heads=num_heads,\n",
        "            patch_size=patch_size,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop,\n",
        "        )\n",
        "\n",
        "        self.drop_path: nn.Module = timm_DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2: nn.Module = norm_layer(embed_dim)\n",
        "        self.mlp: nn.Module = MLP(\n",
        "            in_feats=embed_dim,\n",
        "            hid_feats=int(embed_dim * mlp_ratio),\n",
        "            act_name=act_layer,\n",
        "            drop=drop,\n",
        "            MLP_type=mlp_type\n",
        "        )\n",
        "\n",
        "        self.layer_scale: bool = layer_scale is not None and isinstance(layer_scale, (int, float))\n",
        "        if self.layer_scale:\n",
        "            self.gamma1: nn.Parameter = nn.Parameter(layer_scale * torch.ones(embed_dim), requires_grad=True)\n",
        "            self.gamma2: nn.Parameter = nn.Parameter(layer_scale * torch.ones(embed_dim), requires_grad=True)\n",
        "        else:\n",
        "            self.gamma1: float = 1.0\n",
        "            self.gamma2: float = 1.0\n",
        "\n",
        "    def forward(self, x: Tensor, q_ms: Optional[Tensor]) -> Tensor:\n",
        "        B, H, W, D, C = x.size()\n",
        "        shortcut: Tensor = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        x_windows, Hatt, Watt, Datt = get_patches(x, self.patch_size)\n",
        "        x_windows = x_windows.view(-1, self.patch_size ** 3, C)\n",
        "\n",
        "        attn_windows: Tensor = self.attn(x_windows, q_ms)\n",
        "        x = get_image(attn_windows, self.patch_size, Hatt, Watt, Datt, H, W, D)\n",
        "        x = shortcut + self.drop_path(self.gamma1 * x)\n",
        "        x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch Embedding layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chans: int = 3, out_chans: int = 32,\n",
        "                 drop_rate: float = 0,\n",
        "                 kernel_size: int = 3,\n",
        "                 stride: int = 1, padding: int = 1,\n",
        "                 dilation: int = 1, groups: int = 1, bias: bool = False) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        Convnd: Type[nn.Module] = getattr(nn, f\"Conv{ndims}d\")\n",
        "        self.proj: nn.Module = Convnd(in_channels=in_chans, out_channels=out_chans,\n",
        "                              kernel_size=kernel_size,\n",
        "                              stride=stride, padding=padding,\n",
        "                              dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "        self.drop: nn.Module = nn.Dropout(p=drop_rate)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.drop(self.proj(x))\n",
        "        return x\n",
        "\n",
        "class ViTLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer Layer.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_type: str,\n",
        "        dim: int,\n",
        "        dim_out: int,\n",
        "        depth: int,\n",
        "        input_dims: List[int],\n",
        "        num_heads: int,\n",
        "        patch_size: int,\n",
        "        mlp_type: str,\n",
        "        mlp_ratio: float,\n",
        "        qkv_bias: bool,\n",
        "        qk_scale: Optional[float],\n",
        "        drop: float,\n",
        "        attn_drop: float,\n",
        "        drop_path: Union[float, List[float]],\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        norm_type: str,\n",
        "        layer_scale: Optional[float],\n",
        "        act_layer: str\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.patch_size: int = patch_size\n",
        "        self.embed_dim: int = dim\n",
        "        self.input_dims: List[int] = input_dims\n",
        "        self.blocks: nn.ModuleList = nn.ModuleList([\n",
        "            ViTBlock(\n",
        "                embed_dim=dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_type=mlp_type,\n",
        "                patch_size=patch_size,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                attention_type=attention_type,\n",
        "                drop=drop,\n",
        "                attn_drop=attn_drop,\n",
        "                drop_path=drop_path[k] if isinstance(drop_path, list) else drop_path,\n",
        "                act_layer=act_layer,\n",
        "                norm_layer=norm_layer,\n",
        "                layer_scale=layer_scale,\n",
        "                input_dims=input_dims\n",
        "            )\n",
        "            for k in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, inp: Tensor, q_ms: Optional[Tensor], CONCAT_ok: bool) -> Tensor:\n",
        "        x: Tensor = inp.clone()\n",
        "        x = rearrange(x, 'b c h w d -> b h w d c')\n",
        "\n",
        "        if q_ms is not None:\n",
        "            q_ms = rearrange(q_ms, 'b c h w d -> b h w d c')\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            if q_ms is None:\n",
        "                x = blk(x, None)\n",
        "            else:\n",
        "                q_ms_patches, _, _, _ = get_patches(q_ms, self.patch_size)\n",
        "                q_ms_patches = q_ms_patches.view(-1, self.patch_size ** ndims, x.size()[-1])\n",
        "                x = blk(x, q_ms_patches)\n",
        "\n",
        "        x = rearrange(x, 'b h w d c -> b c h w d')\n",
        "\n",
        "        if CONCAT_ok:\n",
        "            x = torch.cat((inp, x), dim=-1)\n",
        "        else:\n",
        "            x = inp + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) module for hierarchical feature processing.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 PYR_SCALES=None,\n",
        "                 feats_num=None,\n",
        "                 hid_dim=None,\n",
        "                 depths=None,\n",
        "                 patch_size=None,\n",
        "                 mlp_ratio=None,\n",
        "                 num_heads=None,\n",
        "                 mlp_type=None,\n",
        "                 norm_type=None,\n",
        "                 act_layer=None,\n",
        "                 drop_path_rate: float = 0.2,\n",
        "                 qkv_bias: bool = True,\n",
        "                 qk_scale: bool = None,\n",
        "                 drop_rate: float = 0.,\n",
        "                 attn_drop_rate: float = 0.,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 layer_scale=None,\n",
        "                 img_size=None,\n",
        "                 NUM_CROSS_ATT=-1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Determine the number of levels for processing\n",
        "        num_levels = len(feats_num)\n",
        "        num_levels = min(num_levels, NUM_CROSS_ATT) if NUM_CROSS_ATT > 0 else num_levels\n",
        "        if WO_SELF_ATT:\n",
        "            num_levels -= 1\n",
        "\n",
        "        # Ensure patch_size is a list\n",
        "        patch_size = patch_size if isinstance(patch_size, list) else [patch_size for _ in range(num_levels)]\n",
        "        hwd = img_size[-1]\n",
        "\n",
        "        # Create patch embedding layers\n",
        "        self.patch_embed = nn.ModuleList([\n",
        "            PatchEmbed(\n",
        "                in_chans=feats_num[i],\n",
        "                out_chans=hid_dim,\n",
        "                drop_rate=drop_rate\n",
        "            ) for i in range(num_levels)\n",
        "        ])\n",
        "\n",
        "        # Generate drop path rate for each layer\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        # Create ViT layers\n",
        "        self.levels = nn.ModuleList()\n",
        "        for i in range(num_levels):\n",
        "            level = ViTLayer(\n",
        "                dim=hid_dim,\n",
        "                dim_out=hid_dim,\n",
        "                depth=depths[i],\n",
        "                num_heads=num_heads[i],\n",
        "                patch_size=patch_size[i],\n",
        "                mlp_type=mlp_type,\n",
        "                attention_type=\"local\" if i == 0 else \"global\",\n",
        "                drop_path=dpr[sum(depths[:i]):sum(depths[:i+1])],\n",
        "                input_dims=img_size[i],\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                norm_layer=norm_layer,\n",
        "                layer_scale=layer_scale,\n",
        "                norm_type=norm_type,\n",
        "                act_layer=act_layer\n",
        "            )\n",
        "            self.levels.append(level)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        \"\"\"Initialize the weights of the module.\"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            timm_trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        \"\"\"Return keywords for no weight decay.\"\"\"\n",
        "        return {'rpb'}\n",
        "\n",
        "    def forward(self, KQs, CONCAT_ok: bool = False):\n",
        "        \"\"\"\n",
        "        Forward pass of the ViT module.\n",
        "\n",
        "        Args:\n",
        "            KQs (List[Tensor]): List of input tensors for each level.\n",
        "            CONCAT_ok (bool): Flag to determine if concatenation is allowed.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Processed output tensor.\n",
        "        \"\"\"\n",
        "        for i, (patch_embed_, level) in enumerate(zip(self.patch_embed, self.levels)):\n",
        "            if i == 0:\n",
        "                # First level: process input without cross-attention\n",
        "                Q = patch_embed_(KQs[i])\n",
        "                x = level(Q, None, CONCAT_ok=CONCAT_ok)\n",
        "                Q = patch_embed_(x)\n",
        "            else:\n",
        "                # Subsequent levels: process with cross-attention\n",
        "                K = patch_embed_(KQs[i])\n",
        "                x = level(Q, K, CONCAT_ok=CONCAT_ok)\n",
        "                Q = x.clone()\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderCnnBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional block for the encoder part of the network.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        stride,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "        affine=True,\n",
        "        eps=1e-05\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # First convolutional block\n",
        "        conv_block_1 = [\n",
        "            nn.Conv3d(\n",
        "                in_channels=in_channels, out_channels=out_channels,\n",
        "                kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                bias=bias\n",
        "            ),\n",
        "            nn.InstanceNorm3d(num_features=out_channels, affine=affine, eps=eps),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Second convolutional block\n",
        "        conv_block_2 = [\n",
        "            nn.Conv3d(\n",
        "                in_channels=out_channels, out_channels=out_channels,\n",
        "                kernel_size=kernel_size, stride=1, padding=padding,\n",
        "                bias=bias\n",
        "            ),\n",
        "            nn.InstanceNorm3d(num_features=out_channels, affine=affine, eps=eps),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Combine both blocks\n",
        "        self._block = nn.Sequential(\n",
        "            *conv_block_1,\n",
        "            *conv_block_2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of the EncoderCnnBlock.\"\"\"\n",
        "        return self._block(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder module for the hierarchical vision transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "        self._num_stages: int = config['num_stages']\n",
        "        self.use_seg: bool = config['use_seg_loss']\n",
        "\n",
        "        # Determine channels of encoder feature maps\n",
        "        encoder_out_channels: torch.Tensor = torch.tensor([config['start_channels'] * 2**stage for stage in range(self._num_stages)])\n",
        "\n",
        "        # Estimate required stages\n",
        "        required_stages: Set[int] = set(int(fmap[-1]) for fmap in config['out_fmaps'])\n",
        "        self._required_stages: Set[int] = required_stages\n",
        "\n",
        "        earliest_required_stage: int = min(required_stages)\n",
        "\n",
        "        # Lateral connections\n",
        "        lateral_in_channels: torch.Tensor = encoder_out_channels[earliest_required_stage:]\n",
        "        lateral_out_channels: torch.Tensor = lateral_in_channels.clip(max=config['fpn_channels'])\n",
        "\n",
        "        self._lateral: nn.ModuleList = nn.ModuleList([\n",
        "            nn.Conv3d(in_channels=in_ch, out_channels=out_ch, kernel_size=1)\n",
        "            for in_ch, out_ch in zip(lateral_in_channels, lateral_out_channels)\n",
        "        ])\n",
        "        self._lateral_levels: int = len(self._lateral)\n",
        "\n",
        "        # Output layers\n",
        "        out_in_channels: List[int] = [lateral_out_channels[-self._num_stages + required_stage].item() for required_stage in required_stages]\n",
        "        out_out_channels: List[int] = [int(config['fpn_channels'])] * len(out_in_channels)\n",
        "        out_out_channels[0] = int(config['fpn_channels'])\n",
        "\n",
        "        self._out: nn.ModuleList = nn.ModuleList([\n",
        "            nn.Conv3d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1)\n",
        "            for in_ch, out_ch in zip(out_in_channels, out_out_channels)\n",
        "        ])\n",
        "\n",
        "        # Upsampling layers\n",
        "        self._up: nn.ModuleList = nn.ModuleList([\n",
        "            nn.ConvTranspose3d(\n",
        "                in_channels=list(reversed(lateral_out_channels))[level],\n",
        "                out_channels=list(reversed(lateral_out_channels))[level+1],\n",
        "                kernel_size=list(reversed(config['strides']))[level],\n",
        "                stride=list(reversed(config['strides']))[level]\n",
        "            )\n",
        "            for level in range(len(lateral_out_channels)-1)\n",
        "        ])\n",
        "\n",
        "        # Multi-scale attention\n",
        "        self.hierarchical_dec: nn.ModuleList = self._create_hierarchical_layers(config, out_out_channels)\n",
        "\n",
        "        if self.use_seg:\n",
        "            self._seg_head: nn.ModuleList = nn.ModuleList([\n",
        "                nn.Conv3d(out_ch, config['num_organs'] + 1, kernel_size=1, stride=1)\n",
        "                for out_ch in out_out_channels\n",
        "            ])\n",
        "\n",
        "    def _create_hierarchical_layers(self, config: Dict[str, Any], out_out_channels: List[int]) -> nn.ModuleList:\n",
        "        \"\"\"Create hierarchical layers for multi-scale attention.\"\"\"\n",
        "        out: nn.ModuleList = nn.ModuleList()\n",
        "        img_size: List[List[int]] = []\n",
        "        feats_num: List[int] = []\n",
        "\n",
        "        for k, out_ch in enumerate(out_out_channels):\n",
        "            img_size.append([int(item/(2**(self._num_stages-k-1))) for item in config['data_size']])\n",
        "            feats_num.append(out_ch)\n",
        "            n: int = len(feats_num)\n",
        "\n",
        "            if k == 0:\n",
        "                out.append(nn.Identity())\n",
        "            else:\n",
        "                out.append(\n",
        "                    ViT(\n",
        "                        NUM_CROSS_ATT=config.get('NUM_CROSS_ATT', _NUM_CROSS_ATT),\n",
        "                        PYR_SCALES=[1.],\n",
        "                        feats_num=feats_num,\n",
        "                        hid_dim=int(config.get('fpn_channels', 64)),\n",
        "                        depths=[int(config.get('depths', 1))]*n,\n",
        "                        patch_size=[int(config.get('patch_size', 2))]*n,\n",
        "                        mlp_ratio=int(config.get('mlp_ratio', 2)),\n",
        "                        num_heads=[int(config.get('num_heads', 32))]*n,\n",
        "                        mlp_type='basic',\n",
        "                        norm_type='BatchNorm2d',\n",
        "                        act_layer='gelu',\n",
        "                        drop_path_rate=config.get('drop_path_rate', 0.2),\n",
        "                        qkv_bias=config.get('qkv_bias', True),\n",
        "                        qk_scale=None,\n",
        "                        drop_rate=config.get('drop_rate', 0.),\n",
        "                        attn_drop_rate=config.get('attn_drop_rate', 0.),\n",
        "                        norm_layer=nn.LayerNorm,\n",
        "                        layer_scale=1e-5,\n",
        "                        img_size=img_size\n",
        "                    )\n",
        "                )\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
        "        \"\"\"Forward pass of the Decoder.\"\"\"\n",
        "        lateral_out: List[Tensor] = [lateral(fmap) for lateral, fmap in zip(self._lateral, list(x.values())[-self._lateral_levels:])]\n",
        "\n",
        "        up_out: List[Tensor] = []\n",
        "        for idx, x in enumerate(reversed(lateral_out)):\n",
        "            if idx != 0:\n",
        "                x = x + up\n",
        "\n",
        "            if idx < self._lateral_levels - 1:\n",
        "                up = self._up[idx](x)\n",
        "\n",
        "            up_out.append(x)\n",
        "\n",
        "        cnn_outputs: Dict[int, Tensor] = {stage: self._out[idx](fmap) for idx, (fmap, stage) in enumerate(zip(reversed(up_out), self._required_stages))}\n",
        "        return self._forward_hierarchical(cnn_outputs)\n",
        "\n",
        "    def _forward_hierarchical(self, cnn_outputs: Dict[int, Tensor]) -> Dict[str, Tensor]:\n",
        "        \"\"\"Forward pass through the hierarchical decoder.\"\"\"\n",
        "        xs: List[Tensor] = [cnn_outputs[key].clone() for key in range(max(cnn_outputs.keys()), min(cnn_outputs.keys())-1, -1)]\n",
        "\n",
        "        out_dict: Dict[str, Tensor] = {}\n",
        "        QK: List[Tensor] = []\n",
        "        for i, key in enumerate(range(max(cnn_outputs.keys()), min(cnn_outputs.keys())-1, -1)):\n",
        "            QK = [xs[i]] + QK\n",
        "            if i == 0:\n",
        "                Pi = QK[0]\n",
        "            else:\n",
        "                Pi = self.hierarchical_dec[i](QK)\n",
        "            QK[0] = Pi\n",
        "            out_dict[f'P{key}'] = Pi\n",
        "\n",
        "            if self.use_seg:\n",
        "                Pi_seg = self._seg_head[i](Pi)\n",
        "                out_dict[f'S{key}'] = Pi_seg\n",
        "\n",
        "        return out_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class HierarchicalViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Hierarchical Vision Transformer (HViT) for image processing tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "\n",
        "        # Configuration parameters\n",
        "        self.backbone: str = config['backbone_net']\n",
        "        in_channels: int = 2 * config.get('in_channels', 1)  # source + target\n",
        "        kernel_size: int = config.get('kernel_size', 3)\n",
        "        emb_dim: int = config.get('start_channels', 32)\n",
        "        data_size: Tuple[int, ...] = config.get('data_size', [160, 192, 224])\n",
        "        self.out_fmaps: List[str] = config.get('out_fmaps', ['P4', 'P3', 'P2', 'P1'])\n",
        "\n",
        "        # Calculate number of stages\n",
        "        num_stages: int = min(int(math.log2(min(data_size))) - 1,\n",
        "                              max(int(fmap[-1]) for fmap in self.out_fmaps) + 1)\n",
        "\n",
        "        strides: List[int] = [1] + [2] * (num_stages - 1)\n",
        "        kernel_sizes: List[int] = [kernel_size] * num_stages\n",
        "\n",
        "        config['num_stages'] = num_stages\n",
        "        config['strides'] = strides\n",
        "\n",
        "        # Build encoder\n",
        "        self._encoder: nn.ModuleList = nn.ModuleList()\n",
        "        if self.backbone in ['fpn', 'FPN']:\n",
        "            for k in range(num_stages):\n",
        "                blk = EncoderCnnBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=emb_dim,\n",
        "                    kernel_size=kernel_sizes[k],\n",
        "                    stride=strides[k]\n",
        "                )\n",
        "                self._encoder.append(blk)\n",
        "\n",
        "                in_channels = emb_dim\n",
        "                emb_dim *= 2\n",
        "\n",
        "        # Build decoder\n",
        "        if self.backbone in ['fpn', 'FPN']:\n",
        "            self._decoder: Decoder = Decoder(config)\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Initialize model weights.\n",
        "        \"\"\"\n",
        "        # TODO: Implement weight initialization\n",
        "\n",
        "    def forward(self, x: Tensor, verbose: bool = False) -> Dict[str, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the HierarchicalViT model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor.\n",
        "            verbose (bool): If True, print shape information.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Tensor]: Output feature maps.\n",
        "        \"\"\"\n",
        "        down: Dict[str, Tensor] = {}\n",
        "        if self.backbone in ['fpn', 'FPN']:\n",
        "            for stage_id, module in enumerate(self._encoder):\n",
        "                x = module(x)\n",
        "                down[f'C{stage_id}'] = x\n",
        "            up = self._decoder(down)\n",
        "\n",
        "        if verbose:\n",
        "            for key, item in down.items():\n",
        "                print(f'down {key}', item.shape)\n",
        "            for key, item in up.items():\n",
        "                print(f'up {key}', item.shape)\n",
        "        return up\n",
        "\n",
        "\n",
        "class RegistrationHead(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Registration head for generating displacement fields.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
        "        super().__init__()\n",
        "        conv3d = nn.Conv3d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2\n",
        "        )\n",
        "        # Initialize weights with small random values\n",
        "        conv3d.weight = nn.Parameter(torch.zeros_like(conv3d.weight).normal_(0, 1e-5))\n",
        "        conv3d.bias = nn.Parameter(torch.zeros(conv3d.bias.shape))\n",
        "        self.add_module('conv3d', conv3d)\n",
        "\n",
        "\n",
        "class HViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Hierarchical Vision Transformer (HViT) model for image registration.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: dict):\n",
        "        super(HViT, self).__init__()\n",
        "        self.upsample_df: bool = config.get('upsample_df', False)\n",
        "        self.upsample_scale_factor: int = config.get('upsample_scale_factor', 2)\n",
        "        self.scale_level_df: str = config.get('scale_level_df', 'P1')\n",
        "\n",
        "        self.deformable: HierarchicalViT = HierarchicalViT(config)\n",
        "        self.avg_pool: nn.AvgPool3d = nn.AvgPool3d(3, stride=2, padding=1)\n",
        "        self.spatial_trans: SpatialTransformer = SpatialTransformer(config['data_size'])\n",
        "        self.reg_head: RegistrationHead = RegistrationHead(\n",
        "            in_channels=config.get('fpn_channels', 64),\n",
        "            out_channels=ndims,\n",
        "            kernel_size=ndims,\n",
        "        )\n",
        "\n",
        "    def forward(self, source: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the HViT model.\n",
        "\n",
        "        Args:\n",
        "            source (Tensor): Source image tensor.\n",
        "            target (Tensor): Target image tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Tensor, Tensor]: Moved image and displacement field.\n",
        "        \"\"\"\n",
        "        x: Tensor = torch.cat((source, target), dim=1)\n",
        "        x_dec: Dict[str, Tensor] = self.deformable(x)\n",
        "\n",
        "        # Extract features at the specified scale level\n",
        "        x_dec: Tensor = x_dec[self.scale_level_df]\n",
        "        flow: Tensor = self.reg_head(x_dec)\n",
        "\n",
        "        if self.upsample_df:\n",
        "            flow = nn.Upsample(scale_factor=self.upsample_scale_factor,\n",
        "                               mode='trilinear',\n",
        "                               align_corners=False)(flow)\n",
        "\n",
        "        moved: Tensor = self.spatial_trans(source, flow)\n",
        "        return moved, flow\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the HViT model\n",
        "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
        "    B = 1\n",
        "    H, W, D = 160//2, 192//2, 224//2\n",
        "\n",
        "    for fpn_channels in [64]:\n",
        "        config = {\n",
        "            'NUM_CROSS_ATT': _NUM_CROSS_ATT,\n",
        "            'out_fmaps': ['P4', 'P3', 'P2', 'P1'],\n",
        "            'scale_level_df': 'P1',\n",
        "            'upsample_df': True,\n",
        "            'upsample_scale_factor': 2,\n",
        "            'fpn_channels': fpn_channels,\n",
        "            'start_channels': 32,\n",
        "            'patch_size': 2,\n",
        "            'bspl': False,\n",
        "\n",
        "            'backbone_net': 'fpn',\n",
        "            'in_channels': 1,\n",
        "            'data_size': [H, W, D],\n",
        "            'bias': True,\n",
        "            'norm_type': 'instance',\n",
        "            'cuda': 0,\n",
        "            'kernel_size': 3,\n",
        "            'depths': 1,\n",
        "            'mlp_ratio': 2,\n",
        "\n",
        "            'num_heads': 32,\n",
        "            'drop_path_rate': 0.,\n",
        "            'qkv_bias': True,\n",
        "            'drop_rate': 0.,\n",
        "            'attn_drop_rate': 0.,\n",
        "\n",
        "            'use_seg_loss': False,\n",
        "            'use_seg_proxy_loss': False,\n",
        "            'num_organs': -1\n",
        "        }\n",
        "\n",
        "        source = torch.rand([1, 1, H, W, D])\n",
        "        tgt = torch.rand([1, 1, H, W, D])\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        model = HViT(config)\n",
        "        model.to(device)\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "            source = source.to(dtype=torch.float16).to(device)\n",
        "            tgt = tgt.to(dtype=torch.float16).to(device)\n",
        "\n",
        "            moved, flow = model(source, tgt)\n",
        "            print('\\n\\nmoved {} flow {}'.format(moved.shape, flow.shape))\n",
        "\n",
        "            max_mem_mb = torch.cuda.max_memory_allocated() / 1024**3\n",
        "            print(\"[+] Maximum memory:\\t{:.2f}GB: >>> \\t{:.0f} feats\".format(max_mem_mb, config['fpn_channels']) if max_mem_mb is not None else \"\")\n",
        "            print(\"[+] Required Total memory:\\t{:.2f}GB\".format(torch.cuda.get_device_properties(0).total_memory/1024**3))\n",
        "            print(\"[+] Trainable params:\\t{:.5f} m\".format(count_parameters(model)/1e6))"
      ],
      "metadata": {
        "id": "f8giI1U-uUZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "E4eSOBdDu1jH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from math import exp\n",
        "import math\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Grad3D(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    N-D gradient loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, penalty='l1', loss_mult=None):\n",
        "        super().__init__()\n",
        "        self.penalty = penalty\n",
        "        self.loss_mult = loss_mult\n",
        "\n",
        "    def forward(self, y_pred):\n",
        "        dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])\n",
        "        dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])\n",
        "        dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])\n",
        "\n",
        "        if self.penalty == 'l2':\n",
        "            dy = dy * dy\n",
        "            dx = dx * dx\n",
        "            dz = dz * dz\n",
        "\n",
        "        d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)\n",
        "        grad = d / 3.0\n",
        "\n",
        "        if self.loss_mult is not None:\n",
        "            grad *= self.loss_mult\n",
        "        return grad\n",
        "\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"Dice loss\"\"\"\n",
        "\n",
        "    def __init__(self, num_class=36):\n",
        "        super().__init__()\n",
        "        self.num_class = num_class\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_true = nn.functional.one_hot(y_true, num_classes=self.num_class)\n",
        "        y_true = torch.squeeze(y_true, 1)\n",
        "        y_true = y_true.permute(0, 4, 1, 2, 3).contiguous()\n",
        "        intersection = y_pred * y_true\n",
        "        intersection = intersection.sum(dim=[2, 3, 4])\n",
        "        union = torch.pow(y_pred, 2).sum(dim=[2, 3, 4]) + torch.pow(y_true, 2).sum(dim=[2, 3, 4])\n",
        "        dsc = (2.*intersection) / (union + 1e-5)\n",
        "        dsc_loss = (1-torch.mean(dsc))\n",
        "        return dsc_loss\n",
        "\n",
        "def DiceScore(y_pred, y_true, num_class):\n",
        "    y_true = nn.functional.one_hot(y_true, num_classes=num_class)\n",
        "    y_true = torch.squeeze(y_true, 1)\n",
        "    y_true = y_true.permute(0, 4, 1, 2, 3).contiguous()\n",
        "    intersection = y_pred * y_true\n",
        "    intersection = intersection.sum(dim=[2, 3, 4])\n",
        "    union = torch.pow(y_pred, 2).sum(dim=[2, 3, 4]) + torch.pow(y_true, 2).sum(dim=[2, 3, 4])\n",
        "    dsc = (2.*intersection) / (union + 1e-5)\n",
        "    return dsc\n",
        "\n",
        "\n",
        "loss_functions = {\n",
        "    \"mse\": nn.MSELoss(),\n",
        "    \"dice\": DiceLoss(),\n",
        "    \"grad\": Grad3D(penalty='l2')\n",
        "}\n"
      ],
      "metadata": {
        "id": "gvDXwAaJu2FR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "HUUO25cVu9uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import yaml\n",
        "from torch import nn\n",
        "\n",
        "def read_yaml_file(file_path):\n",
        "    \"\"\"\n",
        "    Reads a YAML file and returns the content as a dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): The path to the YAML file to read.\n",
        "\n",
        "    Returns:\n",
        "    dict: The content of the YAML file as a dictionary.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        try:\n",
        "            content = yaml.safe_load(file)  # Load the YAML file content\n",
        "            return content\n",
        "        except yaml.YAMLError as e:\n",
        "            print(f\"Error reading YAML file: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "    def __init__(self, save_dir):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        # Create handlers\n",
        "        console_handler = logging.StreamHandler()\n",
        "\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        file_handler = logging.FileHandler(os.path.join(save_dir, \"logfile.log\"))\n",
        "\n",
        "        # Create formatters and add it to handlers\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        console_handler.setFormatter(formatter)\n",
        "        file_handler.setFormatter(formatter)\n",
        "\n",
        "        # Add handlers to the logger\n",
        "        self.logger.addHandler(console_handler)\n",
        "        self.logger.addHandler(file_handler)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def info(self, message):\n",
        "        self.logger.info(message)\n",
        "\n",
        "    def warning(self, message):\n",
        "        self.logger.warning(message)\n",
        "\n",
        "    def error(self, message):\n",
        "        self.logger.error(message)\n",
        "\n",
        "    def debug(self, message):\n",
        "        self.logger.debug(message)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "\n",
        "def get_one_hot(inp_seg, num_labels):\n",
        "    B, C, H, W, D = inp_seg.shape\n",
        "    inp_onehot = nn.functional.one_hot(inp_seg.long(), num_classes=num_labels)\n",
        "    inp_onehot = inp_onehot.squeeze(dim=1)\n",
        "    inp_onehot = inp_onehot.permute(0, 4, 1, 2, 3).contiguous()\n",
        "    return inp_onehot\n"
      ],
      "metadata": {
        "id": "qqtmyMOUu-zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "6VMinE2Hu5V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from lightning import LightningModule\n",
        "\n",
        "from src import logger, checkpoints_dir\n",
        "from src.model.hvit import HViT\n",
        "from src.model.hvit_light import HViT_Light\n",
        "from src.loss import loss_functions, DiceScore\n",
        "from src.utils import get_one_hot\n",
        "\n",
        "dtype_map = {\n",
        "    'bf16': torch.bfloat16,\n",
        "    'fp32': torch.float32,\n",
        "    'fp16': torch.float16\n",
        "}\n",
        "\n",
        "class LiTHViT(LightningModule):\n",
        "    def __init__(self, args, config, wandb_logger=None, save_model_every_n_epochs=10):\n",
        "        super().__init__()\n",
        "        self.automatic_optimization = False\n",
        "        self.args = args\n",
        "        self.config = config\n",
        "        self.best_val_loss = 1e8\n",
        "        self.save_model_every_n_epochs = save_model_every_n_epochs\n",
        "        self.lr = args.lr\n",
        "        self.last_epoch = 0\n",
        "        self.tgt2src_reg = args.tgt2src_reg\n",
        "        self.hvit_light = args.hvit_light\n",
        "        self.precision = args.precision\n",
        "\n",
        "        self.hvit = HViT_Light(config) if self.hvit_light else HViT(config)\n",
        "\n",
        "        self.loss_weights = {\n",
        "            \"mse\": self.args.mse_weights,\n",
        "            \"dice\": self.args.dice_weights,\n",
        "            \"grad\": self.args.grad_weights\n",
        "        }\n",
        "        self.wandb_logger = wandb_logger\n",
        "        self.test_step_outputs = []\n",
        "\n",
        "    def _forward(self, batch, calc_score: bool = False, tgt2src_reg: bool = False):\n",
        "        _loss = {}\n",
        "        _score = 0.\n",
        "\n",
        "\n",
        "        dtype_ = dtype_map.get(self.precision, torch.float32)\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=dtype_):\n",
        "            if tgt2src_reg:\n",
        "                target, source = batch[0].to(dtype=dtype_), batch[1].to(dtype=dtype_)\n",
        "                tgt_seg, src_seg = batch[2], batch[3]\n",
        "            else:\n",
        "                source, target = batch[0].to(dtype=dtype_), batch[1].to(dtype=dtype_)\n",
        "                src_seg, tgt_seg = batch[2], batch[3]\n",
        "\n",
        "            moved, flow = self.hvit(source, target)\n",
        "\n",
        "            if calc_score:\n",
        "                moved_seg = self._get_one_hot_from_src(src_seg, flow, self.args.num_labels)\n",
        "                _score = DiceScore(moved_seg, tgt_seg.long(), self.args.num_labels)\n",
        "\n",
        "            _loss = {}\n",
        "            for key, weight in self.loss_weights.items():\n",
        "                if key == \"mse\":\n",
        "                    _loss[key] = weight * loss_functions[key](moved, target)\n",
        "                elif key == \"dice\":\n",
        "                    moved_seg = self._get_one_hot_from_src(src_seg, flow, self.args.num_labels)\n",
        "                    _loss[key] = weight * loss_functions[key](moved_seg, tgt_seg.long())\n",
        "                elif key == \"grad\":\n",
        "                    _loss[key] = weight * loss_functions[key](flow)\n",
        "\n",
        "            _loss[\"avg_loss\"] = sum(_loss.values()) / len(_loss)\n",
        "        return _loss, _score\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.hvit.train()\n",
        "        opt = self.optimizers()\n",
        "\n",
        "        loss1, _ = self._forward(batch, calc_score=False)\n",
        "        self.manual_backward(loss1[\"avg_loss\"])\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if self.tgt2src_reg:\n",
        "            loss2, _ = self._forward(batch, tgt2src_reg=True, calc_score=False)\n",
        "            self.manual_backward(loss2[\"avg_loss\"])\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        total_loss = {\n",
        "            key: (loss1[key].item() + loss2[key].item()) / 2 if self.tgt2src_reg and key in loss2 else loss1[key].item()\n",
        "            for key in loss1.keys()\n",
        "        }\n",
        "\n",
        "        self.wandb_logger.log_metrics(total_loss, step=self.global_step)\n",
        "        return total_loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        if self.current_epoch % self.save_model_every_n_epochs == 0:\n",
        "            checkpoint_path = f\"{checkpoints_dir}/model_epoch_{self.current_epoch}.ckpt\"\n",
        "            self.trainer.save_checkpoint(checkpoint_path)\n",
        "            logger.info(f\"Saved model at epoch {self.current_epoch}\")\n",
        "\n",
        "        current_lr = self.optimizers().param_groups[0]['lr']\n",
        "        self.wandb_logger.log_metrics({\"learning_rate\": current_lr}, step=self.global_step)\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        with torch.no_grad():\n",
        "            self.hvit.eval()\n",
        "            _loss, _score = self._forward(batch, calc_score=True)\n",
        "\n",
        "        # Log each component of the validation loss\n",
        "        for loss_name, loss_value in _loss.items():\n",
        "            self.log(f\"val_{loss_name}\", loss_value, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        # Log the mean validation score if available\n",
        "        if _score is not None:\n",
        "            self.log(\"val_score\", _score.mean(), on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        # Log to wandb\n",
        "        log_dict = {f\"val_{k}\": v.item() for k, v in _loss.items()}\n",
        "        log_dict.update({\n",
        "            \"val_score_mean\": _score.mean().item() if _score is not None else None,\n",
        "        })\n",
        "        self.wandb_logger.log_metrics({k: v for k, v in log_dict.items() if v is not None}, step=self.global_step)\n",
        "\n",
        "        return {\"val_loss\": _loss[\"avg_loss\"], \"val_score\": _score.mean().item()}\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Callback method called at the end of the validation epoch.\n",
        "        Saves the best model based on validation loss and logs metrics.\n",
        "        \"\"\"\n",
        "        val_loss = self.trainer.callback_metrics.get(\"val_loss\")\n",
        "\n",
        "        if val_loss is not None and self.current_epoch > 0:\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                best_model_path = f\"{checkpoints_dir}/best_model.ckpt\"\n",
        "                self.trainer.save_checkpoint(best_model_path)\n",
        "                self.wandb_logger.experiment.log({\n",
        "                    \"best_model_saved\": best_model_path,\n",
        "                    \"best_val_loss\": self.best_val_loss.item()\n",
        "                })\n",
        "                logger.info(f\"New best model saved with validation loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Performs a single test step on a batch of data.\n",
        "\n",
        "        Args:\n",
        "            batch: The input batch of data.\n",
        "            batch_idx: The index of the current batch.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the test Dice score.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.hvit.eval()\n",
        "            _, _score = self._forward(batch, calc_score=True)\n",
        "\n",
        "        # Ensure _score is a tensor and take the mean\n",
        "        _score = _score.mean() if isinstance(_score, torch.Tensor) else torch.tensor(_score).mean()\n",
        "\n",
        "        self.test_step_outputs.append(_score)\n",
        "\n",
        "        # Log to wandb only if the logger is available\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"test_dice\": _score.item()}, step=self.global_step)\n",
        "\n",
        "        # Return as a dict with tensor values\n",
        "        return {\"test_dice\": _score}\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Callback method called at the end of the test epoch.\n",
        "        Computes and logs the average test Dice score.\n",
        "        \"\"\"\n",
        "        # Calculate the average Dice score across all test steps\n",
        "        avg_test_dice = torch.stack(self.test_step_outputs).mean()\n",
        "\n",
        "        # Log the average test Dice score\n",
        "        self.log(\"avg_test_dice\", avg_test_dice, prog_bar=True)\n",
        "\n",
        "        # Log to wandb if available\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"total_test_dice_avg\": avg_test_dice.item()})\n",
        "\n",
        "        # Clear the test step outputs list for the next test epoch\n",
        "        self.test_step_outputs.clear()\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configures the optimizer and learning rate scheduler for the model.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the optimizer and learning rate scheduler configuration.\n",
        "        \"\"\"\n",
        "        optimizer = torch.optim.Adam(self.hvit.parameters(), lr=self.lr, weight_decay=0, amsgrad=True)\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=self.lr_lambda)\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"epoch\",\n",
        "                \"frequency\": 1,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def lr_lambda(self, epoch):\n",
        "        \"\"\"\n",
        "        Defines the learning rate schedule.\n",
        "\n",
        "        Args:\n",
        "            epoch: The current epoch number.\n",
        "\n",
        "        Returns:\n",
        "            The learning rate multiplier for the given epoch.\n",
        "        \"\"\"\n",
        "        max_epochs = self.trainer.max_epochs\n",
        "        return math.pow(1 - epoch / max_epochs, 0.9)\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_checkpoint(cls, checkpoint_path, args=None, wandb_logger=None):\n",
        "        \"\"\"\n",
        "        Loads a model from a checkpoint file.\n",
        "\n",
        "        Args:\n",
        "            checkpoint_path: Path to the checkpoint file.\n",
        "            args: Optional arguments to override saved ones.\n",
        "            wandb_logger: Optional WandB logger instance.\n",
        "\n",
        "        Returns:\n",
        "            An instance of the model loaded from the checkpoint.\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "        args = args or checkpoint.get('hyper_parameters', {}).get('args')\n",
        "        config = checkpoint.get('hyper_parameters', {}).get('config')\n",
        "\n",
        "        model = cls(args, config, wandb_logger)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "        if 'hyper_parameters' in checkpoint:\n",
        "            hyper_params = checkpoint['hyper_parameters']\n",
        "            for attr in ['lr', 'best_val_loss', 'last_epoch']:\n",
        "                setattr(model, attr, hyper_params.get(attr, getattr(model, attr)))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        \"\"\"\n",
        "        Callback to save additional information in the checkpoint.\n",
        "\n",
        "        Args:\n",
        "            checkpoint: The checkpoint dictionary to be saved.\n",
        "        \"\"\"\n",
        "        checkpoint['hyper_parameters'] = {\n",
        "            'config': self.config,\n",
        "            'lr': self.lr,\n",
        "            'best_val_loss': self.best_val_loss,\n",
        "            'last_epoch': self.current_epoch\n",
        "        }\n",
        "\n",
        "    def _get_one_hot_from_src(self, src_seg, flow, num_labels):\n",
        "        \"\"\"\n",
        "        Converts source segmentation to one-hot encoding and applies deformation.\n",
        "\n",
        "        Args:\n",
        "            src_seg: Source segmentation.\n",
        "            flow: Deformation flow.\n",
        "            num_labels: Number of segmentation labels.\n",
        "\n",
        "        Returns:\n",
        "            Deformed one-hot encoded segmentation.\n",
        "        \"\"\"\n",
        "        src_seg_onehot = get_one_hot(src_seg, self.args.num_labels)\n",
        "        deformed_segs = [\n",
        "            self.hvit.spatial_trans(src_seg_onehot[:, i:i+1, ...].float(), flow.float())\n",
        "            for i in range(num_labels)\n",
        "        ]\n",
        "        return torch.cat(deformed_segs, dim=1)\n"
      ],
      "metadata": {
        "id": "TBae9tu7u2bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "iD8FtixuvFPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path(__file__).resolve().parent.parent.parent))\n",
        "\n",
        "\n",
        "import argparse\n",
        "import wandb\n",
        "import torch\n",
        "from lightning import Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "# Add this line after the imports\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "\n",
        "from src import logger\n",
        "from src.trainer import LiTHViT\n",
        "from src.utils import read_yaml_file\n",
        "from src.data.datasets import get_dataloader\n",
        "\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Run training or inference\")\n",
        "    parser.add_argument(\"--num_gpus\", type=int, default='1', help=\"Number of GPUs to use. Use '-1' for all available GPUs.\")\n",
        "    parser.add_argument(\"--experiment_name\", type=str, default=\"OASIS\", help=\"Experiment name\")\n",
        "    parser.add_argument(\"--mode\", choices=[\"train\", \"inference\"], default=\"train\", help=\"Mode to run: train or inference\")\n",
        "    parser.add_argument(\"--train_data_path\", type=str, default=\"/dss/dssmcmlfs01/pr62la/pr62la-dss-0002/Mori/DATA/OASIS/OASIS_L2R_2021_task03/train\", help=\"Path to the train set\")\n",
        "    parser.add_argument(\"--val_data_path\", type=str, default=\"/dss/dssmcmlfs01/pr62la/pr62la-dss-0002/Mori/DATA/OASIS/OASIS_L2R_2021_task03/test\", help=\"Path to the validation set\")\n",
        "    parser.add_argument(\"--test_data_path\", type=str, default=\"/home/mori/HViT/OASIS_small/test\", help=\"Path to the test set\")\n",
        "    parser.add_argument(\"--checkpoint_path\", type=str, default=None, help=\"Path to the model/checkpoint_path to load\")\n",
        "    parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None, help=\"Path to the best model\")\n",
        "    parser.add_argument(\"--mse_weights\",type=float, default=1, help=\"MSE Loss weights\")\n",
        "    parser.add_argument(\"--dice_weights\", type=float, default=1, help=\"Dice Loss weights\")\n",
        "    parser.add_argument(\"--grad_weights\", type=float, default=0.02, help=\"Grad Loss weights\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=2, help=\"Batch size\")\n",
        "    parser.add_argument(\"--tgt2src_reg\", type=bool, default=True, help=\"target to source registration during training\")\n",
        "    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of workers\")\n",
        "    parser.add_argument(\"--max_epochs\", type=int, default=1000, help=\"Maximum number of epochs\")\n",
        "    parser.add_argument(\"--num_labels\", type=int, default=36, help=\"Number of labels\")\n",
        "    parser.add_argument(\"--precision\", type=str, default='bf16', help=\"Precision\")\n",
        "    parser.add_argument(\"--hvit_light\", type=bool, default=True, help=\"Use HViT-Light\")\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def main():\n",
        "    args = parse_arguments()\n",
        "    config = read_yaml_file(\"./config/config.yaml\")\n",
        "\n",
        "    # Initialize a single WandbLogger instance\n",
        "    wandb_logger = WandbLogger(project=\"wandb_HViT\", name=args.experiment_name)\n",
        "\n",
        "    # get dataloaders\n",
        "    train_dataloader = get_dataloader(data_path = args.train_data_path,\n",
        "                                      input_dim=config[\"data_size\"],\n",
        "                                      batch_size=args.batch_size,\n",
        "                                      is_pair=False)\n",
        "\n",
        "    val_dataloader = get_dataloader(data_path = args.val_data_path,\n",
        "                                    input_dim=config[\"data_size\"],\n",
        "                                    batch_size=args.batch_size,\n",
        "                                    shuffle = False,\n",
        "                                    is_pair=True)\n",
        "\n",
        "    # Determine number of GPUs to use\n",
        "    devices = min(int(args.num_gpus), torch.cuda.device_count()) if args.num_gpus > 0 else -1\n",
        "    print(f\"Using {devices} GPUs ...\")\n",
        "\n",
        "    # setup trainer for specified number of GPUs\n",
        "    trainer = Trainer(max_epochs=args.max_epochs,\n",
        "                      logger=[wandb_logger],\n",
        "                      precision=args.precision,\n",
        "                      accelerator=\"gpu\",\n",
        "                      devices=devices,\n",
        "                      strategy=\"ddp\" if devices > 1 else \"auto\")  # Use \"auto\" for single GPU\n",
        "\n",
        "    # train/test\n",
        "    if args.mode == \"train\":\n",
        "        if args.resume_from_checkpoint:\n",
        "            model = LiTHViT.load_from_checkpoint(args.resume_from_checkpoint, args=args, wandb_logger=wandb_logger)\n",
        "            print(f\"Resuming training from epoch {model.last_epoch + 1}\")\n",
        "        else:\n",
        "            model = LiTHViT(args, config, wandb_logger=wandb_logger)\n",
        "            print(\"Starting new training run\")\n",
        "        logger.info(\"Starting training\")\n",
        "        trainer.fit(model,\n",
        "                    train_dataloaders=train_dataloader,\n",
        "                    val_dataloaders=val_dataloader,\n",
        "                    datamodule=None,\n",
        "                    ckpt_path=args.resume_from_checkpoint)\n",
        "\n",
        "    elif args.mode == \"inference\":\n",
        "        logger.info(\"Starting inference\")\n",
        "\n",
        "        test_dataloader = get_dataloader(data_path = args.test_data_path,\n",
        "                                input_dim=config[\"data_size\"],\n",
        "                                is_pair=True,\n",
        "                                batch_size=args.batch_size,\n",
        "                                shuffle = False)\n",
        "\n",
        "\n",
        "        # # Get the latest checkpoint folder\n",
        "        # checkpoints_dir = Path(\"checkpoints\")\n",
        "        # checkpoints = sorted([d for d in checkpoints_dir.iterdir() if d.is_dir()], key=os.path.getctime, reverse=True)\n",
        "        # latest_checkpoint = checkpoints[1] if checkpoints else None\n",
        "\n",
        "        # if os.path.exists(latest_checkpoint):\n",
        "        #     logger.info(f\"Using latest checkpoint: {latest_checkpoint}\")\n",
        "\n",
        "        #     if args.checkpoint_path:\n",
        "        #         best_model_path = f\"{args.checkpoint_path}/best_model.ckpt\"\n",
        "        #     else:\n",
        "        #         best_model_path = f\"{latest_checkpoint}/best_model.ckpt\"\n",
        "\n",
        "        if args.checkpoint_path:\n",
        "            model = LiTHViT.load_from_checkpoint(args.checkpoint_path, args=args, wandb_logger=wandb_logger)\n",
        "            print(f\"Checkpoint loaded. Resuming from epoch {model.last_epoch + 1}\")\n",
        "        else:\n",
        "            raise Exception(\"No checkpoint found\")\n",
        "        trainer.test(model, dataloaders=test_dataloader)\n",
        "\n",
        "    # Finish the wandb run\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "uJ7DW4eovGSR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}