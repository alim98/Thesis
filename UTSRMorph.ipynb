{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0D9tmsOvecubdDk0o8ISy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/Thesis/blob/main/UTSRMorph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 1) INSTALL DEPENDENCIES\n",
        "################################################################################\n",
        "\n",
        "!pip install einops timm ml_collections\n",
        "\n",
        "################################################################################\n",
        "# 2) IMPORTS\n",
        "################################################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from timm.models.layers import DropPath, trunc_normal_, to_3tuple\n",
        "from einops import rearrange\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "# needed for the config dictionary\n",
        "import ml_collections\n",
        "from typing import Dict, List, Optional, Sequence, Tuple, Union"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvHoBircLynH",
        "outputId": "75f5c4f1-1c91-4e5b-e907-7d8662e3db7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
            "Collecting ml_collections\n",
            "  Downloading ml_collections-1.0.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
            "Downloading ml_collections-1.0.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml_collections\n",
            "Successfully installed ml_collections-1.0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOKrEcf7IHx7",
        "outputId": "5406c2f2-8a9d-416d-c85e-7f5d2bee21e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 1, 160, 192, 224])\n",
            "Flow shape:   torch.Size([1, 3, 160, 192, 224])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "################################################################################\n",
        "# 3) CONFIGS (merged from configs_UTSRMorph_dice.py as an example)\n",
        "################################################################################\n",
        "\n",
        "def get_UTSRMorph_config():\n",
        "    \"\"\"\n",
        "    Example config used for building the UTSRMorph model.\n",
        "    Customize as needed.\n",
        "    \"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.if_transskip = True   # whether to use Transformer skip connections\n",
        "    config.if_convskip = True    # whether to use Convolution skip connections\n",
        "    config.patch_size = 4\n",
        "    config.in_chans = 2\n",
        "    config.embed_dim = 96\n",
        "    config.depths = (2, 2, 2, 2)\n",
        "    config.num_heads = (4, 4, 4, 4)\n",
        "    config.window_size = (5, 6, 7)\n",
        "    config.mlp_ratio = 4\n",
        "    config.pat_merg_rf = 4\n",
        "    config.qkv_bias = False\n",
        "    config.drop_rate = 0\n",
        "    config.drop_path_rate = 0.3\n",
        "    config.ape = False\n",
        "    config.spe = False\n",
        "    config.rpe = True\n",
        "    config.patch_norm = True\n",
        "    config.use_checkpoint = False\n",
        "    config.out_indices = (0, 1, 2, 3)\n",
        "    config.reg_head_chan = 16\n",
        "    config.img_size = (160, 192, 224)\n",
        "    return config\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# 4) MODEL CODE (merged from UTSRMorph_dice.py and internal references)\n",
        "################################################################################\n",
        "\n",
        "############################\n",
        "# Channel Attention (CA)\n",
        "############################\n",
        "class CA(nn.Module):\n",
        "    \"\"\"Channel attention used in RCAN.\"\"\"\n",
        "    def __init__(self, num_feat, squeeze_factor=16):\n",
        "        super(CA, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool3d(1),\n",
        "            nn.Conv3d(num_feat, num_feat // squeeze_factor, 1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(num_feat // squeeze_factor, num_feat, 1, padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.attention(x)\n",
        "        return x * y\n",
        "\n",
        "############################\n",
        "# CAB: small Conv+CA block\n",
        "############################\n",
        "class CAB(nn.Module):\n",
        "    def __init__(self, num_feat, compress_ratio=3, squeeze_factor=30):\n",
        "        super(CAB, self).__init__()\n",
        "        self.cab = nn.Sequential(\n",
        "            nn.Conv3d(num_feat, num_feat // compress_ratio, 3, 1, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv3d(num_feat // compress_ratio, num_feat, 3, 1, 1),\n",
        "            CA(num_feat, squeeze_factor)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cab(x)\n",
        "\n",
        "############################\n",
        "# Basic MLP\n",
        "############################\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
        "                 act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "############################\n",
        "# Patch partition & reverse\n",
        "############################\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    x: (B, H, W, L, C)\n",
        "    window_size: (Wx, Wy, Wz)\n",
        "    \"\"\"\n",
        "    B, H, W, L, C = x.shape\n",
        "    Wx, Wy, Wz = window_size\n",
        "    # reshape\n",
        "    x = x.view(B, H // Wx, Wx, W // Wy, Wy, L // Wz, Wz, C)\n",
        "    # permute + flatten\n",
        "    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous()\n",
        "    windows = windows.view(-1, Wx, Wy, Wz, C)\n",
        "    return windows\n",
        "\n",
        "def window_reverse(windows, window_size, H, W, L):\n",
        "    \"\"\"\n",
        "    windows: (num_windows*B, Wx, Wy, Wz, C)\n",
        "    window_size: (Wx, Wy, Wz)\n",
        "    \"\"\"\n",
        "    Wx, Wy, Wz = window_size\n",
        "    B = int(windows.shape[0] / (H * W * L / Wx / Wy / Wz))\n",
        "    x = windows.view(B, H // Wx, W // Wy, L // Wz, Wx, Wy, Wz, -1)\n",
        "    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous()\n",
        "    x = x.view(B, H, W, L, -1)\n",
        "    return x\n",
        "\n",
        "############################\n",
        "# 3D Unfold Helpers (for OAB)\n",
        "############################\n",
        "def filter_dilated_rows(tensor: torch.Tensor,\n",
        "                        dilation: Tuple[int, int, int],\n",
        "                        dilated_kernel_size: Tuple[int, int, int],\n",
        "                        kernel_size: Tuple[int, int, int]) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Helper to remove extra rows from dilation.\n",
        "    \"\"\"\n",
        "    kernel_rank = len(kernel_size)\n",
        "    indices_to_keep = [\n",
        "        list(range(0, dilated_kernel_size[i], dilation[i])) for i in range(kernel_rank)\n",
        "    ]\n",
        "    tensor_np = tensor.cpu().numpy()  # to numpy\n",
        "    axis_offset = len(tensor.shape) - kernel_rank\n",
        "\n",
        "    for dim in range(kernel_rank):\n",
        "        tensor_np = np.take(tensor_np, indices_to_keep[dim], axis=axis_offset + dim)\n",
        "\n",
        "    return torch.from_numpy(tensor_np).to(tensor.device)\n",
        "\n",
        "def unfold3d(tensor: torch.Tensor,\n",
        "             kernel_size: Union[int, Tuple[int,int,int]],\n",
        "             padding: Union[int, Tuple[int,int,int]]=0,\n",
        "             stride: Union[int, Tuple[int,int,int]]=1,\n",
        "             dilation: Union[int, Tuple[int,int,int]]=1):\n",
        "    \"\"\"\n",
        "    3D version of Torch's unfold operation.\n",
        "    \"\"\"\n",
        "    if len(tensor.shape) != 5:\n",
        "        raise ValueError(f\"Input must be 5D [B, C, D, H, W]. Got {tensor.shape}\")\n",
        "\n",
        "    if isinstance(kernel_size, int):\n",
        "        kernel_size = (kernel_size, kernel_size, kernel_size)\n",
        "    if isinstance(padding, int):\n",
        "        padding = (padding, padding, padding)\n",
        "    if isinstance(stride, int):\n",
        "        stride = (stride, stride, stride)\n",
        "    if isinstance(dilation, int):\n",
        "        dilation = (dilation, dilation, dilation)\n",
        "\n",
        "    # Pad first\n",
        "    B, C, D, H, W = tensor.shape\n",
        "    pad_d, pad_h, pad_w = padding\n",
        "    tensor = F.pad(tensor, (pad_w, pad_w, pad_h, pad_h, pad_d, pad_d))\n",
        "\n",
        "    # Effective kernel size with dilation\n",
        "    dilated_kernel = (\n",
        "        kernel_size[0] + (kernel_size[0]-1)*(dilation[0]-1),\n",
        "        kernel_size[1] + (kernel_size[1]-1)*(dilation[1]-1),\n",
        "        kernel_size[2] + (kernel_size[2]-1)*(dilation[2]-1),\n",
        "    )\n",
        "\n",
        "    # unfold\n",
        "    tensor = tensor.unfold(2, dilated_kernel[0], stride[0])\n",
        "    tensor = tensor.unfold(3, dilated_kernel[1], stride[1])\n",
        "    tensor = tensor.unfold(4, dilated_kernel[2], stride[2])\n",
        "\n",
        "    # remove extraneous rows if dilation > 1\n",
        "    if dilation != (1,1,1):\n",
        "        tensor = filter_dilated_rows(tensor, dilation, dilated_kernel, kernel_size)\n",
        "\n",
        "    # rearrange\n",
        "    tensor = tensor.permute(0,2,3,4,1,5,6,7)\n",
        "    # shape: (B, D_out, H_out, W_out, C, kD, kH, kW)\n",
        "    tensor = tensor.reshape(B, -1, C*np.prod(kernel_size)).transpose(1,2)\n",
        "    # shape: (B, D_out*H_out*W_out, C*kD*kH*kW)\n",
        "    return tensor\n",
        "\n",
        "############################\n",
        "# OAB: Overlapping Attention\n",
        "############################\n",
        "class OAB(nn.Module):\n",
        "    \"\"\"\n",
        "    Overlapping cross-attention block:\n",
        "      - Queries from standard local windows\n",
        "      - Keys/Values from larger overlapping window\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, window_size, overlap_ratio, num_heads,\n",
        "                 qkv_bias=True, qk_scale=None, mlp_ratio=2,\n",
        "                 norm_layer=nn.LayerNorm, rpe=True):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim**-0.5\n",
        "        self.overlap_win_size = int(window_size * overlap_ratio) + window_size\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
        "\n",
        "        # relative position bias table\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((window_size + self.overlap_win_size - 1)**3, num_heads)\n",
        "        )\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        self.register_buffer(\"relative_position_index_OAB\",\n",
        "            self._create_rpe_index(window_size, self.overlap_win_size))\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(dim, mlp_hidden_dim, act_layer=nn.GELU)\n",
        "        self.H = self.W = self.T = None\n",
        "        self.rpe = rpe\n",
        "\n",
        "    def _create_rpe_index(self, window_size, window_size_ext):\n",
        "        \"\"\"\n",
        "        Build relative position indices for standard window vs extended (overlapping) window.\n",
        "        \"\"\"\n",
        "        ws = window_size\n",
        "        wse = window_size_ext\n",
        "        coords_h = torch.arange(ws)\n",
        "        coords_w = torch.arange(ws)\n",
        "        coords_t = torch.arange(ws)\n",
        "        coords_ori = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t]))\n",
        "        coords_ori_flat = coords_ori.flatten(1)  # shape (3, ws^3)\n",
        "\n",
        "        coords_he = torch.arange(wse)\n",
        "        coords_we = torch.arange(wse)\n",
        "        coords_te = torch.arange(wse)\n",
        "        coords_ext = torch.stack(torch.meshgrid([coords_he, coords_we, coords_te]))\n",
        "        coords_ext_flat = coords_ext.flatten(1)  # shape (3, wse^3)\n",
        "\n",
        "        rel_coords = coords_ext_flat[:,None,:] - coords_ori_flat[:,:,None]  # (3, ws^3, wse^3)\n",
        "        rel_coords = rel_coords.permute(1,2,0).contiguous()  # (ws^3, wse^3, 3)\n",
        "\n",
        "        # shift start to 0\n",
        "        rel_coords[...,0] += ws - wse + 1\n",
        "        rel_coords[...,1] += ws - wse + 1\n",
        "        rel_coords[...,2] += ws - wse + 1\n",
        "\n",
        "        # flatten to a single index\n",
        "        factor = (ws + wse - 1)*(ws + wse - 1)\n",
        "        rel_coords[...,0] *= factor\n",
        "        rel_coords[...,1] *= (ws + wse - 1)\n",
        "        rel_index = rel_coords.sum(-1)  # (ws^3, wse^3)\n",
        "        return rel_index.view(-1)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        x shape: (B, H*W*T, C)\n",
        "        \"\"\"\n",
        "        B, L, C = x.shape\n",
        "        H, W, T = self.H, self.W, self.T\n",
        "        assert L == H*W*T\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, T, C)\n",
        "\n",
        "        # pad to multiples of self.window_size\n",
        "        pad_r = (self.window_size - H % self.window_size) % self.window_size\n",
        "        pad_b = (self.window_size - W % self.window_size) % self.window_size\n",
        "        pad_h = (self.window_size - T % self.window_size) % self.window_size\n",
        "        x = F.pad(x, (0,0, 0,pad_h, 0,pad_b, 0,pad_r))\n",
        "        _, Hp, Wp, Tp, _ = x.shape\n",
        "\n",
        "        # Q,K,V\n",
        "        qkv = self.qkv(x).reshape(B, Hp, Wp, Tp, 3, C).permute(4,0,5,1,2,3)\n",
        "        # shape: (3, B, C, Hp, Wp, Tp)\n",
        "        q = qkv[0].permute(0,2,3,4,1)  # (B, Hp, Wp, Tp, C)\n",
        "        kv = torch.cat([qkv[1], qkv[2]], dim=1)  # (B, 2*C, Hp, Wp, Tp)\n",
        "\n",
        "        # partition Q into standard windows\n",
        "        q_windows = window_partition(q, (self.window_size,)*3)  # (nW*B, ws, ws, ws, C)\n",
        "        q_windows = q_windows.view(-1, self.window_size**3, C)\n",
        "\n",
        "        # unfold K,V in bigger overlapping windows\n",
        "        kv_windows = unfold3d(kv, kernel_size=self.overlap_win_size,\n",
        "                              stride=self.window_size,\n",
        "                              padding=(self.window_size)//2)\n",
        "        # kv_windows shape: (B, D_out*H_out*W_out, 2*C*(ow^3)) ???\n",
        "\n",
        "        # rearrange into separate (k, v)\n",
        "        # note: we must reshape carefully\n",
        "        # let's do: (b (nW)) in one dimension, and (ow^3, c) in another\n",
        "        nc=2; ow=self.overlap_win_size; ch=C\n",
        "        kv_windows = rearrange(\n",
        "            kv_windows,\n",
        "            'b (nc ch owh oww owt) nw -> nc (b nw) (owh oww owt) ch',\n",
        "            nc=nc, ch=ch, owh=ow, oww=ow, owt=ow\n",
        "        )\n",
        "        k_windows, v_windows = kv_windows[0], kv_windows[1]\n",
        "\n",
        "        # Multi-head attention\n",
        "        b_, nq, _ = q_windows.shape\n",
        "        _, n_, _ = k_windows.shape\n",
        "        d = self.dim // self.num_heads\n",
        "\n",
        "        q = q_windows.reshape(b_, nq, self.num_heads, d).permute(0, 2, 1, 3)\n",
        "        k = k_windows.reshape(b_, n_, self.num_heads, d).permute(0, 2, 1, 3)\n",
        "        v = v_windows.reshape(b_, n_, self.num_heads, d).permute(0, 2, 1, 3)\n",
        "\n",
        "        q = q*self.scale\n",
        "        attn = q @ k.transpose(-2, -1)\n",
        "\n",
        "        # add relative position bias\n",
        "        if self.rpe:\n",
        "            rpb = self.relative_position_bias_table[self.relative_position_index_OAB.view(-1)]\n",
        "            # rpb shape: (ws^3 * wse^3, num_heads)\n",
        "            # but we have actual shapes: ws^3 -> self.window_size**3\n",
        "            # wse^3 -> self.overlap_win_size**3\n",
        "            size_q = self.window_size**3\n",
        "            size_k = self.overlap_win_size**3\n",
        "            rpb = rpb.view(size_q, size_k, self.num_heads).permute(2,0,1)\n",
        "            # shape: (num_heads, ws^3, wse^3)\n",
        "            attn = attn + rpb.unsqueeze(0)\n",
        "\n",
        "        attn = self.softmax(attn)\n",
        "        out = (attn @ v).transpose(1,2).reshape(b_, nq, self.dim)\n",
        "\n",
        "        # merge small windows back\n",
        "        out = out.view(-1, self.window_size, self.window_size, self.window_size, C)\n",
        "        out = window_reverse(out, (self.window_size,)*3, Hp, Wp, Tp)\n",
        "\n",
        "        # unpad\n",
        "        if pad_r>0 or pad_b>0 or pad_h>0:\n",
        "            out = out[:, :H, :W, :T, :].contiguous()\n",
        "\n",
        "        out = out.view(B, H*W*T, C)\n",
        "        out = self.proj(out) + shortcut\n",
        "\n",
        "        # final MLP\n",
        "        out = out + self.mlp(self.norm2(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "############################\n",
        "# WindowAttention: W-MSA\n",
        "############################\n",
        "class WindowAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Window-based multi-head self attention (W-MSA) with optional relative position bias.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None,\n",
        "                 rpe=True, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim**-0.5\n",
        "        self.rpe = rpe\n",
        "\n",
        "        # relative pos bias table\n",
        "        table_size = (2*window_size[0]-1)*(2*window_size[1]-1)*(2*window_size[2]-1)\n",
        "        self.relative_position_bias_table = nn.Parameter(torch.zeros(table_size, num_heads))\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "\n",
        "        # build relative index\n",
        "        coords_h = torch.arange(window_size[0])\n",
        "        coords_w = torch.arange(window_size[1])\n",
        "        coords_t = torch.arange(window_size[2])\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t])) # (3, wH, wW, wT)\n",
        "        coords_flat = coords.flatten(1) # shape (3, wH*wW*wT)\n",
        "\n",
        "        if rpe:\n",
        "            rel = coords_flat[:,:,None] - coords_flat[:,None,:] # (3, n, n)\n",
        "            rel = rel.permute(1,2,0).contiguous() # (n, n, 3)\n",
        "            # shift\n",
        "            rel[...,0] += window_size[0]-1\n",
        "            rel[...,1] += window_size[1]-1\n",
        "            rel[...,2] += window_size[2]-1\n",
        "            # 3D flatten\n",
        "            pos_factor = (2*window_size[1]-1)*(2*window_size[2]-1)\n",
        "            rel[...,0] *= pos_factor\n",
        "            rel[...,1] *= (2*window_size[2]-1)\n",
        "            rel_index = rel.sum(-1)\n",
        "            self.register_buffer(\"relative_position_index\", rel_index)\n",
        "\n",
        "        # qkv\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        x: (nW*B, window_size^3, C)\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(\n",
        "            B_, N, 3, self.num_heads, C//self.num_heads\n",
        "        ).permute(2,0,3,1,4)  # shape: (3, B_, num_heads, N, d)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q = q*self.scale\n",
        "        attn = q @ k.transpose(-2, -1)  # (B_, num_heads, N, N)\n",
        "\n",
        "        if self.rpe:\n",
        "            # add rpb\n",
        "            table_size = self.window_size[0]*self.window_size[1]*self.window_size[2]\n",
        "            rpb = self.relative_position_bias_table[\n",
        "                self.relative_position_index.view(-1)\n",
        "            ].view(table_size, table_size, self.num_heads)\n",
        "            rpb = rpb.permute(2,0,1) # (num_heads, N, N)\n",
        "            attn = attn + rpb.unsqueeze(0)  # shape broadcast\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_//nW, nW, self.num_heads, N, N)\n",
        "            attn = attn + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = self.softmax(attn)\n",
        "        else:\n",
        "            attn = self.softmax(attn)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn@v).transpose(1,2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "############################\n",
        "# SwinTransformerBlock -> FAB\n",
        "############################\n",
        "class FAB(nn.Module):\n",
        "    \"\"\"\n",
        "    Fusion Attention Block = W-MSA (or SW-MSA) + local convolution (CAB).\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, window_size=(7,7,7), shift_size=(0,0,0),\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, rpe=True,\n",
        "                 drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = WindowAttention(dim, window_size, num_heads, qkv_bias, qk_scale,\n",
        "                                    rpe, attn_drop, drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path>0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden = int(dim*mlp_ratio)\n",
        "        self.mlp = Mlp(dim, mlp_hidden, act_layer=act_layer, drop=drop)\n",
        "\n",
        "        # local conv\n",
        "        self.conv_block = CAB(num_feat=dim, compress_ratio=3, squeeze_factor=30)\n",
        "        self.H = self.W = self.T = None\n",
        "\n",
        "    def forward(self, x, mask_matrix):\n",
        "        \"\"\"\n",
        "        x: (B, H*W*T, C)\n",
        "        \"\"\"\n",
        "        H, W, T = self.H, self.W, self.T\n",
        "        B, L, C = x.shape\n",
        "        assert L == H*W*T\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x_3d = x.view(B, H, W, T, C)\n",
        "\n",
        "        # local conv\n",
        "        conv_x = self.conv_block(x_3d.permute(0,4,1,2,3))  # (B,C,H,W,T)\n",
        "        conv_x = conv_x.permute(0,2,3,4,1).contiguous().view(B,H*W*T,C)\n",
        "\n",
        "        # pad for shifting\n",
        "        pad_r = (self.window_size[0]-H%self.window_size[0])%self.window_size[0]\n",
        "        pad_b = (self.window_size[1]-W%self.window_size[1])%self.window_size[1]\n",
        "        pad_h = (self.window_size[2]-T%self.window_size[2])%self.window_size[2]\n",
        "        x_3d = F.pad(x_3d, (0,0, 0,pad_h, 0,pad_b, 0,pad_r))\n",
        "        Hp, Wp, Tp = x_3d.shape[1], x_3d.shape[2], x_3d.shape[3]\n",
        "\n",
        "        # shift\n",
        "        if any(self.shift_size):\n",
        "            shifted_x = torch.roll(x_3d, shifts=(-self.shift_size[0],\n",
        "                                                 -self.shift_size[1],\n",
        "                                                 -self.shift_size[2]),\n",
        "                                   dims=(1,2,3))\n",
        "            attn_mask = mask_matrix\n",
        "        else:\n",
        "            shifted_x = x_3d\n",
        "            attn_mask = None\n",
        "\n",
        "        # partition\n",
        "        x_windows = window_partition(shifted_x, self.window_size)\n",
        "        x_windows = x_windows.view(-1, self.window_size[0]*self.window_size[1]*self.window_size[2], C)\n",
        "\n",
        "        # W-MSA or SW-MSA\n",
        "        attn_windows = self.attn(x_windows, attn_mask)\n",
        "\n",
        "        # reverse windows\n",
        "        attn_windows = attn_windows.view(-1, *self.window_size, C)\n",
        "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp, Tp)\n",
        "\n",
        "        # reverse shift\n",
        "        if any(self.shift_size):\n",
        "            x_3d = torch.roll(shifted_x, shifts=(self.shift_size[0],\n",
        "                                                 self.shift_size[1],\n",
        "                                                 self.shift_size[2]),\n",
        "                              dims=(1,2,3))\n",
        "        else:\n",
        "            x_3d = shifted_x\n",
        "\n",
        "        if pad_r>0 or pad_b>0 or pad_h>0:\n",
        "            x_3d = x_3d[:,:H,:W,:T,:].contiguous()\n",
        "\n",
        "        # reshape back\n",
        "        x_attn = x_3d.view(B, H*W*T, C)\n",
        "\n",
        "        # sum up\n",
        "        x = shortcut + self.drop_path(x_attn) + conv_x*0.01\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "############################\n",
        "# Patch Merging\n",
        "############################\n",
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, dim, norm_layer=nn.LayerNorm, reduce_factor=2):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.reduction = nn.Linear(8*dim, (8//reduce_factor)*dim, bias=False)\n",
        "        self.norm = norm_layer(8*dim)\n",
        "\n",
        "    def forward(self, x, H, W, T):\n",
        "        B, L, C = x.shape\n",
        "        assert L==H*W*T\n",
        "        assert H%2==0 and W%2==0 and T%2==0\n",
        "\n",
        "        x_3d = x.view(B,H,W,T,C)\n",
        "        x0 = x_3d[:, 0::2, 0::2, 0::2, :]\n",
        "        x1 = x_3d[:, 1::2, 0::2, 0::2, :]\n",
        "        x2 = x_3d[:, 0::2, 1::2, 0::2, :]\n",
        "        x3 = x_3d[:, 0::2, 0::2, 1::2, :]\n",
        "        x4 = x_3d[:, 1::2, 1::2, 0::2, :]\n",
        "        x5 = x_3d[:, 0::2, 1::2, 1::2, :]\n",
        "        x6 = x_3d[:, 1::2, 0::2, 1::2, :]\n",
        "        x7 = x_3d[:, 1::2, 1::2, 1::2, :]\n",
        "\n",
        "        x_cat = torch.cat([x0,x1,x2,x3,x4,x5,x6,x7], dim=-1)  # (B,H/2,W/2,T/2, 8*C)\n",
        "        x_cat = x_cat.view(B, -1, 8*C)\n",
        "        x_cat = self.norm(x_cat)\n",
        "        x_cat = self.reduction(x_cat)\n",
        "        return x_cat\n",
        "\n",
        "############################\n",
        "# BasicLayer: stack of FAB + OAB\n",
        "############################\n",
        "class BasicLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A stage in the Swin-like architecture.\n",
        "    Repeats FAB blocks and then an Overlapping Attention Block (OAB).\n",
        "    Optionally, does PatchMerging at the end.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, depth, num_heads, window_size=(7,7,7),\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, rpe=True,\n",
        "                 drop=0., attn_drop=0., drop_path=0.,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 downsample=None,\n",
        "                 use_checkpoint=False,\n",
        "                 pat_merg_rf=2):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = (window_size[0]//2, window_size[1]//2, window_size[2]//2)\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.blocks = nn.ModuleList([\n",
        "            FAB(dim, num_heads, window_size,\n",
        "                (0,0,0) if (i%2==0) else self.shift_size,\n",
        "                mlp_ratio, qkv_bias, qk_scale, rpe, drop, attn_drop,\n",
        "                drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                norm_layer=norm_layer)\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.overlap_attn = OAB(dim, window_size=4, overlap_ratio=0.5,\n",
        "                                num_heads=4, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                mlp_ratio=mlp_ratio, norm_layer=norm_layer)\n",
        "\n",
        "        if downsample is not None:\n",
        "            self.downsample = downsample(dim, norm_layer, reduce_factor=pat_merg_rf)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x, H, W, T):\n",
        "        \"\"\"\n",
        "        x: (B, H*W*T, C)\n",
        "        \"\"\"\n",
        "        # create the SW-MSA mask\n",
        "        Hp = int(math.ceil(H/self.window_size[0]))*self.window_size[0]\n",
        "        Wp = int(math.ceil(W/self.window_size[1]))*self.window_size[1]\n",
        "        Tp = int(math.ceil(T/self.window_size[2]))*self.window_size[2]\n",
        "        img_mask = torch.zeros((1, Hp, Wp, Tp, 1), device=x.device)\n",
        "\n",
        "        h_slices = (slice(0,-self.window_size[0]),\n",
        "                    slice(-self.window_size[0], -self.shift_size[0]),\n",
        "                    slice(-self.shift_size[0], None))\n",
        "        w_slices = (slice(0,-self.window_size[1]),\n",
        "                    slice(-self.window_size[1], -self.shift_size[1]),\n",
        "                    slice(-self.shift_size[1], None))\n",
        "        t_slices = (slice(0,-self.window_size[2]),\n",
        "                    slice(-self.window_size[2], -self.shift_size[2]),\n",
        "                    slice(-self.shift_size[2], None))\n",
        "        cnt=0\n",
        "        for hh in h_slices:\n",
        "            for ww in w_slices:\n",
        "                for tt in t_slices:\n",
        "                    img_mask[:, hh, ww, tt, :] = cnt\n",
        "                    cnt+=1\n",
        "\n",
        "        mask_windows = window_partition(img_mask, self.window_size)\n",
        "        mask_windows = mask_windows.view(-1, self.window_size[0]*self.window_size[1]*self.window_size[2])\n",
        "        attn_mask = mask_windows.unsqueeze(1)-mask_windows.unsqueeze(2)\n",
        "        attn_mask = attn_mask.masked_fill(attn_mask!=0, float(-100.0)).masked_fill(attn_mask==0, float(0.0))\n",
        "\n",
        "        # pass blocks\n",
        "        for blk in self.blocks:\n",
        "            blk.H, blk.W, blk.T = H, W, T\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x, attn_mask)\n",
        "            else:\n",
        "                x = blk(x, attn_mask)\n",
        "\n",
        "        # OverlapAttn\n",
        "        self.overlap_attn.H, self.overlap_attn.W, self.overlap_attn.T = H, W, T\n",
        "        if self.use_checkpoint:\n",
        "            x = checkpoint.checkpoint(self.overlap_attn, x, None)\n",
        "        else:\n",
        "            x = self.overlap_attn(x, None)\n",
        "\n",
        "        # Patch Merging?\n",
        "        if self.downsample is not None:\n",
        "            x_down = self.downsample(x, H, W, T)\n",
        "            Wh, Ww, Wt = (H+1)//2, (W+1)//2, (T+1)//2\n",
        "            return x, H, W, T, x_down, Wh, Ww, Wt\n",
        "        else:\n",
        "            return x, H, W, T, x, H, W, T\n",
        "\n",
        "############################\n",
        "# PatchEmbed\n",
        "############################\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
        "        super().__init__()\n",
        "        patch_size = to_3tuple(patch_size)\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer is not None else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W, T = x.shape\n",
        "        ps = self.patch_size\n",
        "        # pad if needed\n",
        "        if T%ps[2]!=0:\n",
        "            x = F.pad(x, (0, ps[2]-T%ps[2]))\n",
        "        if W%ps[1]!=0:\n",
        "            x = F.pad(x, (0,0, 0, ps[1]-W%ps[1]))\n",
        "        if H%ps[0]!=0:\n",
        "            x = F.pad(x, (0,0, 0,0, 0, ps[0]-H%ps[0]))\n",
        "        x = self.proj(x)\n",
        "        # x shape: (B, embed_dim, H/ps, W/ps, T/ps)\n",
        "        if self.norm is not None:\n",
        "            Hp, Wp, Tp = x.shape[2], x.shape[3], x.shape[4]\n",
        "            x = x.flatten(2).transpose(1,2)  # (B, n, C)\n",
        "            x = self.norm(x)\n",
        "            x = x.transpose(1,2).view(B, self.embed_dim, Hp, Wp, Tp)\n",
        "        return x\n",
        "\n",
        "############################\n",
        "# The main SwinTransformer\n",
        "############################\n",
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self, pretrain_img_size=224, patch_size=4, in_chans=3,\n",
        "                 embed_dim=96, depths=(2,2,6,2), num_heads=(3,6,12,24),\n",
        "                 window_size=(7,7,7), mlp_ratio=4.,\n",
        "                 qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.2, norm_layer=nn.LayerNorm, ape=False, spe=False,\n",
        "                 rpe=True, patch_norm=True, out_indices=(0,1,2,3),\n",
        "                 frozen_stages=-1, use_checkpoint=False, pat_merg_rf=2):\n",
        "        super().__init__()\n",
        "        self.num_layers = len(depths)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ape = ape\n",
        "        self.spe = spe\n",
        "        self.rpe = rpe\n",
        "        self.patch_norm = patch_norm\n",
        "        self.out_indices = out_indices\n",
        "        self.frozen_stages = frozen_stages\n",
        "\n",
        "        # patch embed\n",
        "        self.patch_embed = PatchEmbed(patch_size, in_chans, embed_dim,\n",
        "                                      norm_layer if patch_norm else None)\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # absolute pos embedding\n",
        "        if self.ape:\n",
        "            pass  # not used in these configs\n",
        "        elif self.spe:\n",
        "            # sinusoidal 3D pos embed\n",
        "            self.pos_embd = SinPositionalEncoding3D(embed_dim).cuda()\n",
        "\n",
        "        # drop path schedule\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        # build layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(self.num_layers):\n",
        "            layer = BasicLayer(dim=int(embed_dim*(2**i)),\n",
        "                               depth=depths[i],\n",
        "                               num_heads=num_heads[i],\n",
        "                               window_size=window_size,\n",
        "                               mlp_ratio=mlp_ratio,\n",
        "                               qkv_bias=qkv_bias,\n",
        "                               qk_scale=qk_scale,\n",
        "                               rpe=rpe,\n",
        "                               drop=drop_rate,\n",
        "                               attn_drop=attn_drop_rate,\n",
        "                               drop_path=dpr[sum(depths[:i]):sum(depths[:i+1])],\n",
        "                               norm_layer=norm_layer,\n",
        "                               downsample=PatchMerging if i<self.num_layers-1 else None,\n",
        "                               use_checkpoint=use_checkpoint,\n",
        "                               pat_merg_rf=pat_merg_rf)\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        # final out dims\n",
        "        num_features = [int(embed_dim*(2**i)) for i in range(self.num_layers)]\n",
        "        self.num_features = num_features\n",
        "\n",
        "        # norms for outputs\n",
        "        for i_layer in out_indices:\n",
        "            layer_ = norm_layer(num_features[i_layer])\n",
        "            self.add_module(f\"norm{i_layer}\", layer_)\n",
        "\n",
        "        self._freeze_stages()\n",
        "\n",
        "    def _freeze_stages(self):\n",
        "        if self.frozen_stages>=0:\n",
        "            self.patch_embed.eval()\n",
        "            for p in self.patch_embed.parameters():\n",
        "                p.requires_grad=False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # patch embed\n",
        "        x = self.patch_embed(x)\n",
        "        B, C, Hp, Wp, Tp = x.shape\n",
        "\n",
        "        if self.ape:\n",
        "            pass  # skipping usage\n",
        "        elif self.spe:\n",
        "            x = (x + self.pos_embd(x)).flatten(2).transpose(1,2)\n",
        "        else:\n",
        "            x = x.flatten(2).transpose(1,2)\n",
        "\n",
        "        x = self.pos_drop(x)  # (B, n, C)\n",
        "        outs = []\n",
        "        # forward stages\n",
        "        for i in range(self.num_layers):\n",
        "            layer = self.layers[i]\n",
        "            x_out, H, W, T, x, Wh, Ww, Wt = layer(x, Hp, Wp, Tp)\n",
        "            # store\n",
        "            if i in self.out_indices:\n",
        "                norm_layer = getattr(self, f\"norm{i}\")\n",
        "                x_out = norm_layer(x_out)\n",
        "                out_f = x_out.view(B, H, W, T, self.num_features[i]).permute(0,4,1,2,3)\n",
        "                outs.append(out_f)\n",
        "            # update resolution\n",
        "            Hp, Wp, Tp = Wh, Ww, Wt\n",
        "        return outs\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        super().train(mode)\n",
        "        self._freeze_stages()\n",
        "\n",
        "############################\n",
        "# Sinusoidal (optionally)\n",
        "############################\n",
        "class SinPositionalEncoding3D(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        channels = int(np.ceil(channels/6)*2)\n",
        "        if channels%2:\n",
        "            channels+=1\n",
        "        self.channels = channels\n",
        "        self.inv_freq = 1. / (10000**(torch.arange(0, channels, 2).float()/channels))\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        # expecting (B, C, X, Y, Z)\n",
        "        # we transpose to (B, X, Y, Z, C)\n",
        "        tensor = tensor.permute(0,2,3,4,1)\n",
        "        B, X, Y, Z, C = tensor.shape\n",
        "        pos_x = torch.arange(X, device=tensor.device).type(self.inv_freq.type())\n",
        "        pos_y = torch.arange(Y, device=tensor.device).type(self.inv_freq.type())\n",
        "        pos_z = torch.arange(Z, device=tensor.device).type(self.inv_freq.type())\n",
        "\n",
        "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
        "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
        "        sin_inp_z = torch.einsum(\"i,j->ij\", pos_z, self.inv_freq)\n",
        "\n",
        "        emb_x = torch.cat([sin_inp_x.sin(), sin_inp_x.cos()], dim=-1).unsqueeze(1).unsqueeze(1)\n",
        "        emb_y = torch.cat([sin_inp_y.sin(), sin_inp_y.cos()], dim=-1).unsqueeze(1)\n",
        "        emb_z = torch.cat([sin_inp_z.sin(), sin_inp_z.cos()], dim=-1)\n",
        "\n",
        "        emb = torch.zeros((X,Y,Z,self.channels*3), device=tensor.device, dtype=tensor.dtype)\n",
        "        emb[:,:,:, :self.channels] = emb_x\n",
        "        emb[:,:,:, self.channels:2*self.channels] = emb_y\n",
        "        emb[:,:,:, 2*self.channels:] = emb_z\n",
        "        emb = emb[None, :,:,:,:C].repeat(B,1,1,1,1)\n",
        "        return emb.permute(0,4,1,2,3)\n",
        "\n",
        "############################\n",
        "# PixelShuffle3D\n",
        "############################\n",
        "class PixelShuffle3d(nn.Module):\n",
        "    def __init__(self, scale):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, inp):\n",
        "        b, c, d, h, w = inp.size()\n",
        "        oc = c // (self.scale**3)\n",
        "        od, oh, ow = d*self.scale, h*self.scale, w*self.scale\n",
        "        out = inp.view(b, oc, self.scale, self.scale, self.scale, d, h, w)\n",
        "        out = out.permute(0,1,5,2,6,3,7,4).contiguous()\n",
        "        out = out.view(b, oc, od, oh, ow)\n",
        "        return out\n",
        "\n",
        "############################\n",
        "# ConvergeHead (used for up)\n",
        "############################\n",
        "class ConvergeHead(nn.Module):\n",
        "    def __init__(self, in_dim, up_ratio, kernel_size, padding):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.up_ratio = up_ratio\n",
        "        self.conv = nn.Conv3d(in_dim, (up_ratio**3)*in_dim, kernel_size, 1, padding, 1, in_dim)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hp = self.conv(x)\n",
        "        poxel = PixelShuffle3d(self.up_ratio)\n",
        "        hp = poxel(hp)\n",
        "        return hp\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv3d, nn.Linear)):\n",
        "            nn.init.normal_(m.weight, std=0.001)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm3d):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "############################\n",
        "# SR block for upsampling\n",
        "############################\n",
        "class Conv3dReLU(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1, use_batchnorm=True):\n",
        "        conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
        "        relu = nn.LeakyReLU(inplace=True)\n",
        "        nm = nn.BatchNorm3d(out_channels) if use_batchnorm else nn.InstanceNorm3d(out_channels)\n",
        "        super().__init__(conv, nm, relu)\n",
        "\n",
        "class SR(nn.Module):\n",
        "    \"\"\"\n",
        "    Super-resolution upsampling block (PixelShuffle3D).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, skip_channels=0, use_batchnorm=True):\n",
        "        super().__init__()\n",
        "        self.up = ConvergeHead(in_channels, 2, 3, 1)  # 2x up, kernel=3\n",
        "        self.conv1 = Conv3dReLU(in_channels+skip_channels, out_channels, 3, 1, use_batchnorm=use_batchnorm)\n",
        "        self.conv2 = Conv3dReLU(out_channels, out_channels, 3, 1, use_batchnorm=use_batchnorm)\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        x = self.up(x)\n",
        "        if skip is not None:\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "############################\n",
        "# Registration Head\n",
        "############################\n",
        "class RegistrationHead(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n",
        "        conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
        "        conv3d.weight = nn.Parameter(Normal(0,1e-5).sample(conv3d.weight.shape))\n",
        "        conv3d.bias = nn.Parameter(torch.zeros(conv3d.bias.shape))\n",
        "        super().__init__(conv3d)\n",
        "\n",
        "############################\n",
        "# Spatial Transformer\n",
        "############################\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    N-D Spatial Transformer (via grid_sample).\n",
        "    \"\"\"\n",
        "    def __init__(self, size, mode='bilinear'):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        vectors = [torch.arange(0, s) for s in size]\n",
        "        grids = torch.meshgrid(vectors, indexing='ij' if hasattr(torch, 'meshgrid') else None)\n",
        "        grid = torch.stack(grids)\n",
        "        grid = grid.unsqueeze(0).float()  # (1, ndim, D, H, W)\n",
        "        self.register_buffer('grid', grid)\n",
        "\n",
        "    def forward(self, src, flow):\n",
        "        \"\"\"\n",
        "        src: (B, C, D, H, W)\n",
        "        flow: (B, 3, D, H, W)\n",
        "        \"\"\"\n",
        "        new_locs = self.grid + flow\n",
        "        shape = flow.shape[2:]\n",
        "\n",
        "        # normalize to [-1,1]\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:, i, ...] = 2*(new_locs[:, i, ...]/(shape[i]-1) - 0.5)\n",
        "\n",
        "        if len(shape)==3:\n",
        "            new_locs = new_locs.permute(0,2,3,4,1)\n",
        "            new_locs = new_locs[..., [2,1,0]]\n",
        "        return F.grid_sample(src, new_locs, align_corners=False, mode=self.mode)\n",
        "\n",
        "############################\n",
        "# UTSRMorph Model\n",
        "############################\n",
        "class UTSRMorph(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.if_convskip = config.if_convskip\n",
        "        self.if_transskip = config.if_transskip\n",
        "        embed_dim = config.embed_dim\n",
        "\n",
        "        # 1) Swin Transformer as encoder\n",
        "        self.transformer = SwinTransformer(\n",
        "            patch_size=config.patch_size,\n",
        "            in_chans=config.in_chans,\n",
        "            embed_dim=config.embed_dim,\n",
        "            depths=config.depths,\n",
        "            num_heads=config.num_heads,\n",
        "            window_size=config.window_size,\n",
        "            mlp_ratio=config.mlp_ratio,\n",
        "            qkv_bias=config.qkv_bias,\n",
        "            drop_rate=config.drop_rate,\n",
        "            drop_path_rate=config.drop_path_rate,\n",
        "            ape=config.ape,\n",
        "            spe=config.spe,\n",
        "            rpe=config.rpe,\n",
        "            patch_norm=config.patch_norm,\n",
        "            use_checkpoint=config.use_checkpoint,\n",
        "            out_indices=config.out_indices,\n",
        "            pat_merg_rf=config.pat_merg_rf\n",
        "        )\n",
        "\n",
        "        # 2) Decoder (SR blocks)\n",
        "        self.up0 = SR(embed_dim*8, embed_dim*4,\n",
        "                      skip_channels=(embed_dim*4 if self.if_transskip else 0),\n",
        "                      use_batchnorm=False)\n",
        "        self.up1 = SR(embed_dim*4, embed_dim*2,\n",
        "                      skip_channels=(embed_dim*2 if self.if_transskip else 0),\n",
        "                      use_batchnorm=False)\n",
        "        self.up2 = SR(embed_dim*2, embed_dim,\n",
        "                      skip_channels=(embed_dim if self.if_transskip else 0),\n",
        "                      use_batchnorm=False)\n",
        "        self.up3 = SR(embed_dim,\n",
        "                      config.reg_head_chan,\n",
        "                      skip_channels=(embed_dim//2 if self.if_convskip else 0),\n",
        "                      use_batchnorm=False)\n",
        "\n",
        "        # optional conv skip\n",
        "        self.c1 = Conv3dReLU(2, embed_dim//2, 3, 1, use_batchnorm=False)\n",
        "\n",
        "        # 3) Head\n",
        "        self.reg_head = RegistrationHead(config.reg_head_chan, 3, 3)\n",
        "        self.spatial_trans = SpatialTransformer(config.img_size)\n",
        "        self.avg_pool = nn.AvgPool3d(3, stride=2, padding=1)\n",
        "        self.up = ConvergeHead(3, 2, 3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (B, 2, D, H, W)  # 2 -> [moving, fixed]\n",
        "        \"\"\"\n",
        "        B, C, D, H, W = x.shape\n",
        "        # separate the 'moving' image for final warp\n",
        "        source = x[:, :1, ...]  # shape: (B,1,D,H,W)\n",
        "\n",
        "        # if conv skip\n",
        "        if self.if_convskip:\n",
        "            x_s1 = self.avg_pool(x)         # downsample\n",
        "            f4 = self.c1(x_s1)              # (B, embed_dim//2, D/2, H/2, W/2)\n",
        "        else:\n",
        "            f4 = None\n",
        "\n",
        "        # run Swin Transformer\n",
        "        out_feats = self.transformer(x)  # a list of 4 feature maps (lowest to highest resolution)\n",
        "\n",
        "        # handle skip from the transformer's multi-scale features\n",
        "        if self.if_transskip:\n",
        "            f1 = out_feats[-2]  # second last scale\n",
        "            f2 = out_feats[-3]\n",
        "            f3 = out_feats[-4]\n",
        "        else:\n",
        "            f1 = f2 = f3 = None\n",
        "\n",
        "        # decode / upsample\n",
        "        x = self.up0(out_feats[-1], f1)\n",
        "        x = self.up1(x, f2)\n",
        "        x = self.up2(x, f3)\n",
        "        x = self.up3(x, f4)\n",
        "\n",
        "        # final reg head\n",
        "        flow = self.reg_head(x)   # (B,3,D,H,W) at final resolution\n",
        "        flow = self.up(flow)      # 2x up if needed\n",
        "        out = self.spatial_trans(source, flow)  # warp the moving image\n",
        "        return out, flow\n",
        "\n",
        "################################################################################\n",
        "# 5) QUICK TEST\n",
        "################################################################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create config\n",
        "    config = get_UTSRMorph_config()\n",
        "    # Build model\n",
        "    model = UTSRMorph(config).cuda()\n",
        "    model.eval()\n",
        "\n",
        "    # Create a dummy input with shape (B=1, 2 channels, D,H,W)\n",
        "    # matching config.img_size = (160,192,224)\n",
        "    # so the input is (1,2,160,192,224)\n",
        "    dummy = torch.randn(1, 2, 160, 192, 224).cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out, flow = model(dummy)\n",
        "\n",
        "    print(\"Output shape:\", out.shape)  # expect (1, 1, 160,192,224)\n",
        "    print(\"Flow shape:  \", flow.shape) # expect (1, 3, 160,192,224)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 0) INSTALL DEPENDENCIES\n",
        "################################################################################\n",
        "\n",
        "!pip install einops timm ml_collections surface-distance\n",
        "\n",
        "################################################################################\n",
        "# 1) IMPORTS\n",
        "################################################################################\n",
        "\n",
        "import os, sys, math, glob, random, pickle, datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from natsort import natsorted\n",
        "from typing import Dict, List, Optional, Sequence, Tuple, Union\n",
        "from math import exp\n",
        "import scipy.ndimage\n",
        "import torch.optim as optim\n",
        "\n",
        "# third-party\n",
        "import ml_collections\n",
        "from einops import rearrange\n",
        "from timm.models.layers import DropPath, trunc_normal_, to_3tuple\n",
        "from surface_distance import compute_surface_distances, compute_robust_hausdorff, compute_dice_coefficient\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# 2) UTILS: pkload, register_model, AverageMeter, dice_val, etc.\n",
        "#    (Merged from 'utils.py')\n",
        "################################################################################\n",
        "\n",
        "def pkload(fname):\n",
        "    with open(fname, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.vals = []\n",
        "        self.std = 0\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        self.vals.append(val)\n",
        "        self.std = np.std(self.vals)\n",
        "\n",
        "def pad_image(img, target_size):\n",
        "    rows_to_pad = max(target_size[0] - img.shape[2], 0)\n",
        "    cols_to_pad = max(target_size[1] - img.shape[3], 0)\n",
        "    slcs_to_pad = max(target_size[2] - img.shape[4], 0)\n",
        "    padded_img = F.pad(img, (0, slcs_to_pad, 0, cols_to_pad, 0, rows_to_pad), \"constant\", 0)\n",
        "    return padded_img\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    N-D Spatial Transformer, originally from Voxelmorph.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, mode='bilinear'):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        vectors = [torch.arange(0, s) for s in size]\n",
        "        grids = torch.meshgrid(vectors)\n",
        "        grid = torch.stack(grids)\n",
        "        grid = torch.unsqueeze(grid, 0)\n",
        "        grid = grid.type(torch.FloatTensor).cuda()\n",
        "        self.register_buffer('grid', grid)\n",
        "    def forward(self, src, flow):\n",
        "        new_locs = self.grid + flow\n",
        "        shape = flow.shape[2:]\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n",
        "        if len(shape) == 2:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 1)\n",
        "            new_locs = new_locs[..., [1, 0]]\n",
        "        elif len(shape) == 3:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 4, 1)\n",
        "            new_locs = new_locs[..., [2, 1, 0]]\n",
        "        return F.grid_sample(src, new_locs, align_corners=True, mode=self.mode)\n",
        "\n",
        "class register_model(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple wrapper to warp an image with a displacement field.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=(64, 256, 256), mode='bilinear'):\n",
        "        super(register_model, self).__init__()\n",
        "        self.spatial_trans = SpatialTransformer(img_size, mode)\n",
        "    def forward(self, x):\n",
        "        img = x[0].cuda()\n",
        "        flow = x[1].cuda()\n",
        "        out = self.spatial_trans(img, flow)\n",
        "        return out\n",
        "\n",
        "def dice_val_VOI(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    DSC for labels [1..35], ignoring 0\n",
        "    \"\"\"\n",
        "    VOI_lbls = list(range(1, 36))\n",
        "    pred = y_pred.detach().cpu().numpy()[0, 0, ...]\n",
        "    true = y_true.detach().cpu().numpy()[0, 0, ...]\n",
        "    DSCs = np.zeros((len(VOI_lbls)))\n",
        "    idx = 0\n",
        "    for i in VOI_lbls:\n",
        "        pred_i = (pred == i)\n",
        "        true_i = (true == i)\n",
        "        inter = np.sum(pred_i * true_i)\n",
        "        union = np.sum(pred_i) + np.sum(true_i)\n",
        "        dsc = 2.0*inter/(union + 1e-5)\n",
        "        DSCs[idx] = dsc\n",
        "        idx += 1\n",
        "    return np.mean(DSCs)\n",
        "\n",
        "def dice_val(y_pred, y_true, num_clus):\n",
        "    \"\"\"\n",
        "    Standard DSC, ignoring background?\n",
        "    \"\"\"\n",
        "    y_pred = nn.functional.one_hot(y_pred, num_classes=num_clus)\n",
        "    y_pred = torch.squeeze(y_pred, 1).permute(0,4,1,2,3).contiguous()\n",
        "    y_true = nn.functional.one_hot(y_true, num_classes=num_clus)\n",
        "    y_true = torch.squeeze(y_true, 1).permute(0,4,1,2,3).contiguous()\n",
        "    intersection = y_pred * y_true\n",
        "    intersection = intersection.sum(dim=[2,3,4])\n",
        "    union = y_pred.sum(dim=[2,3,4]) + y_true.sum(dim=[2,3,4])\n",
        "    dsc = (2.*intersection)/(union+1e-5)\n",
        "    return torch.mean(torch.mean(dsc, dim=1))\n",
        "\n",
        "################################################################################\n",
        "# 3) LOSSES (Merged from 'losses.py')\n",
        "################################################################################\n",
        "\n",
        "class Grad3d(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    N-D gradient loss, from VoxelMorph\n",
        "    \"\"\"\n",
        "    def __init__(self, penalty='l1', loss_mult=None):\n",
        "        super(Grad3d, self).__init__()\n",
        "        self.penalty = penalty\n",
        "        self.loss_mult = loss_mult\n",
        "    def forward(self, y_pred, y_true):\n",
        "        dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])\n",
        "        dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])\n",
        "        dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])\n",
        "        if self.penalty == 'l2':\n",
        "            dy = dy*dy\n",
        "            dx = dx*dx\n",
        "            dz = dz*dz\n",
        "        d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)\n",
        "        grad = d/3.0\n",
        "        if self.loss_mult is not None:\n",
        "            grad *= self.loss_mult\n",
        "        return grad\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Dice loss if needed\n",
        "    \"\"\"\n",
        "    def __init__(self, num_class=36):\n",
        "        super().__init__()\n",
        "        self.num_class = num_class\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_true = nn.functional.one_hot(y_true, num_classes=self.num_class)\n",
        "        y_true = torch.squeeze(y_true, 1).permute(0,4,1,2,3).contiguous()\n",
        "        intersection = y_pred*y_true\n",
        "        intersection = intersection.sum(dim=[2,3,4])\n",
        "        union = y_pred.pow(2).sum(dim=[2,3,4]) + y_true.pow(2).sum(dim=[2,3,4])\n",
        "        dsc = 2.*intersection/(union+1e-5)\n",
        "        dsc = (1-torch.mean(dsc))\n",
        "        return dsc\n",
        "\n",
        "class NCC_vxm(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Local normalized cross correlation (VoxelMorph)\n",
        "    \"\"\"\n",
        "    def __init__(self, win=None):\n",
        "        super(NCC_vxm, self).__init__()\n",
        "        self.win = win\n",
        "    def forward(self, y_true, y_pred):\n",
        "        Ii = y_true\n",
        "        Ji = y_pred\n",
        "        ndims = len(list(Ii.size())) - 2\n",
        "        assert ndims in [1,2,3]\n",
        "        win = [9]*ndims if self.win is None else self.win\n",
        "        sum_filt = torch.ones([1,1,*win]).to(\"cuda\")\n",
        "        pad_no = math.floor(win[0]/2)\n",
        "        if ndims==1:\n",
        "            stride=(1,)\n",
        "            padding=(pad_no,)\n",
        "        elif ndims==2:\n",
        "            stride=(1,1)\n",
        "            padding=(pad_no,pad_no)\n",
        "        else:\n",
        "            stride=(1,1,1)\n",
        "            padding=(pad_no,pad_no,pad_no)\n",
        "\n",
        "        conv_fn = getattr(F, 'conv%dd' % ndims)\n",
        "        I2 = Ii*Ii\n",
        "        J2 = Ji*Ji\n",
        "        IJ = Ii*Ji\n",
        "        I_sum = conv_fn(Ii, sum_filt, stride=stride, padding=padding)\n",
        "        J_sum = conv_fn(Ji, sum_filt, stride=stride, padding=padding)\n",
        "        I2_sum = conv_fn(I2, sum_filt, stride=stride, padding=padding)\n",
        "        J2_sum = conv_fn(J2, sum_filt, stride=stride, padding=padding)\n",
        "        IJ_sum = conv_fn(IJ, sum_filt, stride=stride, padding=padding)\n",
        "        win_size = np.prod(win)\n",
        "        u_I = I_sum/win_size\n",
        "        u_J = J_sum/win_size\n",
        "        cross = IJ_sum - u_J*I_sum - u_I*J_sum + u_I*u_J*win_size\n",
        "        I_var = I2_sum - 2*u_I*I_sum + u_I*u_I*win_size\n",
        "        J_var = J2_sum - 2*u_J*J_sum + u_J*u_J*win_size\n",
        "        cc = cross*cross/(I_var*J_var+1e-5)\n",
        "        return -torch.mean(cc)\n",
        "\n",
        "################################################################################\n",
        "# 4) SWIN TRANSFORMER + UTSRMorph MODEL (Single-Notebook Version)\n",
        "################################################################################\n",
        "\n",
        "#\n",
        "# We copy the single-notebook code from your previous instructions,\n",
        "# then paste here. For brevity, we rename them as \"SwinTransformer\" and \"UTSRMorph\".\n",
        "#\n",
        "\n",
        "# ---- config example (like get_UTSRMorph_config)\n",
        "def get_UTSRMorph_config():\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.if_transskip = True\n",
        "    config.if_convskip = True\n",
        "    config.patch_size = 4\n",
        "    config.in_chans = 2\n",
        "    config.embed_dim = 96\n",
        "    config.depths = (2,2,2,2)\n",
        "    config.num_heads = (4,4,4,4)\n",
        "    config.window_size = (5,6,7)\n",
        "    config.mlp_ratio = 4\n",
        "    config.pat_merg_rf = 4\n",
        "    config.qkv_bias = False\n",
        "    config.drop_rate = 0\n",
        "    config.drop_path_rate = 0.3\n",
        "    config.ape = False\n",
        "    config.spe = False\n",
        "    config.rpe = True\n",
        "    config.patch_norm = True\n",
        "    config.use_checkpoint = False\n",
        "    config.out_indices = (0,1,2,3)\n",
        "    config.reg_head_chan = 16\n",
        "    config.img_size = (160,192,224)\n",
        "    return config\n",
        "\n",
        "# --- basic building blocks for the model (Swin parts) ---\n",
        "class CA(nn.Module):\n",
        "    def __init__(self, num_feat, squeeze_factor=16):\n",
        "        super(CA, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool3d(1),\n",
        "            nn.Conv3d(num_feat, num_feat//squeeze_factor, 1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(num_feat//squeeze_factor, num_feat, 1, padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        y = self.attention(x)\n",
        "        return x*y\n",
        "\n",
        "class CAB(nn.Module):\n",
        "    def __init__(self, num_feat, compress_ratio=3, squeeze_factor=30):\n",
        "        super(CAB, self).__init__()\n",
        "        self.cab = nn.Sequential(\n",
        "            nn.Conv3d(num_feat, num_feat//compress_ratio, 3,1,1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv3d(num_feat//compress_ratio, num_feat, 3,1,1),\n",
        "            CA(num_feat, squeeze_factor)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.cab(x)\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
        "                 act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    B, H, W, L, C = x.shape\n",
        "    x = x.view(B, H//window_size[0], window_size[0], W//window_size[1], window_size[1], L//window_size[2], window_size[2], C)\n",
        "    windows = x.permute(0,1,3,5,2,4,6,7).contiguous().view(-1, window_size[0], window_size[1], window_size[2], C)\n",
        "    return windows\n",
        "\n",
        "def window_reverse(windows, window_size, H, W, L):\n",
        "    B = int(windows.shape[0]/(H*W*L/window_size[0]/window_size[1]/window_size[2]))\n",
        "    x = windows.view(B, H//window_size[0], W//window_size[1], L//window_size[2],\n",
        "                     window_size[0], window_size[1], window_size[2], -1)\n",
        "    x = x.permute(0,1,4,2,5,3,6,7).contiguous().view(B,H,W,L,-1)\n",
        "    return x\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None,\n",
        "                 rpe=True, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size=window_size\n",
        "        self.num_heads=num_heads\n",
        "        head_dim = dim//num_heads\n",
        "        self.scale = qk_scale or head_dim**-0.5\n",
        "        self.rpe = rpe\n",
        "        table_size=(2*window_size[0]-1)*(2*window_size[1]-1)*(2*window_size[2]-1)\n",
        "        self.relative_position_bias_table = nn.Parameter(torch.zeros(table_size,num_heads))\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        coords_h = torch.arange(window_size[0])\n",
        "        coords_w = torch.arange(window_size[1])\n",
        "        coords_t = torch.arange(window_size[2])\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t]))\n",
        "        coords_flat = coords.flatten(1)\n",
        "        if rpe:\n",
        "            rel = coords_flat[:,:,None] - coords_flat[:,None,:]\n",
        "            rel = rel.permute(1,2,0).contiguous()\n",
        "            rel[...,0]+=window_size[0]-1\n",
        "            rel[...,1]+=window_size[1]-1\n",
        "            rel[...,2]+=window_size[2]-1\n",
        "            pos_factor=(2*window_size[1]-1)*(2*window_size[2]-1)\n",
        "            rel[...,0]*=pos_factor\n",
        "            rel[...,1]*=(2*window_size[2]-1)\n",
        "            rel_index=rel.sum(-1)\n",
        "            self.register_buffer(\"relative_position_index\", rel_index)\n",
        "        self.qkv=nn.Linear(dim, dim*3, bias=qkv_bias)\n",
        "        self.attn_drop=nn.Dropout(attn_drop)\n",
        "        self.proj=nn.Linear(dim, dim)\n",
        "        self.proj_drop=nn.Dropout(proj_drop)\n",
        "        self.softmax=nn.Softmax(dim=-1)\n",
        "    def forward(self, x, mask=None):\n",
        "        B_,N,C=x.shape\n",
        "        qkv=self.qkv(x).reshape(B_,N,3,self.num_heads,C//self.num_heads).permute(2,0,3,1,4)\n",
        "        q, k, v=qkv[0], qkv[1], qkv[2]\n",
        "        q=q*self.scale\n",
        "        attn=(q @ k.transpose(-2,-1))\n",
        "        if self.rpe:\n",
        "            table_size=self.window_size[0]*self.window_size[1]*self.window_size[2]\n",
        "            rpb=self.relative_position_bias_table[self.relative_position_index.view(-1)]\n",
        "            rpb=rpb.view(table_size, table_size, self.num_heads).permute(2,0,1)\n",
        "            attn=attn + rpb.unsqueeze(0)\n",
        "        if mask is not None:\n",
        "            nW=mask.shape[0]\n",
        "            attn=attn.view(B_//nW, nW,self.num_heads,N,N)\n",
        "            attn=attn+mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn=attn.view(-1,self.num_heads,N,N)\n",
        "            attn=self.softmax(attn)\n",
        "        else:\n",
        "            attn=self.softmax(attn)\n",
        "        attn=self.attn_drop(attn)\n",
        "        x=(attn@v).transpose(1,2).reshape(B_,N,C)\n",
        "        x=self.proj(x)\n",
        "        x=self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class FAB(nn.Module):\n",
        "    \"\"\"\n",
        "    Fusion Attention Block = (W-MSA or SW-MSA) + local conv\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, window_size=(7,7,7), shift_size=(0,0,0),\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, rpe=True,\n",
        "                 drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim=dim\n",
        "        self.num_heads=num_heads\n",
        "        self.window_size=window_size\n",
        "        self.shift_size=shift_size\n",
        "        self.mlp_ratio=mlp_ratio\n",
        "        self.norm1=norm_layer(dim)\n",
        "        self.attn=WindowAttention(dim, window_size, num_heads, qkv_bias, qk_scale, rpe, attn_drop, drop)\n",
        "        self.drop_path=DropPath(drop_path) if drop_path>0 else nn.Identity()\n",
        "        self.norm2=norm_layer(dim)\n",
        "        mlp_hidden=int(dim*mlp_ratio)\n",
        "        self.mlp=Mlp(dim, mlp_hidden, act_layer=act_layer, drop=drop)\n",
        "        self.conv_block = CAB(num_feat=dim, compress_ratio=3, squeeze_factor=30)\n",
        "        self.H=self.W=self.T=None\n",
        "    def forward(self, x, mask_matrix):\n",
        "        H,W,T=self.H,self.W,self.T\n",
        "        B,L,C=x.shape\n",
        "        shortcut=x\n",
        "        x=self.norm1(x)\n",
        "        x_3d=x.view(B,H,W,T,C)\n",
        "        conv_x=self.conv_block(x_3d.permute(0,4,1,2,3))\n",
        "        conv_x=conv_x.permute(0,2,3,4,1).contiguous().view(B,H*W*T,C)\n",
        "        pad_r=(self.window_size[0]-H%self.window_size[0])%self.window_size[0]\n",
        "        pad_b=(self.window_size[1]-W%self.window_size[1])%self.window_size[1]\n",
        "        pad_h=(self.window_size[2]-T%self.window_size[2])%self.window_size[2]\n",
        "        x_3d=F.pad(x_3d,(0,0,0,pad_h,0,pad_b,0,pad_r))\n",
        "        Hp, Wp, Tp=x_3d.shape[1], x_3d.shape[2], x_3d.shape[3]\n",
        "        if any(self.shift_size):\n",
        "            shifted_x=torch.roll(x_3d, shifts=(-self.shift_size[0],-self.shift_size[1],-self.shift_size[2]), dims=(1,2,3))\n",
        "            attn_mask=mask_matrix\n",
        "        else:\n",
        "            shifted_x=x_3d\n",
        "            attn_mask=None\n",
        "        x_windows=window_partition(shifted_x,self.window_size)\n",
        "        x_windows=x_windows.view(-1,self.window_size[0]*self.window_size[1]*self.window_size[2],C)\n",
        "        attn_windows=self.attn(x_windows, attn_mask)\n",
        "        attn_windows=attn_windows.view(-1,*self.window_size,C)\n",
        "        shifted_x=window_reverse(attn_windows,self.window_size,Hp,Wp,Tp)\n",
        "        if any(self.shift_size):\n",
        "            x_3d=torch.roll(shifted_x,shifts=(self.shift_size[0],self.shift_size[1],self.shift_size[2]),dims=(1,2,3))\n",
        "        else:\n",
        "            x_3d=shifted_x\n",
        "        if pad_r>0 or pad_b>0 or pad_h>0:\n",
        "            x_3d=x_3d[:,:H,:W,:T,:].contiguous()\n",
        "        x_attn=x_3d.view(B,H*W*T,C)\n",
        "        x=shortcut + self.drop_path(x_attn) + conv_x*0.01\n",
        "        x=x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class PatchMerging(nn.Module):\n",
        "    \"\"\"\n",
        "    3D Patch Merging\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, norm_layer=nn.LayerNorm, reduce_factor=2):\n",
        "        super().__init__()\n",
        "        self.dim=dim\n",
        "        self.reduction=nn.Linear(8*dim,(8//reduce_factor)*dim,bias=False)\n",
        "        self.norm=norm_layer(8*dim)\n",
        "    def forward(self, x, H, W, T):\n",
        "        B,L,C=x.shape\n",
        "        x_3d=x.view(B,H,W,T,C)\n",
        "        x0=x_3d[:,0::2,0::2,0::2,:]\n",
        "        x1=x_3d[:,1::2,0::2,0::2,:]\n",
        "        x2=x_3d[:,0::2,1::2,0::2,:]\n",
        "        x3=x_3d[:,0::2,0::2,1::2,:]\n",
        "        x4=x_3d[:,1::2,1::2,0::2,:]\n",
        "        x5=x_3d[:,0::2,1::2,1::2,:]\n",
        "        x6=x_3d[:,1::2,0::2,1::2,:]\n",
        "        x7=x_3d[:,1::2,1::2,1::2,:]\n",
        "        x_cat=torch.cat([x0,x1,x2,x3,x4,x5,x6,x7],dim=-1)\n",
        "        x_cat=x_cat.view(B,-1,8*C)\n",
        "        x_cat=self.norm(x_cat)\n",
        "        x_cat=self.reduction(x_cat)\n",
        "        return x_cat\n",
        "\n",
        "class BasicLayer(nn.Module):\n",
        "    def __init__(self, dim, depth, num_heads, window_size=(7,7,7),\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, rpe=True,\n",
        "                 drop=0., attn_drop=0., drop_path=0.,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 downsample=None, use_checkpoint=False,\n",
        "                 pat_merg_rf=2):\n",
        "        super().__init__()\n",
        "        self.window_size=window_size\n",
        "        self.shift_size=(window_size[0]//2, window_size[1]//2, window_size[2]//2)\n",
        "        self.depth=depth\n",
        "        self.use_checkpoint=use_checkpoint\n",
        "        self.blocks=nn.ModuleList([\n",
        "            FAB(dim,num_heads,window_size,(0,0,0) if (i%2==0) else self.shift_size,\n",
        "                mlp_ratio,qkv_bias,qk_scale,rpe,drop,attn_drop,\n",
        "                drop_path[i] if isinstance(drop_path,list) else drop_path,\n",
        "                norm_layer=norm_layer)\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "        self.overlap_attn=None\n",
        "        # Overlapping block can be added here if needed\n",
        "        self.downsample=downsample(dim=dim, norm_layer=norm_layer, reduce_factor=pat_merg_rf) if downsample else None\n",
        "    def forward(self,x,H,W,T):\n",
        "        Hp=int(math.ceil(H/self.window_size[0]))*self.window_size[0]\n",
        "        Wp=int(math.ceil(W/self.window_size[1]))*self.window_size[1]\n",
        "        Tp=int(math.ceil(T/self.window_size[2]))*self.window_size[2]\n",
        "        img_mask=torch.zeros((1,Hp,Wp,Tp,1), device=x.device)\n",
        "        h_slices=(slice(0,-self.window_size[0]),\n",
        "                  slice(-self.window_size[0], -self.shift_size[0]),\n",
        "                  slice(-self.shift_size[0], None))\n",
        "        w_slices=(slice(0,-self.window_size[1]),\n",
        "                  slice(-self.window_size[1], -self.shift_size[1]),\n",
        "                  slice(-self.shift_size[1], None))\n",
        "        t_slices=(slice(0,-self.window_size[2]),\n",
        "                  slice(-self.window_size[2], -self.shift_size[2]),\n",
        "                  slice(-self.shift_size[2], None))\n",
        "        cnt=0\n",
        "        for hh in h_slices:\n",
        "            for ww in w_slices:\n",
        "                for tt in t_slices:\n",
        "                    img_mask[:,hh,ww,tt,:]=cnt\n",
        "                    cnt+=1\n",
        "        mask_windows=window_partition(img_mask,self.window_size)\n",
        "        mask_windows=mask_windows.view(-1,self.window_size[0]*self.window_size[1]*self.window_size[2])\n",
        "        attn_mask=mask_windows.unsqueeze(1)-mask_windows.unsqueeze(2)\n",
        "        attn_mask=attn_mask.masked_fill(attn_mask!=0,float(-100.0)).masked_fill(attn_mask==0,float(0.0))\n",
        "        for blk in self.blocks:\n",
        "            blk.H, blk.W, blk.T=H,W,T\n",
        "            if self.use_checkpoint:\n",
        "                x=checkpoint.checkpoint(blk,x,attn_mask)\n",
        "            else:\n",
        "                x=blk(x,attn_mask)\n",
        "        if self.downsample is not None:\n",
        "            x_down=self.downsample(x,H,W,T)\n",
        "            Wh, Ww, Wt=(H+1)//2,(W+1)//2,(T+1)//2\n",
        "            return x,H,W,T, x_down,Wh,Ww,Wt\n",
        "        else:\n",
        "            return x,H,W,T, x,H,W,T\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
        "        super().__init__()\n",
        "        patch_size=to_3tuple(patch_size)\n",
        "        self.patch_size=patch_size\n",
        "        self.in_chans=in_chans\n",
        "        self.embed_dim=embed_dim\n",
        "        self.proj=nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm=norm_layer(embed_dim) if norm_layer else None\n",
        "    def forward(self, x):\n",
        "        B,C,H,W,T=x.shape\n",
        "        ps=self.patch_size\n",
        "        if T%ps[2]!=0:\n",
        "            x=F.pad(x,(0,ps[2]-T%ps[2]))\n",
        "        if W%ps[1]!=0:\n",
        "            x=F.pad(x,(0,0,0,ps[1]-W%ps[1]))\n",
        "        if H%ps[0]!=0:\n",
        "            x=F.pad(x,(0,0,0,0,0,ps[0]-H%ps[0]))\n",
        "        x=self.proj(x)\n",
        "        if self.norm is not None:\n",
        "            Hp,Wp,Tp=x.shape[2], x.shape[3], x.shape[4]\n",
        "            x=x.flatten(2).transpose(1,2)\n",
        "            x=self.norm(x)\n",
        "            x=x.transpose(1,2).view(B,self.embed_dim,Hp,Wp,Tp)\n",
        "        return x\n",
        "\n",
        "class SinPositionalEncoding3D(nn.Module):\n",
        "    # optional\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        channels=int(np.ceil(channels/6)*2)\n",
        "        if channels%2: channels+=1\n",
        "        self.channels=channels\n",
        "        self.inv_freq=1./(10000**(torch.arange(0,channels,2).float()/channels))\n",
        "    def forward(self, tensor):\n",
        "        tensor=tensor.permute(0,2,3,4,1)\n",
        "        B,X,Y,Z,C=tensor.shape\n",
        "        pos_x=torch.arange(X, device=tensor.device).type(self.inv_freq.type())\n",
        "        pos_y=torch.arange(Y, device=tensor.device).type(self.inv_freq.type())\n",
        "        pos_z=torch.arange(Z, device=tensor.device).type(self.inv_freq.type())\n",
        "        sin_inp_x=torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
        "        sin_inp_y=torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
        "        sin_inp_z=torch.einsum(\"i,j->ij\", pos_z, self.inv_freq)\n",
        "        emb_x=torch.cat([sin_inp_x.sin(), sin_inp_x.cos()], dim=-1).unsqueeze(1).unsqueeze(1)\n",
        "        emb_y=torch.cat([sin_inp_y.sin(), sin_inp_y.cos()], dim=-1).unsqueeze(1)\n",
        "        emb_z=torch.cat([sin_inp_z.sin(), sin_inp_z.cos()], dim=-1)\n",
        "        emb=torch.zeros((X,Y,Z,self.channels*3), device=tensor.device, dtype=tensor.dtype)\n",
        "        emb[:,:,:, :self.channels]=emb_x\n",
        "        emb[:,:,:, self.channels:2*self.channels]=emb_y\n",
        "        emb[:,:,:,2*self.channels:]=emb_z\n",
        "        emb=emb[None,:,:, :,:C].repeat(B,1,1,1,1)\n",
        "        return emb.permute(0,4,1,2,3)\n",
        "\n",
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self, pretrain_img_size=224,\n",
        "                 patch_size=4, in_chans=3, embed_dim=96,\n",
        "                 depths=[2,2,6,2], num_heads=[3,6,12,24], window_size=(7,7,7),\n",
        "                 mlp_ratio=4., qkv_bias=True,qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.2,\n",
        "                 norm_layer=nn.LayerNorm, ape=False, spe=False, rpe=True,\n",
        "                 patch_norm=True,out_indices=(0,1,2,3),frozen_stages=-1,\n",
        "                 use_checkpoint=False, pat_merg_rf=2):\n",
        "        super().__init__()\n",
        "        self.num_layers=len(depths)\n",
        "        self.embed_dim=embed_dim\n",
        "        self.ape=ape\n",
        "        self.spe=spe\n",
        "        self.rpe=rpe\n",
        "        self.patch_norm=patch_norm\n",
        "        self.out_indices=out_indices\n",
        "        self.frozen_stages=frozen_stages\n",
        "        self.patch_embed=PatchEmbed(patch_size, in_chans, embed_dim, norm_layer if patch_norm else None)\n",
        "        self.pos_drop=nn.Dropout(p=drop_rate)\n",
        "        dpr=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "        self.layers=nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer=BasicLayer(dim=int(embed_dim*2**i_layer),\n",
        "                             depth=depths[i_layer],\n",
        "                             num_heads=num_heads[i_layer],\n",
        "                             window_size=window_size,\n",
        "                             mlp_ratio=mlp_ratio,\n",
        "                             qkv_bias=qkv_bias,\n",
        "                             qk_scale=qk_scale,\n",
        "                             rpe=rpe,\n",
        "                             drop=drop_rate,\n",
        "                             attn_drop=attn_drop_rate,\n",
        "                             drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer+1])],\n",
        "                             norm_layer=norm_layer,\n",
        "                             downsample=PatchMerging if (i_layer<self.num_layers-1) else None,\n",
        "                             use_checkpoint=use_checkpoint,\n",
        "                             pat_merg_rf=pat_merg_rf)\n",
        "            self.layers.append(layer)\n",
        "        num_features=[int(embed_dim*2**i) for i in range(self.num_layers)]\n",
        "        self.num_features=num_features\n",
        "        for i_layer in out_indices:\n",
        "            layer_=norm_layer(num_features[i_layer])\n",
        "            self.add_module(f\"norm{i_layer}\", layer_)\n",
        "    def forward(self, x):\n",
        "        x=self.patch_embed(x)\n",
        "        B,C,Hp,Wp,Tp=x.shape\n",
        "        x=x.flatten(2).transpose(1,2)\n",
        "        x=self.pos_drop(x)\n",
        "        outs=[]\n",
        "        for i in range(self.num_layers):\n",
        "            layer=self.layers[i]\n",
        "            x_out,H,W,T, x, Wh,Ww,Wt=layer(x, Hp,Wp,Tp)\n",
        "            if i in self.out_indices:\n",
        "                norm_layer=getattr(self, f\"norm{i}\")\n",
        "                x_out=norm_layer(x_out)\n",
        "                out_f=x_out.view(B,H,W,T,self.num_features[i]).permute(0,4,1,2,3).contiguous()\n",
        "                outs.append(out_f)\n",
        "            Hp,Wp,Tp=Wh,Ww,Wt\n",
        "        return outs\n",
        "\n",
        "class PixelShuffle3d(nn.Module):\n",
        "    def __init__(self, scale):\n",
        "        super().__init__()\n",
        "        self.scale=scale\n",
        "    def forward(self, inp):\n",
        "        b,c,d,h,w=inp.size()\n",
        "        oc=c//(self.scale**3)\n",
        "        od, oh, ow=d*self.scale,h*self.scale,w*self.scale\n",
        "        out=inp.view(b, oc, self.scale, self.scale, self.scale, d,h,w)\n",
        "        out=out.permute(0,1,5,2,6,3,7,4).contiguous()\n",
        "        out=out.view(b, oc, od, oh, ow)\n",
        "        return out\n",
        "\n",
        "class ConvergeHead(nn.Module):\n",
        "    def __init__(self, in_dim, up_ratio, kernel_size, padding):\n",
        "        super().__init__()\n",
        "        self.in_dim=in_dim\n",
        "        self.up_ratio=up_ratio\n",
        "        self.conv=nn.Conv3d(in_dim, (up_ratio**3)*in_dim, kernel_size,1,padding,1,in_dim)\n",
        "        self.apply(self._init_weights)\n",
        "    def forward(self, x):\n",
        "        hp=self.conv(x)\n",
        "        poxel=PixelShuffle3d(self.up_ratio)\n",
        "        hp=poxel(hp)\n",
        "        return hp\n",
        "    def _init_weights(self,m):\n",
        "        if isinstance(m,(nn.Conv3d,nn.Linear)):\n",
        "            nn.init.normal_(m.weight,std=0.001)\n",
        "            nn.init.constant_(m.bias,0)\n",
        "        elif isinstance(m,nn.BatchNorm3d):\n",
        "            nn.init.constant_(m.weight,1)\n",
        "            nn.init.constant_(m.bias,0)\n",
        "\n",
        "class Conv3dReLU(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1, use_batchnorm=True):\n",
        "        conv=nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
        "        relu=nn.LeakyReLU(inplace=True)\n",
        "        nm=nn.BatchNorm3d(out_channels) if use_batchnorm else nn.InstanceNorm3d(out_channels)\n",
        "        super(Conv3dReLU,self).__init__(conv, nm, relu)\n",
        "\n",
        "class SR(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, skip_channels=0, use_batchnorm=True):\n",
        "        super().__init__()\n",
        "        self.up=ConvergeHead(in_channels,2,3,1)\n",
        "        self.conv1=Conv3dReLU(in_channels+skip_channels, out_channels,3,1,use_batchnorm=use_batchnorm)\n",
        "        self.conv2=Conv3dReLU(out_channels,out_channels,3,1,use_batchnorm=use_batchnorm)\n",
        "    def forward(self,x, skip=None):\n",
        "        x=self.up(x)\n",
        "        if skip is not None:\n",
        "            x=torch.cat([x, skip], dim=1)\n",
        "        x=self.conv1(x)\n",
        "        x=self.conv2(x)\n",
        "        return x\n",
        "\n",
        "class RegistrationHead(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n",
        "        conv3d=nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2)\n",
        "        conv3d.weight=nn.Parameter(Normal(0,1e-5).sample(conv3d.weight.shape))\n",
        "        conv3d.bias=nn.Parameter(torch.zeros(conv3d.bias.shape))\n",
        "        super().__init__(conv3d)\n",
        "\n",
        "class SpatialTransformer_forUTSR(nn.Module):\n",
        "    def __init__(self,size,mode='bilinear'):\n",
        "        super().__init__()\n",
        "        self.mode=mode\n",
        "        vectors=[torch.arange(0,s) for s in size]\n",
        "        grids=torch.meshgrid(vectors)\n",
        "        grid=torch.stack(grids)\n",
        "        grid=torch.unsqueeze(grid,0).float()\n",
        "        self.register_buffer('grid',grid)\n",
        "    def forward(self, src, flow):\n",
        "        new_locs=self.grid + flow\n",
        "        shape=flow.shape[2:]\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:,i,...]=2*(new_locs[:,i,...]/(shape[i]-1)-0.5)\n",
        "        if len(shape)==3:\n",
        "            new_locs=new_locs.permute(0,2,3,4,1)\n",
        "            new_locs=new_locs[..., [2,1,0]]\n",
        "        return F.grid_sample(src,new_locs,align_corners=False,mode=self.mode)\n",
        "\n",
        "class UTSRMorph(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(UTSRMorph,self).__init__()\n",
        "        self.if_convskip=config.if_convskip\n",
        "        self.if_transskip=config.if_transskip\n",
        "        embed_dim=config.embed_dim\n",
        "        self.transformer=SwinTransformer(\n",
        "            patch_size=config.patch_size,\n",
        "            in_chans=config.in_chans,\n",
        "            embed_dim=config.embed_dim,\n",
        "            depths=config.depths,\n",
        "            num_heads=config.num_heads,\n",
        "            window_size=config.window_size,\n",
        "            mlp_ratio=config.mlp_ratio,\n",
        "            qkv_bias=config.qkv_bias,\n",
        "            drop_rate=config.drop_rate,\n",
        "            drop_path_rate=config.drop_path_rate,\n",
        "            ape=config.ape,\n",
        "            spe=config.spe,\n",
        "            rpe=config.rpe,\n",
        "            patch_norm=config.patch_norm,\n",
        "            use_checkpoint=config.use_checkpoint,\n",
        "            out_indices=config.out_indices,\n",
        "            pat_merg_rf=config.pat_merg_rf\n",
        "        )\n",
        "        self.up0=SR(embed_dim*8, embed_dim*4, skip_channels=(embed_dim*4 if self.if_transskip else 0), use_batchnorm=False)\n",
        "        self.up1=SR(embed_dim*4, embed_dim*2, skip_channels=(embed_dim*2 if self.if_transskip else 0), use_batchnorm=False)\n",
        "        self.up2=SR(embed_dim*2, embed_dim, skip_channels=(embed_dim if self.if_transskip else 0), use_batchnorm=False)\n",
        "        self.up3=SR(embed_dim, config.reg_head_chan, skip_channels=(embed_dim//2 if self.if_convskip else 0),use_batchnorm=False)\n",
        "        self.c1=Conv3dReLU(2, embed_dim//2,3,1,use_batchnorm=False)\n",
        "        self.reg_head=RegistrationHead(config.reg_head_chan,3,3)\n",
        "        self.spatial_trans=SpatialTransformer_forUTSR(config.img_size)\n",
        "        self.avg_pool=nn.AvgPool3d(3,stride=2,padding=1)\n",
        "        self.up=ConvergeHead(3,2,3,1)\n",
        "    def forward(self, x):\n",
        "        # x: shape (B,2,H,W,D)\n",
        "        source=x[:,0:1,:,:,:]\n",
        "        if self.if_convskip:\n",
        "            x_s1=self.avg_pool(x)\n",
        "            f4=self.c1(x_s1)\n",
        "        else:\n",
        "            f4=None\n",
        "        out_feats=self.transformer(x)\n",
        "        if self.if_transskip:\n",
        "            f1=out_feats[-2]\n",
        "            f2=out_feats[-3]\n",
        "            f3=out_feats[-4]\n",
        "        else:\n",
        "            f1=None; f2=None; f3=None\n",
        "        x=self.up0(out_feats[-1], f1)\n",
        "        x=self.up1(x,f2)\n",
        "        x=self.up2(x,f3)\n",
        "        x=self.up3(x,f4)\n",
        "        flow=self.reg_head(x)\n",
        "        flow=self.up(flow)\n",
        "        out=self.spatial_trans(source, flow)\n",
        "        return out, flow\n",
        "\n",
        "################################################################################\n",
        "# 5) THE TRAINING SCRIPT (Merged from 'train_UTSRMorph_oasis.py')\n",
        "################################################################################\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, save_dir):\n",
        "        self.terminal=sys.stdout\n",
        "        self.log=open(save_dir+\"logfile.log\",\"a\")\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "    def flush(self):\n",
        "        pass\n",
        "\n",
        "def mk_grid_img(grid_step, line_thickness=1, grid_sz=(160,192,224)):\n",
        "    grid_img=np.zeros(grid_sz)\n",
        "    for j in range(0, grid_img.shape[1], grid_step):\n",
        "        grid_img[:, j+line_thickness-1, :]=1\n",
        "    for i in range(0, grid_img.shape[2], grid_step):\n",
        "        grid_img[:, :, i+line_thickness-1]=1\n",
        "    grid_img=grid_img[None,None,...]\n",
        "    grid_img=torch.from_numpy(grid_img).cuda().float()\n",
        "    return grid_img\n",
        "\n",
        "def comput_fig(img):\n",
        "    img=img.detach().cpu().numpy()[0,0,48:64,:,:]\n",
        "    fig=plt.figure(figsize=(12,12),dpi=180)\n",
        "    for i in range(img.shape[0]):\n",
        "        plt.subplot(4,4,i+1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[i,:,:], cmap='gray')\n",
        "    fig.subplots_adjust(wspace=0,hspace=0)\n",
        "    return fig\n",
        "\n",
        "def save_checkpoint(state, save_dir='models', filename='checkpoint.pth.tar', max_model_num=8):\n",
        "    torch.save(state, save_dir+filename)\n",
        "    model_lists=natsorted(glob.glob(save_dir+'*'))\n",
        "    while len(model_lists)>max_model_num:\n",
        "        os.remove(model_lists[0])\n",
        "        model_lists=natsorted(glob.glob(save_dir+'*'))\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, MAX_EPOCHES, INIT_LR, power=0.9):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr']= round(INIT_LR * np.power(1 - (epoch)/MAX_EPOCHES , power),8)\n",
        "\n",
        "# Dummy data placeholders: data/datasets modules\n",
        "# (In your actual code, these come from 'data' folder.)\n",
        "class DummyOASISBrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files, transforms=None):\n",
        "        self.files=files\n",
        "        self.transforms=transforms\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        # returning random data for demonstration\n",
        "        x=np.random.rand(160,192,224).astype(np.float32)\n",
        "        y=np.random.rand(160,192,224).astype(np.float32)\n",
        "        x_seg=(x*10).astype(np.int16)\n",
        "        y_seg=(y*10).astype(np.int16)\n",
        "        return x[None,:,:,:], y[None,:,:,:], x_seg[None,:,:,:], y_seg[None,:,:,:]\n",
        "\n",
        "class DummyOASISBrainInferDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files, transforms=None):\n",
        "        self.files=files\n",
        "        self.transforms=transforms\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        x=np.random.rand(160,192,224).astype(np.float32)\n",
        "        y=np.random.rand(160,192,224).astype(np.float32)\n",
        "        x_seg=(x*10).astype(np.int16)\n",
        "        y_seg=(y*10).astype(np.int16)\n",
        "        return x[None,:,:,:], y[None,:,:,:], x_seg[None,:,:,:], y_seg[None,:,:,:]\n",
        "\n",
        "def train_UTSRMorph():\n",
        "    batch_size=1\n",
        "    train_dir='(example path)'\n",
        "    val_dir='(example val path)'\n",
        "    weights=[1,1]\n",
        "    save_dir='UTSRMorph_ncc_{}_diffusion_{}/'.format(weights[0], weights[1])\n",
        "    if not os.path.exists('experiments/'+save_dir):\n",
        "        os.makedirs('experiments/'+save_dir)\n",
        "    if not os.path.exists('logs/'+save_dir):\n",
        "        os.makedirs('logs/'+save_dir)\n",
        "    # sys.stdout=Logger('logs/'+save_dir) # optional\n",
        "    lr=1e-4\n",
        "    epoch_start=0\n",
        "    max_epoch=5\n",
        "    cont_training=False\n",
        "    config=get_UTSRMorph_config()\n",
        "    model=UTSRMorph(config).cuda()\n",
        "    reg_model=register_model(config.img_size, 'nearest').cuda()\n",
        "    reg_model_bilin=register_model(config.img_size, 'bilinear').cuda()\n",
        "    if cont_training:\n",
        "        pass\n",
        "    updated_lr=lr\n",
        "    train_set=DummyOASISBrainDataset(glob.glob(train_dir+'*.pkl'))\n",
        "    val_set=DummyOASISBrainInferDataset(glob.glob(val_dir+'*.pkl'))\n",
        "    train_loader=DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    val_loader=DataLoader(val_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
        "    optimizer=optim.Adam(model.parameters(), lr=updated_lr, weight_decay=0, amsgrad=True)\n",
        "    criterion_ncc=NCC_vxm()\n",
        "    criterion_reg=Grad3d(penalty='l2')\n",
        "    best_dsc=0\n",
        "    writer=SummaryWriter(log_dir='logs/'+save_dir)\n",
        "    for epoch in range(epoch_start,max_epoch):\n",
        "        print(f'--- Epoch {epoch} / {max_epoch} ---')\n",
        "        loss_all=AverageMeter()\n",
        "        idx=0\n",
        "        time_start=time.time()\n",
        "        for data in train_loader:\n",
        "            idx+=1\n",
        "            model.train()\n",
        "            adjust_learning_rate(optimizer, epoch, max_epoch, lr)\n",
        "            data=[t.cuda() for t in data]\n",
        "            x=data[0]\n",
        "            y=data[1]\n",
        "            x_seg=data[2]\n",
        "            y_seg=data[3]\n",
        "            if random.random()<=0.5:\n",
        "                x_in=torch.cat((x,y),dim=1)\n",
        "                output, flow=model(x_in)\n",
        "                loss_ncc=criterion_ncc(output,y)*weights[0]\n",
        "                loss_reg=criterion_reg(flow,y)*weights[1]\n",
        "                loss=loss_ncc+loss_reg\n",
        "                loss_all.update(loss.item(), y.numel())\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                del x_in, output, flow\n",
        "            else:\n",
        "                y_in=torch.cat((y,x),dim=1)\n",
        "                output, flow=model(y_in)\n",
        "                loss_ncc=criterion_ncc(output,x)*weights[0]\n",
        "                loss_reg=criterion_reg(flow,x)*weights[1]\n",
        "                loss=loss_ncc+loss_reg\n",
        "                loss_all.update(loss.item(), x.numel())\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                del y_in,output, flow\n",
        "            print('Iter {} of {} loss {:.4f}, ImgSim {:.6f}, Reg {:.6f}'.format(idx,len(train_loader),loss.item(),loss_ncc.item(),loss_reg.item()))\n",
        "        writer.add_scalar('Loss/train',loss_all.avg,epoch)\n",
        "        print('Epoch {} loss {:.4f}'.format(epoch, loss_all.avg))\n",
        "        eval_dsc=AverageMeter()\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                model.eval()\n",
        "                data=[t.cuda() for t in data]\n",
        "                x=data[0]\n",
        "                y=data[1]\n",
        "                x_seg=data[2]\n",
        "                y_seg=data[3]\n",
        "                x_in=torch.cat((x,y),dim=1)\n",
        "                grid_img=mk_grid_img(8,1,config.img_size)\n",
        "                output=model(x_in)\n",
        "                def_out=reg_model([x_seg.float(), output[1]])\n",
        "                def_grid=reg_model_bilin([grid_img.float(), output[1]])\n",
        "                dsc_val=dice_val_VOI(def_out.long(), y_seg.long())\n",
        "                eval_dsc.update(dsc_val.item(), x.size(0))\n",
        "                print(eval_dsc.avg)\n",
        "        best_dsc=max(eval_dsc.avg, best_dsc)\n",
        "        save_checkpoint({\n",
        "            'epoch':epoch+1,\n",
        "            'state_dict':model.state_dict(),\n",
        "            'best_dsc':best_dsc,\n",
        "            'optimizer':optimizer.state_dict()\n",
        "        }, save_dir='experiments/'+save_dir, filename='dsc{:.4f}.pth.tar'.format(eval_dsc.avg))\n",
        "        time_end=time.time()\n",
        "        alltime=(time_end-time_start)*(max_epoch-1-epoch)\n",
        "        timeresult=str(datetime.timedelta(seconds=alltime))\n",
        "        print(\"time:\"+timeresult)\n",
        "        writer.add_scalar('DSC/validate', eval_dsc.avg, epoch)\n",
        "        # maybe skip plotting if in colab\n",
        "        loss_all.reset()\n",
        "    writer.close()\n",
        "\n",
        "################################################################################\n",
        "# 6) INFERENCE SCRIPT (Merged from 'infer_UTSRMorph_oasis.py')\n",
        "################################################################################\n",
        "\n",
        "def infer_UTSRMorph():\n",
        "    test_dir='(example test path)'\n",
        "    save_dir='(example submit path)'\n",
        "    model_idx=-1\n",
        "    weights=[1,1,1]\n",
        "    model_folder='UTSRMorph_ncc_{}_diffusion_{}/'.format(weights[0],weights[1])\n",
        "    model_dir='experiments/'+model_folder\n",
        "    config=get_UTSRMorph_config()\n",
        "    model=UTSRMorph(config)\n",
        "    # load some best model\n",
        "    # best_model=torch.load(model_dir + natsorted(os.listdir(model_dir))[model_idx])['state_dict']\n",
        "    # model.load_state_dict(best_model)\n",
        "    model.cuda()\n",
        "    img_size=(160,192,224)\n",
        "    reg_model=register_model(img_size, 'nearest').cuda()\n",
        "    # build dummy data loader\n",
        "    test_set=DummyOASISBrainInferDataset(glob.glob(test_dir+'*.pkl'))\n",
        "    test_loader=DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
        "    file_names=glob.glob(test_dir+'*.pkl')\n",
        "    dice_all=[]\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(file_names):\n",
        "            # your actual code loads data differently\n",
        "            x,y,x_seg,y_seg=pkload(data)\n",
        "            x, y=torch.from_numpy(x).cuda(), torch.from_numpy(y).cuda()\n",
        "            x_seg,y_seg=torch.from_numpy(x_seg).cuda(), torch.from_numpy(y_seg).cuda()\n",
        "            model.eval()\n",
        "            x_in=torch.cat((x,y),dim=1)\n",
        "            x_def, flow=model(x_in)\n",
        "            def_out=reg_model([x_seg.float(), flow])\n",
        "            dsc_val=dice_val_VOI(def_out.long(), y_seg.long())\n",
        "            dice_all.append(dsc_val)\n",
        "    dice_all=np.array(dice_all)\n",
        "    print(\"Mean DSC:\",dice_all.mean())\n",
        "\n",
        "################################################################################\n",
        "# 7) analysis_oasis.py (Optional - for advanced metrics)\n",
        "################################################################################\n",
        "\n",
        "def jacobian_determinant_np(disp):\n",
        "    _,_,H,W,D=disp.shape\n",
        "    gradx=np.array([-0.5,0,0.5]).reshape(1,3,1,1)\n",
        "    grady=np.array([-0.5,0,0.5]).reshape(1,1,3,1)\n",
        "    gradz=np.array([-0.5,0,0.5]).reshape(1,1,1,3)\n",
        "    gradx_disp=np.stack([scipy.ndimage.correlate(disp[:,0,:,:,:],gradx,mode='constant',cval=0.0),\n",
        "                          scipy.ndimage.correlate(disp[:,1,:,:,:],gradx,mode='constant',cval=0.0),\n",
        "                          scipy.ndimage.correlate(disp[:,2,:,:,:],gradx,mode='constant',cval=0.0)], axis=1)\n",
        "    grady_disp=np.stack([scipy.ndimage.correlate(disp[:,0,:,:,:],grady,mode='constant',cval=0.0),\n",
        "                          scipy.ndimage.correlate(disp[:,1,:,:,:],grady,mode='constant',cval=0.0),\n",
        "                          scipy.ndimage.correlate(disp[:,2,:,:,:],grady,mode='constant',cval=0.0)], axis=1)\n",
        "    gradz_disp=np.stack([scipy.ndimage.correlate(disp[:,0,:,:,:],gradz,mode='constant',cval=0.0),\n",
        "                          scipy.ndimage.correlate(disp[:,1,:,:,:],gradz,mode='constant',cval=0.0),\n",
        "                          scipy.ndimage.correlate(disp[:,2,:,:,:],gradz,mode='constant',cval=0.0)], axis=1)\n",
        "    grad_disp=np.concatenate([gradx_disp,grady_disp,gradz_disp], 0)\n",
        "    jacobian=grad_disp+np.eye(3,3).reshape(3,3,1,1,1)\n",
        "    jacobian=jacobian[:,:,2:-2,2:-2,2:-2]\n",
        "    jacdet= jacobian[0,0,:,:,:]* (jacobian[1,1,:,:,:]*jacobian[2,2,:,:,:]- jacobian[1,2,:,:,:]*jacobian[2,1,:,:,:]) \\\n",
        "            -jacobian[1,0,:,:,:]* (jacobian[0,1,:,:,:]*jacobian[2,2,:,:,:]- jacobian[0,2,:,:,:]*jacobian[2,1,:,:,:]) \\\n",
        "            +jacobian[2,0,:,:,:]* (jacobian[0,1,:,:,:]*jacobian[1,2,:,:,:]- jacobian[0,2,:,:,:]*jacobian[1,1,:,:,:])\n",
        "    return jacdet\n",
        "\n",
        "################################################################################\n",
        "# 8) MAIN LAUNCH (uncomment as needed)\n",
        "################################################################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"All code loaded. Now you can train or infer as desired.\\n\")\n",
        "    print(\"To train, call train_UTSRMorph()\")\n",
        "    print(\"To infer, call infer_UTSRMorph()\")\n",
        "\n",
        "    # Example usage:\n",
        "    # train_UTSRMorph()\n",
        "    # infer_UTSRMorph()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQXhW4jwOmFw",
        "outputId": "d1b301e0-b7b2-413f-fdf2-7ed7e3399532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
            "Requirement already satisfied: ml_collections in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Collecting surface-distance\n",
            "  Downloading surface_distance-0.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from surface-distance) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from surface-distance) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
            "Downloading surface_distance-0.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: surface-distance\n",
            "Successfully installed surface-distance-0.1\n",
            "All code loaded. Now you can train or infer as desired.\n",
            "\n",
            "To train, call train_UTSRMorph()\n",
            "To infer, call infer_UTSRMorph()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 0) INSTALL DEPENDENCIES\n",
        "################################################################################\n",
        "\n",
        "!pip install einops timm ml_collections surface-distance\n",
        "\n",
        "################################################################################\n",
        "# 1) IMPORTS\n",
        "################################################################################\n",
        "\n",
        "import os, sys, math, glob, random, pickle, datetime, collections\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from natsort import natsorted\n",
        "from typing import Dict, List, Optional, Sequence, Tuple, Union\n",
        "from math import exp\n",
        "import scipy.ndimage\n",
        "import torch.optim as optim\n",
        "\n",
        "# third-party\n",
        "import ml_collections\n",
        "from einops import rearrange\n",
        "from timm.models.layers import DropPath, trunc_normal_, to_3tuple\n",
        "from surface_distance import compute_surface_distances, compute_robust_hausdorff, compute_dice_coefficient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTQ-2e3_SVSL",
        "outputId": "bff5fb1e-b33f-43dd-f9dd-acd24cc82f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
            "Requirement already satisfied: ml_collections in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: surface-distance in /usr/local/lib/python3.10/dist-packages (0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from surface-distance) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from surface-distance) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "################################################################################\n",
        "# 2) DATA UTILS: from data/data_utils.py\n",
        "################################################################################\n",
        "\n",
        "M = 2**32 - 1\n",
        "\n",
        "def init_fn(worker):\n",
        "    import random, torch, numpy as np\n",
        "    seed = torch.LongTensor(1).random_().item()\n",
        "    seed = (seed + worker) % M\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "def pkload(fname):\n",
        "    with open(fname, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "################################################################################\n",
        "# 3) DATASET CLASSES: OASISBrainDataset and OASISBrainInferDataset\n",
        "#    from data/datasets.py\n",
        "################################################################################\n",
        "\n",
        "class OASISBrainDataset(Dataset):\n",
        "    def __init__(self, data_path, transforms):\n",
        "        \"\"\"\n",
        "        data_path: list of pkl file paths\n",
        "        transforms: a Compose of transformations\n",
        "        \"\"\"\n",
        "        self.paths = data_path\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        # pick a random other file as 'target'\n",
        "        tar_list = self.paths.copy()\n",
        "        tar_list.remove(path)\n",
        "        random.shuffle(tar_list)\n",
        "        tar_file = tar_list[0]\n",
        "\n",
        "        x, x_seg = pkload(path)\n",
        "        y, y_seg = pkload(tar_file)\n",
        "\n",
        "        # add channel dims\n",
        "        x, y = x[None, ...], y[None, ...]\n",
        "        x_seg, y_seg = x_seg[None, ...], y_seg[None, ...]\n",
        "\n",
        "        # apply transforms\n",
        "        x, x_seg = self.transforms([x, x_seg])\n",
        "        y, y_seg = self.transforms([y, y_seg])\n",
        "\n",
        "        # convert to contiguous array -> then torch\n",
        "        x, y = np.ascontiguousarray(x), np.ascontiguousarray(y)\n",
        "        x_seg, y_seg = np.ascontiguousarray(x_seg), np.ascontiguousarray(y_seg)\n",
        "\n",
        "        x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "        x_seg, y_seg = torch.from_numpy(x_seg), torch.from_numpy(y_seg)\n",
        "\n",
        "        return x, y, x_seg, y_seg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "class OASISBrainInferDataset(Dataset):\n",
        "    def __init__(self, data_path, transforms):\n",
        "        \"\"\"\n",
        "        data_path: list of pkl file paths\n",
        "        transforms: a Compose of transformations\n",
        "        \"\"\"\n",
        "        self.paths = data_path\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        x, y, x_seg, y_seg = pkload(path)\n",
        "\n",
        "        # add channel dims\n",
        "        x, y = x[None, ...], y[None, ...]\n",
        "        x_seg, y_seg = x_seg[None, ...], y_seg[None, ...]\n",
        "\n",
        "        # apply transforms\n",
        "        x, x_seg = self.transforms([x, x_seg])\n",
        "        y, y_seg = self.transforms([y, y_seg])\n",
        "\n",
        "        # convert to contiguous array -> then torch\n",
        "        x, y = np.ascontiguousarray(x), np.ascontiguousarray(y)\n",
        "        x_seg, y_seg = np.ascontiguousarray(x_seg), np.ascontiguousarray(y_seg)\n",
        "\n",
        "        x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "        x_seg, y_seg = torch.from_numpy(x_seg), torch.from_numpy(y_seg)\n",
        "\n",
        "        return x, y, x_seg, y_seg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "################################################################################\n",
        "# 4) TRANSFORMATIONS: from data/trans.py (plus data/rand.py embedded)\n",
        "################################################################################\n",
        "\n",
        "class Uniform(object):\n",
        "    def __init__(self, a, b):\n",
        "        self.a=a\n",
        "        self.b=b\n",
        "    def sample(self):\n",
        "        import random\n",
        "        return random.uniform(self.a, self.b)\n",
        "\n",
        "class Gaussian(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean=mean\n",
        "        self.std=std\n",
        "    def sample(self):\n",
        "        import random\n",
        "        return random.gauss(self.mean, self.std)\n",
        "\n",
        "class Constant(object):\n",
        "    def __init__(self, val):\n",
        "        self.val=val\n",
        "    def sample(self):\n",
        "        return self.val\n",
        "\n",
        "# Base class for transforms\n",
        "class Base(object):\n",
        "    def sample(self,*shape):\n",
        "        return shape\n",
        "    def tf(self,img,k=0):\n",
        "        return img\n",
        "    def __call__(self, img, dim=3, reuse=False):\n",
        "        if not reuse:\n",
        "            im = img if isinstance(img,np.ndarray) else img[0]\n",
        "            shape = im.shape[1:dim+1]\n",
        "            self.sample(*shape)\n",
        "        if isinstance(img, (list, tuple)):\n",
        "            return [self.tf(x, k_idx) for k_idx, x in enumerate(img)]\n",
        "        return self.tf(img)\n",
        "\n",
        "class Identity(Base):\n",
        "    def __str__(self):\n",
        "        return 'Identity()'\n",
        "\n",
        "class Compose(Base):\n",
        "    def __init__(self, ops):\n",
        "        if not isinstance(ops, (list, tuple)):\n",
        "            ops=[ops]\n",
        "        self.ops=ops\n",
        "    def sample(self,*shape):\n",
        "        for op in self.ops:\n",
        "            shape=op.sample(*shape)\n",
        "    def tf(self,img,k=0):\n",
        "        for op in self.ops:\n",
        "            img=op.tf(img,k)\n",
        "        return img\n",
        "    def __str__(self):\n",
        "        return f\"Compose({self.ops})\"\n",
        "\n",
        "class NumpyType(Base):\n",
        "    \"\"\"\n",
        "    Convert to specific numpy dtype per item\n",
        "    e.g. NumpyType((np.float32, np.int16))\n",
        "    \"\"\"\n",
        "    def __init__(self, types, num=-1):\n",
        "        self.types=types\n",
        "        self.num=num\n",
        "    def tf(self,img,k=0):\n",
        "        if self.num>0 and k>=self.num:\n",
        "            return img\n",
        "        return img.astype(self.types[k])\n",
        "    def __str__(self):\n",
        "        return f\"NumpyType({self.types})\"\n",
        "\n",
        "# Add more transforms as needed from trans.py\n",
        "# e.g. RandomRotion, RandomFlip, etc.\n",
        "\n",
        "################################################################################\n",
        "# 5) TRAIN/INFERENCE UTILS\n",
        "################################################################################\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val=0\n",
        "        self.avg=0\n",
        "        self.sum=0\n",
        "        self.count=0\n",
        "        self.vals=[]\n",
        "        self.std=0\n",
        "    def update(self,val,n=1):\n",
        "        self.val=val\n",
        "        self.sum+=val*n\n",
        "        self.count+=n\n",
        "        self.avg=self.sum/self.count\n",
        "        self.vals.append(val)\n",
        "        self.std=np.std(self.vals)\n",
        "\n",
        "def dice_val_VOI(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Simple DSC across labels 1..35\n",
        "    \"\"\"\n",
        "    pred=y_pred.detach().cpu().numpy()[0,0,...]\n",
        "    true=y_true.detach().cpu().numpy()[0,0,...]\n",
        "    DSCs=[]\n",
        "    for i in range(1,36):\n",
        "        p=(pred==i)\n",
        "        t=(true==i)\n",
        "        inter=(p & t).sum()\n",
        "        union=p.sum()+t.sum()\n",
        "        DSCs.append(2.*inter/(union+1e-5))\n",
        "    return np.mean(DSCs)\n",
        "\n",
        "################################################################################\n",
        "# 6) LOSSES\n",
        "################################################################################\n",
        "\n",
        "class Grad3d(nn.Module):\n",
        "    def __init__(self, penalty='l2', loss_mult=None):\n",
        "        super().__init__()\n",
        "        self.penalty=penalty\n",
        "        self.loss_mult=loss_mult\n",
        "    def forward(self, y_pred, y_true):\n",
        "        dy=torch.abs(y_pred[:,:,1:,:,:]-y_pred[:,:,:-1,:,:])\n",
        "        dx=torch.abs(y_pred[:,:,:,1:,:]-y_pred[:,:,:,:-1,:])\n",
        "        dz=torch.abs(y_pred[:,:,:,:,1:]-y_pred[:,:,:,:,:-1])\n",
        "        if self.penalty=='l2':\n",
        "            dy=dy*dy\n",
        "            dx=dx*dx\n",
        "            dz=dz*dz\n",
        "        d=torch.mean(dx)+torch.mean(dy)+torch.mean(dz)\n",
        "        grad=d/3.0\n",
        "        if self.loss_mult is not None:\n",
        "            grad*=self.loss_mult\n",
        "        return grad\n",
        "\n",
        "class NCC_vxm(nn.Module):\n",
        "    def __init__(self, win=None):\n",
        "        super().__init__()\n",
        "        self.win=win\n",
        "    def forward(self, y_true, y_pred):\n",
        "        Ii=y_true\n",
        "        Ji=y_pred\n",
        "        ndims=len(list(Ii.size()))-2\n",
        "        assert ndims in [1,2,3]\n",
        "        win=[9]*ndims if self.win is None else self.win\n",
        "        sum_filt=torch.ones([1,1,*win]).to(Ii.device)\n",
        "        pad_no=win[0]//2\n",
        "        if ndims==1:\n",
        "            stride=(1,)\n",
        "            padding=(pad_no,)\n",
        "        elif ndims==2:\n",
        "            stride=(1,1)\n",
        "            padding=(pad_no,pad_no)\n",
        "        else:\n",
        "            stride=(1,1,1)\n",
        "            padding=(pad_no,pad_no,pad_no)\n",
        "        conv_fn=getattr(F, f'conv{ndims}d')\n",
        "        I2=Ii*Ii\n",
        "        J2=Ji*Ji\n",
        "        IJ=Ii*Ji\n",
        "        I_sum=conv_fn(Ii, sum_filt, stride=stride, padding=padding)\n",
        "        J_sum=conv_fn(Ji, sum_filt, stride=stride, padding=padding)\n",
        "        I2_sum=conv_fn(I2, sum_filt, stride=stride, padding=padding)\n",
        "        J2_sum=conv_fn(J2, sum_filt, stride=stride, padding=padding)\n",
        "        IJ_sum=conv_fn(IJ, sum_filt, stride=stride, padding=padding)\n",
        "        win_size=np.prod(win)\n",
        "        u_I=I_sum/win_size\n",
        "        u_J=J_sum/win_size\n",
        "        cross=IJ_sum - u_J*I_sum - u_I*J_sum + u_I*u_J*win_size\n",
        "        I_var=I2_sum-2*u_I*I_sum+u_I*u_I*win_size\n",
        "        J_var=J2_sum-2*u_J*J_sum+u_J*u_J*win_size\n",
        "        cc=cross*cross/(I_var*J_var+1e-5)\n",
        "        return -torch.mean(cc)\n",
        "\n",
        "################################################################################\n",
        "# 7) SWIN TRANSFORMER + UTSRMorph MODEL\n",
        "################################################################################\n",
        "\n",
        "class CA(nn.Module):\n",
        "    \"\"\"Channel attention used in RCAN.\"\"\"\n",
        "    def __init__(self,num_feat,squeeze_factor=16):\n",
        "        super().__init__()\n",
        "        self.attention=nn.Sequential(\n",
        "            nn.AdaptiveAvgPool3d(1),\n",
        "            nn.Conv3d(num_feat,num_feat//squeeze_factor,1,padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(num_feat//squeeze_factor,num_feat,1,padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        y=self.attention(x)\n",
        "        return x*y\n",
        "\n",
        "class CAB(nn.Module):\n",
        "    def __init__(self,num_feat,compress_ratio=3,squeeze_factor=30):\n",
        "        super().__init__()\n",
        "        self.cab=nn.Sequential(\n",
        "            nn.Conv3d(num_feat,num_feat//compress_ratio,3,1,1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv3d(num_feat//compress_ratio,num_feat,3,1,1),\n",
        "            CA(num_feat,squeeze_factor)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.cab(x)\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self,in_features,hidden_features=None,out_features=None,act_layer=nn.GELU,drop=0.):\n",
        "        super().__init__()\n",
        "        out_features=out_features or in_features\n",
        "        hidden_features=hidden_features or in_features\n",
        "        self.fc1=nn.Linear(in_features,hidden_features)\n",
        "        self.act=act_layer()\n",
        "        self.fc2=nn.Linear(hidden_features,out_features)\n",
        "        self.drop=nn.Dropout(drop)\n",
        "    def forward(self,x):\n",
        "        x=self.fc1(x)\n",
        "        x=self.act(x)\n",
        "        x=self.drop(x)\n",
        "        x=self.fc2(x)\n",
        "        x=self.drop(x)\n",
        "        return x\n",
        "\n",
        "def window_partition(x,window_size):\n",
        "    B,H,W,L,C=x.shape\n",
        "    x=x.view(B,H//window_size[0],window_size[0],W//window_size[1],window_size[1],L//window_size[2],window_size[2],C)\n",
        "    windows=x.permute(0,1,3,5,2,4,6,7).contiguous().view(-1,window_size[0],window_size[1],window_size[2],C)\n",
        "    return windows\n",
        "\n",
        "def window_reverse(windows,window_size,H,W,L):\n",
        "    B=int(windows.shape[0]/(H*W*L/window_size[0]/window_size[1]/window_size[2]))\n",
        "    x=windows.view(B,H//window_size[0],W//window_size[1],L//window_size[2],window_size[0],window_size[1],window_size[2],-1)\n",
        "    x=x.permute(0,1,4,2,5,3,6,7).contiguous().view(B,H,W,L,-1)\n",
        "    return x\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self,dim,window_size,num_heads,qkv_bias=True,qk_scale=None,rpe=True,attn_drop=0.,proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.dim=dim\n",
        "        self.window_size=window_size\n",
        "        self.num_heads=num_heads\n",
        "        head_dim=dim//num_heads\n",
        "        self.scale=qk_scale or head_dim**-0.5\n",
        "        self.rpe=rpe\n",
        "        # relative pos\n",
        "        table_size=(2*window_size[0]-1)*(2*window_size[1]-1)*(2*window_size[2]-1)\n",
        "        self.relative_position_bias_table=nn.Parameter(torch.zeros(table_size,num_heads))\n",
        "        trunc_normal_(self.relative_position_bias_table,std=.02)\n",
        "        coords_h=torch.arange(window_size[0])\n",
        "        coords_w=torch.arange(window_size[1])\n",
        "        coords_t=torch.arange(window_size[2])\n",
        "        coords=torch.stack(torch.meshgrid([coords_h,coords_w,coords_t]))\n",
        "        coords_flat=coords.flatten(1)\n",
        "        if rpe:\n",
        "            rel=coords_flat[:,:,None]-coords_flat[:,None,:]\n",
        "            rel=rel.permute(1,2,0).contiguous()\n",
        "            rel[...,0]+=window_size[0]-1\n",
        "            rel[...,1]+=window_size[1]-1\n",
        "            rel[...,2]+=window_size[2]-1\n",
        "            pos_factor=(2*window_size[1]-1)*(2*window_size[2]-1)\n",
        "            rel[...,0]*=pos_factor\n",
        "            rel[...,1]*=(2*window_size[2]-1)\n",
        "            rel_index=rel.sum(-1)\n",
        "            self.register_buffer(\"relative_position_index\",rel_index)\n",
        "        self.qkv=nn.Linear(dim,dim*3,bias=qkv_bias)\n",
        "        self.attn_drop=nn.Dropout(attn_drop)\n",
        "        self.proj=nn.Linear(dim,dim)\n",
        "        self.proj_drop=nn.Dropout(proj_drop)\n",
        "        self.softmax=nn.Softmax(dim=-1)\n",
        "    def forward(self,x,mask=None):\n",
        "        B_,N,C=x.shape\n",
        "        qkv=self.qkv(x).reshape(B_,N,3,self.num_heads,C//self.num_heads).permute(2,0,3,1,4)\n",
        "        q,k,v=qkv[0],qkv[1],qkv[2]\n",
        "        q=q*self.scale\n",
        "        attn=q@k.transpose(-2,-1)\n",
        "        if self.rpe:\n",
        "            table_size=self.window_size[0]*self.window_size[1]*self.window_size[2]\n",
        "            rpb=self.relative_position_bias_table[self.relative_position_index.view(-1)]\n",
        "            rpb=rpb.view(table_size,table_size,self.num_heads).permute(2,0,1)\n",
        "            attn=attn+rpb.unsqueeze(0)\n",
        "        if mask is not None:\n",
        "            nW=mask.shape[0]\n",
        "            attn=attn.view(B_//nW,nW,self.num_heads,N,N)\n",
        "            attn=attn+mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn=attn.view(-1,self.num_heads,N,N)\n",
        "            attn=self.softmax(attn)\n",
        "        else:\n",
        "            attn=self.softmax(attn)\n",
        "        attn=self.attn_drop(attn)\n",
        "        x=(attn@v).transpose(1,2).reshape(B_,N,C)\n",
        "        x=self.proj(x)\n",
        "        x=self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class FAB(nn.Module):\n",
        "    \"\"\"\n",
        "    Fusion Attention Block = W-MSA + local conv\n",
        "    \"\"\"\n",
        "    def __init__(self,dim,num_heads,window_size=(7,7,7),shift_size=(0,0,0),\n",
        "                 mlp_ratio=4.,qkv_bias=True,qk_scale=None,rpe=True,drop=0.,\n",
        "                 attn_drop=0.,drop_path=0.,act_layer=nn.GELU,norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim=dim\n",
        "        self.num_heads=num_heads\n",
        "        self.window_size=window_size\n",
        "        self.shift_size=shift_size\n",
        "        self.norm1=norm_layer(dim)\n",
        "        self.attn=WindowAttention(dim,window_size,num_heads,qkv_bias,qk_scale,rpe,attn_drop,drop)\n",
        "        self.drop_path=DropPath(drop_path) if drop_path>0 else nn.Identity()\n",
        "        self.norm2=norm_layer(dim)\n",
        "        mlp_hidden=int(dim*mlp_ratio)\n",
        "        self.mlp=Mlp(dim,mlp_hidden,act_layer=act_layer,drop=drop)\n",
        "        self.conv_block=CAB(dim,3,30)\n",
        "        self.H=self.W=self.T=None\n",
        "    def forward(self,x,mask_matrix):\n",
        "        H,W,T=self.H,self.W,self.T\n",
        "        B,L,C=x.shape\n",
        "        shortcut=x\n",
        "        x=self.norm1(x)\n",
        "        x_3d=x.view(B,H,W,T,C)\n",
        "        conv_x=self.conv_block(x_3d.permute(0,4,1,2,3))\n",
        "        conv_x=conv_x.permute(0,2,3,4,1).contiguous().view(B,H*W*T,C)\n",
        "        pad_r=(self.window_size[0]-H%self.window_size[0])%self.window_size[0]\n",
        "        pad_b=(self.window_size[1]-W%self.window_size[1])%self.window_size[1]\n",
        "        pad_h=(self.window_size[2]-T%self.window_size[2])%self.window_size[2]\n",
        "        x_3d=F.pad(x_3d,(0,0,0,pad_h,0,pad_b,0,pad_r))\n",
        "        Hp,Wp,Tp=x_3d.shape[1],x_3d.shape[2],x_3d.shape[3]\n",
        "        if any(self.shift_size):\n",
        "            shifted_x=torch.roll(x_3d,shifts=(-self.shift_size[0],-self.shift_size[1],-self.shift_size[2]),dims=(1,2,3))\n",
        "            attn_mask=mask_matrix\n",
        "        else:\n",
        "            shifted_x=x_3d\n",
        "            attn_mask=None\n",
        "        x_windows=window_partition(shifted_x,self.window_size)\n",
        "        x_windows=x_windows.view(-1,self.window_size[0]*self.window_size[1]*self.window_size[2],C)\n",
        "        attn_windows=self.attn(x_windows,attn_mask)\n",
        "        attn_windows=attn_windows.view(-1,self.window_size[0],self.window_size[1],self.window_size[2],C)\n",
        "        shifted_x=window_reverse(attn_windows,self.window_size,Hp,Wp,Tp)\n",
        "        if any(self.shift_size):\n",
        "            x_3d=torch.roll(shifted_x,shifts=(self.shift_size[0],self.shift_size[1],self.shift_size[2]),dims=(1,2,3))\n",
        "        else:\n",
        "            x_3d=shifted_x\n",
        "        if pad_r>0 or pad_b>0 or pad_h>0:\n",
        "            x_3d=x_3d[:,:H,:W,:T,:].contiguous()\n",
        "        x_attn=x_3d.view(B,H*W*T,C)\n",
        "        x=shortcut+self.drop_path(x_attn)+conv_x*0.01\n",
        "        x=x+self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self,dim,norm_layer=nn.LayerNorm,reduce_factor=2):\n",
        "        super().__init__()\n",
        "        self.dim=dim\n",
        "        self.reduction=nn.Linear(8*dim,(8//reduce_factor)*dim,bias=False)\n",
        "        self.norm=norm_layer(8*dim)\n",
        "    def forward(self,x,H,W,T):\n",
        "        B,L,C=x.shape\n",
        "        x_3d=x.view(B,H,W,T,C)\n",
        "        x0=x_3d[:,0::2,0::2,0::2,:]\n",
        "        x1=x_3d[:,1::2,0::2,0::2,:]\n",
        "        x2=x_3d[:,0::2,1::2,0::2,:]\n",
        "        x3=x_3d[:,0::2,0::2,1::2,:]\n",
        "        x4=x_3d[:,1::2,1::2,0::2,:]\n",
        "        x5=x_3d[:,0::2,1::2,1::2,:]\n",
        "        x6=x_3d[:,1::2,0::2,1::2,:]\n",
        "        x7=x_3d[:,1::2,1::2,1::2,:]\n",
        "        x_cat=torch.cat([x0,x1,x2,x3,x4,x5,x6,x7],-1)\n",
        "        x_cat=x_cat.view(B,-1,8*C)\n",
        "        x_cat=self.norm(x_cat)\n",
        "        x_cat=self.reduction(x_cat)\n",
        "        return x_cat\n",
        "\n",
        "class BasicLayer(nn.Module):\n",
        "    def __init__(self,dim,depth,num_heads,window_size=(7,7,7),mlp_ratio=4.,qkv_bias=True,\n",
        "                 qk_scale=None,rpe=True,drop=0.,attn_drop=0.,drop_path=0.,norm_layer=nn.LayerNorm,\n",
        "                 downsample=None,use_checkpoint=False,pat_merg_rf=2):\n",
        "        super().__init__()\n",
        "        self.window_size=window_size\n",
        "        self.shift_size=(window_size[0]//2,window_size[1]//2,window_size[2]//2)\n",
        "        self.depth=depth\n",
        "        self.use_checkpoint=use_checkpoint\n",
        "        dpr=drop_path if isinstance(drop_path,list) else [drop_path]*depth\n",
        "        self.blocks=nn.ModuleList([\n",
        "            FAB(dim,num_heads,window_size,(0,0,0) if (i%2==0) else self.shift_size,\n",
        "                mlp_ratio,qkv_bias,qk_scale,rpe,drop,attn_drop,\n",
        "                dpr[i],norm_layer=norm_layer)\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "        self.downsample=downsample(dim=dim,norm_layer=norm_layer,reduce_factor=pat_merg_rf) if downsample else None\n",
        "    def forward(self,x,H,W,T):\n",
        "        import numpy as np\n",
        "        Hp=int(np.ceil(H/self.window_size[0]))*self.window_size[0]\n",
        "        Wp=int(np.ceil(W/self.window_size[1]))*self.window_size[1]\n",
        "        Tp=int(np.ceil(T/self.window_size[2]))*self.window_size[2]\n",
        "        img_mask=torch.zeros((1,Hp,Wp,Tp,1),device=x.device)\n",
        "        h_slices=(slice(0,-self.window_size[0]),slice(-self.window_size[0],-self.shift_size[0]),slice(-self.shift_size[0],None))\n",
        "        w_slices=(slice(0,-self.window_size[1]),slice(-self.window_size[1],-self.shift_size[1]),slice(-self.shift_size[1],None))\n",
        "        t_slices=(slice(0,-self.window_size[2]),slice(-self.window_size[2],-self.shift_size[2]),slice(-self.shift_size[2],None))\n",
        "        cnt=0\n",
        "        for hh in h_slices:\n",
        "            for ww in w_slices:\n",
        "                for tt in t_slices:\n",
        "                    img_mask[:,hh,ww,tt,:]=cnt\n",
        "                    cnt+=1\n",
        "        mask_windows=window_partition(img_mask,self.window_size)\n",
        "        mask_windows=mask_windows.view(-1,self.window_size[0]*self.window_size[1]*self.window_size[2])\n",
        "        attn_mask=mask_windows.unsqueeze(1)-mask_windows.unsqueeze(2)\n",
        "        attn_mask=attn_mask.masked_fill(attn_mask!=0,float(-100.0)).masked_fill(attn_mask==0,float(0.0))\n",
        "        for blk in self.blocks:\n",
        "            blk.H,blk.W,blk.T=H,W,T\n",
        "            if self.use_checkpoint:\n",
        "                x=checkpoint.checkpoint(blk,x,attn_mask)\n",
        "            else:\n",
        "                x=blk(x,attn_mask)\n",
        "        if self.downsample is not None:\n",
        "            x_down=self.downsample(x,H,W,T)\n",
        "            Wh,Ww,Wt=(H+1)//2,(W+1)//2,(T+1)//2\n",
        "            return x,H,W,T,x_down,Wh,Ww,Wt\n",
        "        else:\n",
        "            return x,H,W,T,x,H,W,T\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self,patch_size=4,in_chans=3,embed_dim=96,norm_layer=None):\n",
        "        super().__init__()\n",
        "        patch_size=to_3tuple(patch_size)\n",
        "        self.patch_size=patch_size\n",
        "        self.in_chans=in_chans\n",
        "        self.embed_dim=embed_dim\n",
        "        self.proj=nn.Conv3d(in_chans,embed_dim,kernel_size=patch_size,stride=patch_size)\n",
        "        self.norm=norm_layer(embed_dim) if norm_layer else None\n",
        "    def forward(self,x):\n",
        "        B,C,H,W,D=x.shape\n",
        "        ps=self.patch_size\n",
        "        # pad if needed\n",
        "        if D%ps[2]!=0:\n",
        "            x=F.pad(x,(0,ps[2]-D%ps[2]))\n",
        "        if W%ps[1]!=0:\n",
        "            x=F.pad(x,(0,0,0,ps[1]-W%ps[1]))\n",
        "        if H%ps[0]!=0:\n",
        "            x=F.pad(x,(0,0,0,0,0,ps[0]-H%ps[0]))\n",
        "        x=self.proj(x)\n",
        "        if self.norm is not None:\n",
        "            Hp,Wp,Dp=x.shape[2],x.shape[3],x.shape[4]\n",
        "            x=x.flatten(2).transpose(1,2)\n",
        "            x=self.norm(x)\n",
        "            x=x.transpose(1,2).view(B,self.embed_dim,Hp,Wp,Dp)\n",
        "        return x\n",
        "\n",
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self,pretrain_img_size=224,patch_size=4,in_chans=3,embed_dim=96,\n",
        "                 depths=[2,2,2,2],num_heads=[3,3,6,6],window_size=(7,7,7),mlp_ratio=4.,\n",
        "                 qkv_bias=True,qk_scale=None,drop_rate=0.,attn_drop_rate=0.,drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm,ape=False,spe=False,rpe=True,patch_norm=True,\n",
        "                 out_indices=(0,1,2,3),frozen_stages=-1,use_checkpoint=False,pat_merg_rf=2):\n",
        "        super().__init__()\n",
        "        self.patch_embed=PatchEmbed(patch_size,in_chans,embed_dim,norm_layer if patch_norm else None)\n",
        "        dpr=[x.item() for x in torch.linspace(0,drop_path_rate,sum(depths))]\n",
        "        self.layers=nn.ModuleList()\n",
        "        cur=0\n",
        "        self.num_layers=len(depths)\n",
        "        self.out_indices=out_indices\n",
        "        for i in range(self.num_layers):\n",
        "            layer=BasicLayer(dim=int(embed_dim*2**i),\n",
        "                             depth=depths[i],\n",
        "                             num_heads=num_heads[i],\n",
        "                             window_size=window_size,\n",
        "                             mlp_ratio=mlp_ratio,\n",
        "                             qkv_bias=qkv_bias,\n",
        "                             qk_scale=qk_scale,\n",
        "                             rpe=rpe,\n",
        "                             drop=drop_rate,\n",
        "                             attn_drop=attn_drop_rate,\n",
        "                             drop_path=dpr[cur:cur+depths[i]],\n",
        "                             norm_layer=norm_layer,\n",
        "                             downsample=PatchMerging if (i<self.num_layers-1) else None,\n",
        "                             use_checkpoint=use_checkpoint,\n",
        "                             pat_merg_rf=pat_merg_rf)\n",
        "            self.layers.append(layer)\n",
        "            cur+=depths[i]\n",
        "        num_features=[int(embed_dim*2**i) for i in range(self.num_layers)]\n",
        "        self.num_features=num_features\n",
        "        for i_layer in out_indices:\n",
        "            layer=norm_layer(num_features[i_layer])\n",
        "            self.add_module(f\"norm{i_layer}\", layer)\n",
        "    def forward(self,x):\n",
        "        x=self.patch_embed(x)\n",
        "        B,C,Hp,Wp,Dp=x.shape\n",
        "        x=x.flatten(2).transpose(1,2)\n",
        "        outs=[]\n",
        "        for i,layer in enumerate(self.layers):\n",
        "            x_out,H,W,D,x,Hp,Wp,Dp=layer(x,Hp,Wp,Dp)\n",
        "            if i in self.out_indices:\n",
        "                norm_layer=getattr(self,f\"norm{i}\")\n",
        "                x_out=norm_layer(x_out)\n",
        "                out_f=x_out.view(B,H,W,D,self.num_features[i]).permute(0,4,1,2,3).contiguous()\n",
        "                outs.append(out_f)\n",
        "        return outs\n",
        "\n",
        "class PixelShuffle3d(nn.Module):\n",
        "    def __init__(self,scale):\n",
        "        super().__init__()\n",
        "        self.scale=scale\n",
        "    def forward(self,inp):\n",
        "        b,c,d,h,w=inp.size()\n",
        "        oc=c//(self.scale**3)\n",
        "        od,oh,ow=d*self.scale,h*self.scale,w*self.scale\n",
        "        out=inp.view(b,oc,self.scale,self.scale,self.scale,d,h,w)\n",
        "        out=out.permute(0,1,5,2,6,3,7,4).contiguous()\n",
        "        out=out.view(b,oc,od,oh,ow)\n",
        "        return out\n",
        "\n",
        "class ConvergeHead(nn.Module):\n",
        "    def __init__(self,in_dim,up_ratio,kernel_size,padding):\n",
        "        super().__init__()\n",
        "        self.in_dim=in_dim\n",
        "        self.up_ratio=up_ratio\n",
        "        self.conv=nn.Conv3d(in_dim,(up_ratio**3)*in_dim,kernel_size,1,padding,1,in_dim)\n",
        "        self.apply(self._init_weights)\n",
        "    def forward(self,x):\n",
        "        hp=self.conv(x)\n",
        "        poxel=PixelShuffle3d(self.up_ratio)\n",
        "        hp=poxel(hp)\n",
        "        return hp\n",
        "    def _init_weights(self,m):\n",
        "        if isinstance(m,(nn.Conv3d,nn.Linear)):\n",
        "            nn.init.normal_(m.weight,std=0.001)\n",
        "            nn.init.constant_(m.bias,0)\n",
        "        elif isinstance(m,nn.BatchNorm3d):\n",
        "            nn.init.constant_(m.weight,1)\n",
        "            nn.init.constant_(m.bias,0)\n",
        "\n",
        "class Conv3dReLU(nn.Sequential):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size,padding=0,stride=1,use_batchnorm=True):\n",
        "        conv=nn.Conv3d(in_channels,out_channels,kernel_size,stride=stride,padding=padding,bias=False)\n",
        "        relu=nn.LeakyReLU(inplace=True)\n",
        "        nm=nn.BatchNorm3d(out_channels) if use_batchnorm else nn.InstanceNorm3d(out_channels)\n",
        "        super().__init__(conv,nm,relu)\n",
        "\n",
        "class SR(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,skip_channels=0,use_batchnorm=True):\n",
        "        super().__init__()\n",
        "        self.up=ConvergeHead(in_channels,2,3,1)\n",
        "        self.conv1=Conv3dReLU(in_channels+skip_channels,out_channels,3,1,use_batchnorm=use_batchnorm)\n",
        "        self.conv2=Conv3dReLU(out_channels,out_channels,3,1,use_batchnorm=use_batchnorm)\n",
        "    def forward(self,x,skip=None):\n",
        "        x=self.up(x)\n",
        "        if skip is not None:\n",
        "            x=torch.cat([x,skip],dim=1)\n",
        "        x=self.conv1(x)\n",
        "        x=self.conv2(x)\n",
        "        return x\n",
        "\n",
        "class RegistrationHead(nn.Sequential):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size=3,upsampling=1):\n",
        "        conv3d=nn.Conv3d(in_channels,out_channels,kernel_size,padding=kernel_size//2)\n",
        "        conv3d.weight=nn.Parameter(torch.normal(0,1e-5,size=conv3d.weight.shape))\n",
        "        conv3d.bias=nn.Parameter(torch.zeros(conv3d.bias.shape))\n",
        "        super().__init__(conv3d)\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self,size,mode='bilinear'):\n",
        "        super().__init__()\n",
        "        self.mode=mode\n",
        "        vectors=[torch.arange(0,s) for s in size]\n",
        "        grids=torch.meshgrid(vectors)\n",
        "        grid=torch.stack(grids)\n",
        "        grid=torch.unsqueeze(grid,0).float()\n",
        "        self.register_buffer('grid',grid)\n",
        "    def forward(self,src,flow):\n",
        "        new_locs=self.grid+flow\n",
        "        shape=flow.shape[2:]\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:,i,...]=2*(new_locs[:,i,...]/(shape[i]-1)-0.5)\n",
        "        if len(shape)==3:\n",
        "            new_locs=new_locs.permute(0,2,3,4,1)\n",
        "            new_locs=new_locs[..., [2,1,0]]\n",
        "        return F.grid_sample(src,new_locs,align_corners=False,mode=self.mode)\n",
        "\n",
        "def get_UTSRMorph_config():\n",
        "    config=ml_collections.ConfigDict()\n",
        "    config.if_transskip=True\n",
        "    config.if_convskip=True\n",
        "    config.patch_size=4\n",
        "    config.in_chans=2\n",
        "    config.embed_dim=96\n",
        "    config.depths=(2,2,2,2)\n",
        "    config.num_heads=(4,4,4,4)\n",
        "    config.window_size=(5,6,7)\n",
        "    config.mlp_ratio=4\n",
        "    config.pat_merg_rf=4\n",
        "    config.qkv_bias=False\n",
        "    config.drop_rate=0\n",
        "    config.drop_path_rate=0.3\n",
        "    config.ape=False\n",
        "    config.spe=False\n",
        "    config.rpe=True\n",
        "    config.patch_norm=True\n",
        "    config.use_checkpoint=False\n",
        "    config.out_indices=(0,1,2,3)\n",
        "    config.reg_head_chan=16\n",
        "    config.img_size=(160,192,224)\n",
        "    return config\n",
        "\n",
        "class UTSRMorph(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.if_convskip=config.if_convskip\n",
        "        self.if_transskip=config.if_transskip\n",
        "        embed_dim=config.embed_dim\n",
        "        self.transformer=SwinTransformer(\n",
        "            patch_size=config.patch_size,\n",
        "            in_chans=config.in_chans,\n",
        "            embed_dim=config.embed_dim,\n",
        "            depths=config.depths,\n",
        "            num_heads=config.num_heads,\n",
        "            window_size=config.window_size,\n",
        "            mlp_ratio=config.mlp_ratio,\n",
        "            qkv_bias=config.qkv_bias,\n",
        "            drop_rate=config.drop_rate,\n",
        "            drop_path_rate=config.drop_path_rate,\n",
        "            ape=config.ape,\n",
        "            spe=config.spe,\n",
        "            rpe=config.rpe,\n",
        "            patch_norm=config.patch_norm,\n",
        "            use_checkpoint=config.use_checkpoint,\n",
        "            out_indices=config.out_indices,\n",
        "            pat_merg_rf=config.pat_merg_rf\n",
        "        )\n",
        "        self.up0=SR(embed_dim*8,embed_dim*4,skip_channels=(embed_dim*4 if self.if_transskip else 0),use_batchnorm=False)\n",
        "        self.up1=SR(embed_dim*4,embed_dim*2,skip_channels=(embed_dim*2 if self.if_transskip else 0),use_batchnorm=False)\n",
        "        self.up2=SR(embed_dim*2,embed_dim,skip_channels=(embed_dim if self.if_transskip else 0),use_batchnorm=False)\n",
        "        self.up3=SR(embed_dim,config.reg_head_chan,skip_channels=(embed_dim//2 if self.if_convskip else 0),use_batchnorm=False)\n",
        "        self.c1=Conv3dReLU(2,embed_dim//2,3,1,use_batchnorm=False)\n",
        "        self.reg_head=RegistrationHead(config.reg_head_chan,3,3)\n",
        "        self.spatial_trans=SpatialTransformer(config.img_size)\n",
        "        self.avg_pool=nn.AvgPool3d(3,stride=2,padding=1)\n",
        "        self.up=ConvergeHead(3,2,3,1)\n",
        "    def forward(self,x):\n",
        "        source=x[:,0:1,:,:,:]\n",
        "        if self.if_convskip:\n",
        "            x_s1=self.avg_pool(x)\n",
        "            f4=self.c1(x_s1)\n",
        "        else:\n",
        "            f4=None\n",
        "        out_feats=self.transformer(x)\n",
        "        if self.if_transskip:\n",
        "            f1=out_feats[-2]\n",
        "            f2=out_feats[-3]\n",
        "            f3=out_feats[-4]\n",
        "        else:\n",
        "            f1=None; f2=None; f3=None\n",
        "        x=self.up0(out_feats[-1],f1)\n",
        "        x=self.up1(x,f2)\n",
        "        x=self.up2(x,f3)\n",
        "        x=self.up3(x,f4)\n",
        "        flow=self.reg_head(x)\n",
        "        flow=self.up(flow)\n",
        "        out=self.spatial_trans(source,flow)\n",
        "        return out,flow\n",
        "\n",
        "################################################################################\n",
        "# 8) TRAIN SCRIPT / INFERENCE SCRIPT\n",
        "################################################################################\n",
        "\n",
        "def train_UTSRMorph():\n",
        "    print(\"Train script: you can implement your training logic here.\")\n",
        "    # Example minimal usage\n",
        "    config=get_UTSRMorph_config()\n",
        "    model=UTSRMorph(config).cuda()\n",
        "    # create dataset, loader, etc.\n",
        "    print(\"Model built. Ready to train...\")\n",
        "\n",
        "def infer_UTSRMorph():\n",
        "    print(\"Inference script: implement your inference logic here.\")\n",
        "    config=get_UTSRMorph_config()\n",
        "    model=UTSRMorph(config).cuda()\n",
        "    print(\"Model built. Ready to infer...\")\n",
        "\n",
        "################################################################################\n",
        "# 9) MAIN (Optional)\n",
        "################################################################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"All code loaded in one place!\")\n",
        "    # Example: train_UTSRMorph()\n",
        "    # Example: infer_UTSRMorph()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlw9gh8USM2k",
        "outputId": "f37a262f-a522-4665-edf3-fbb4d58fa420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All code loaded in one place!\n"
          ]
        }
      ]
    }
  ]
}