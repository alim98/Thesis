{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwCkzDHpngqJiFzWnlvkn5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb84bff4a66d4bc78985192206732cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_253dc774b2434c47aa20db295891bf6b",
              "IPY_MODEL_b74f01fbc17f4570a72c07d3cceede84",
              "IPY_MODEL_8c4e397ae80141e7b4d488bf5b7c1c62"
            ],
            "layout": "IPY_MODEL_cbd3ba8b0d8c4485a4b93727bbf06451"
          }
        },
        "253dc774b2434c47aa20db295891bf6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_287d77b6e0b848cb864f9356a6c4bdf9",
            "placeholder": "​",
            "style": "IPY_MODEL_7e889a30ebf4430199db401e0a96a48b",
            "value": "Epoch 1: 100%"
          }
        },
        "b74f01fbc17f4570a72c07d3cceede84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_572b134637eb46298ac3d523407d46f4",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe0421e2047e4971a7f5b20650a66283",
            "value": 50
          }
        },
        "8c4e397ae80141e7b4d488bf5b7c1c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_467eb8df001a4d15ba044d2c842c1d96",
            "placeholder": "​",
            "style": "IPY_MODEL_3c3b8c6839004147a5c5b2c37ebb2938",
            "value": " 50/50 [00:01&lt;00:00, 37.88it/s, v_num=3]"
          }
        },
        "cbd3ba8b0d8c4485a4b93727bbf06451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "287d77b6e0b848cb864f9356a6c4bdf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e889a30ebf4430199db401e0a96a48b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "572b134637eb46298ac3d523407d46f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe0421e2047e4971a7f5b20650a66283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "467eb8df001a4d15ba044d2c842c1d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c3b8c6839004147a5c5b2c37ebb2938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/Thesis/blob/main/EnhancedVMAEPro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZYI889qNGxu",
        "outputId": "6f7d135e-1e00-4250-fad0-4c15ec49a976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip -q install torch torchvision torchaudio\n",
        "!pip -q install einops\n",
        "!pip -q install monai\n",
        "!pip -q install torchmetrics\n",
        "!pip -q install timm\n",
        "!pip -q install pytorch-lightning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: This is a simplified version of the decoder. Incorporating generative and diffusion-based enhancements like Latent Diffusion Models (LDM) requires additional implementation, which can be complex. For demonstration purposes, we'll keep it basic, but you should consider integrating libraries like Diffusers or custom implementations for LDM."
      ],
      "metadata": {
        "id": "C_-cvWIJNZpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pytorch_lightning as pl\n",
        "# from pytorch_lightning import Trainer\n",
        "# from pytorch_lightning.callbacks import ModelSummary"
      ],
      "metadata": {
        "id": "QVarrqcZeXS_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from einops import rearrange\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from monai.transforms import (\n",
        "     Compose, LoadImaged, ScaleIntensityd, EnsureTyped, Resized\n",
        ")\n",
        "from monai.data import CacheDataset, load_decathlon_datalist\n",
        "from torchmetrics import Dice\n",
        "import timm\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelSummary"
      ],
      "metadata": {
        "id": "iOACnZ2teJze"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelSummary\n",
        "from functools import partial\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape [L, N, E]\n",
        "        Returns:\n",
        "            Tensor of shape [L, N, E]\n",
        "        \"\"\"\n",
        "        # Multihead Attention\n",
        "        attn_output, _ = self.attn(x, x, x)  # [L, N, E]\n",
        "        x = x + attn_output\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.mlp(x)  # [L, N, E]\n",
        "        x = x + mlp_output\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x  # [L, N, E]\n",
        "\n",
        "class HierarchicalViTEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim=768,\n",
        "        num_heads=12,\n",
        "        num_layers=6,\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.1,\n",
        "        num_stages=3,\n",
        "        patch_size=(4, 16, 16)  # Adjusted patch size\n",
        "    ):\n",
        "        super(HierarchicalViTEncoder, self).__init__()\n",
        "        self.num_stages = num_stages\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Define transformer layers for each stage\n",
        "        self.transformer_layers = nn.ModuleList()\n",
        "        for stage in range(num_stages):\n",
        "            for _ in range(num_layers):\n",
        "                self.transformer_layers.append(\n",
        "                    TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "                )\n",
        "\n",
        "        # Downsampling layers between stages\n",
        "        self.downsamples = nn.ModuleList()\n",
        "        for stage in range(num_stages - 1):\n",
        "            self.downsamples.append(\n",
        "                nn.Conv3d(embed_dim, embed_dim, kernel_size=2, stride=2)  # Keep embed_dim constant\n",
        "            )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape [B * p, E] where p = D * H * W\n",
        "        Returns:\n",
        "            features: list of feature maps at different scales\n",
        "            x: Tensor of shape [B * p_new, E_new]\n",
        "        \"\"\"\n",
        "        features = []\n",
        "\n",
        "        # Define initial spatial dimensions; adjusted to 4x4x4\n",
        "        D, H, W = 4, 4, 4  # Adjusted to prevent zero dimensions\n",
        "\n",
        "        # Calculate patch count per batch\n",
        "        B_p, E = x.shape\n",
        "        p = D * H * W\n",
        "        assert B_p % p == 0, f\"Batch size {B_p} is not divisible by patch count {p}\"\n",
        "        B = B_p // p\n",
        "\n",
        "        # Reshape to [B, E, D, H, W]\n",
        "        x = x.view(B, E, D, H, W)\n",
        "        # print(f\"Encoder Input Shape: {x.shape}\")  # Debug statement\n",
        "\n",
        "        for stage in range(self.num_stages):\n",
        "            # Apply transformer layers\n",
        "            for _ in range(len(self.transformer_layers) // self.num_stages):\n",
        "                layer = self.transformer_layers.pop(0)\n",
        "\n",
        "                # Reshape x to [B, E, D, H, W] -> [B, p, E]\n",
        "                x_reshaped = x.view(B, E, -1).permute(0, 2, 1)  # [B, p, E]\n",
        "\n",
        "                # Transpose to [p, B, E] for transformer\n",
        "                x_transposed = x_reshaped.permute(1, 0, 2)  # [p, B, E]\n",
        "\n",
        "                # Pass through transformer layer\n",
        "                x_transformed = layer(x_transposed)  # [p, B, E]\n",
        "\n",
        "                # Transpose back to [B, p, E] and reshape to [B, E, D, H, W]\n",
        "                x = x_transformed.permute(1, 0, 2).reshape(B, E, D, H, W)  # [B, E, D, H, W]\n",
        "                # print(f\"Transformer Output Shape: {x.shape}\")  # Debug\n",
        "\n",
        "            # Collect features\n",
        "            features.append(x.clone())\n",
        "            # print(f\"Collected Feature Shape at Stage {stage}: {x.shape}\")  # Debug\n",
        "\n",
        "            if stage < self.num_stages - 1:\n",
        "                # Downsample using the corresponding downsampling layer\n",
        "                x = self.downsamples[stage](x)    # [B, E, D/2, H/2, W/2]\n",
        "                # print(f\"After Downsampling Shape: {x.shape}\")  # Debug\n",
        "\n",
        "                # Update spatial dimensions for next stage\n",
        "                D, H, W = D // 2, H // 2, W // 2\n",
        "                assert D > 0 and H > 0 and W > 0, \"Spatial dimensions reduced to zero or negative\"\n",
        "\n",
        "                # No need to flatten here as the next stage will handle reshaping\n",
        "                # Just ensure x has shape [B, E, D, H, W]\n",
        "\n",
        "        # x = self.norm(x.view(B, E, -1)).reshape(B, E, D, H, W)\n",
        "        # Apply norm on the last dimension\n",
        "        x = x.view(B, E, D * H * W).permute(0, 2, 1)  # [B, D*H*W, E]\n",
        "        x = self.norm(x).permute(0, 2, 1).reshape(B, E, D, H, W)  # Normalize on the embedding dimension\n",
        "# Apply norm on the last dimension\n",
        "        # print(f\"Encoder Output Shape: {x.shape}\")  # Debug\n",
        "        return features, x.view(B * D * H * W, E)  # Return all intermediate features and final output\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=1, embed_dim=768, patch_size=(4, 16, 16)):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv3d(\n",
        "            in_channels, embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape [B, C, D, H, W]\n",
        "        Returns:\n",
        "            patches: Tensor of shape [B * p, E]\n",
        "        \"\"\"\n",
        "        x = self.proj(x)  # [B, E, D', H', W']\n",
        "        B, E, D, H, W = x.shape\n",
        "        x = x.view(B, E, -1).permute(0, 2, 1)  # [B, p, E]\n",
        "        x = self.norm(x)\n",
        "        patches = x.view(B * x.shape[1], E)  # [B * p, E]\n",
        "        return patches\n",
        "\n",
        "class ViT_MAE_Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim=768, num_heads=12, num_layers=6, mlp_ratio=4.0, dropout=0.1):\n",
        "        super(ViT_MAE_Decoder, self).__init__()\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.output_layer = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, multi_scale_features, source, target):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape [B * p_new, E_new]\n",
        "            multi_scale_features: list of tensors from encoder\n",
        "            source, target: Additional inputs for decoder\n",
        "        Returns:\n",
        "            reconstructed_patches, warped_src, phi\n",
        "        \"\"\"\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)  # [B * p_new, E_new]\n",
        "\n",
        "        x = self.norm(x)\n",
        "        reconstructed_patches = self.output_layer(x)  # [B * p_new, E_new]\n",
        "\n",
        "        # Placeholder for warped_src and phi\n",
        "        warped_src = torch.zeros_like(reconstructed_patches)\n",
        "        phi = torch.zeros_like(reconstructed_patches)\n",
        "\n",
        "        return reconstructed_patches, warped_src, phi\n",
        "\n",
        "class RegistrationHead(nn.Module):\n",
        "    def __init__(self, in_channels=768, out_channels=3):\n",
        "        super(RegistrationHead, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv3d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv3 = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        phi = self.conv3(x)\n",
        "        return phi\n",
        "class SpatialTransformer3D(nn.Module):\n",
        "    def __init__(self, in_channels=768, out_channels=768, kernel_size=3, padding=1):\n",
        "        super(SpatialTransformer3D, self).__init__()\n",
        "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding)\n",
        "        self.norm = nn.LayerNorm(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape [B, E, D, H, W]\n",
        "        Returns:\n",
        "            Transformed tensor\n",
        "        \"\"\"\n",
        "        x = self.conv(x)\n",
        "        # Reshape for LayerNorm: [B, D, H, W, E]\n",
        "        x = x.permute(0, 2, 3, 4, 1)\n",
        "        x = self.norm(x)\n",
        "        # Permute back to [B, E, D, H, W]\n",
        "        x = x.permute(0, 4, 1, 2, 3)\n",
        "        return x\n",
        "class VMAEProLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VMAEProLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, reconstructed_patches, target):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            reconstructed_patches: Tensor of shape [B * p, E]\n",
        "            target: Tensor of shape [B * p, E]\n",
        "        Returns:\n",
        "            loss: Scalar tensor\n",
        "        \"\"\"\n",
        "        loss = self.mse(reconstructed_patches, target)\n",
        "        return loss\n",
        "\n",
        "class VMAEProModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=1,\n",
        "        embed_dim=768,\n",
        "        num_heads=12,\n",
        "        num_encoder_layers=6,\n",
        "        num_decoder_layers=6,\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.1,\n",
        "        num_stages=3,\n",
        "        patch_size=(4, 16, 16),\n",
        "        lr=1e-4\n",
        "    ):\n",
        "        super(VMAEProModel, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Patch Embedding\n",
        "        self.patch_embedding = PatchEmbedding(\n",
        "            in_channels=in_channels,\n",
        "            embed_dim=embed_dim,\n",
        "            patch_size=patch_size\n",
        "        )\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = HierarchicalViTEncoder(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_encoder_layers,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            dropout=dropout,\n",
        "            num_stages=num_stages,\n",
        "            patch_size=patch_size\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = ViT_MAE_Decoder(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_decoder_layers,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Registration Head\n",
        "        self.registration_head = RegistrationHead(in_channels=embed_dim)\n",
        "\n",
        "        # Spatial Transformer\n",
        "        self.spatial_transformer = SpatialTransformer3D(in_channels=embed_dim)\n",
        "\n",
        "        # Loss\n",
        "        self.criterion = VMAEProLoss()\n",
        "\n",
        "        # Teacher Encoder (for knowledge distillation or similar purposes)\n",
        "        self.teacher_encoder = HierarchicalViTEncoder(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_encoder_layers,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            dropout=dropout,\n",
        "            num_stages=num_stages,\n",
        "            patch_size=patch_size\n",
        "        )\n",
        "\n",
        "        # Optimizer Learning Rate\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, visible_patches, masked_indices, source, target):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            visible_patches: Tensor of shape [B, C, D, H, W]\n",
        "            masked_indices: Tensor indicating masked patches\n",
        "            source, target: Additional inputs\n",
        "        Returns:\n",
        "            reconstructed_patches, warped_src, phi\n",
        "        \"\"\"\n",
        "        # Patch Embedding\n",
        "        embeddings = self.patch_embedding(visible_patches)  # [B * p, E]\n",
        "        # print(f\"Embeddings Shape: {embeddings.shape}\")  # Debug\n",
        "\n",
        "        # Encoder\n",
        "        multi_scale_features, encoder_output = self.encoder(embeddings)  # [features list], [B * p_new, E_new]\n",
        "\n",
        "        # Decoder\n",
        "        reconstructed_patches, warped_src, phi = self.decoder(\n",
        "            encoder_output, multi_scale_features, source, target\n",
        "        )\n",
        "\n",
        "        return reconstructed_patches, warped_src, phi\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch: Tuple containing (visible_patches, masked_indices, source, target)\n",
        "        Returns:\n",
        "            loss\n",
        "        \"\"\"\n",
        "        visible_patches, masked_indices, source, target = batch\n",
        "\n",
        "        # Forward pass\n",
        "        reconstructed_patches, warped_src, phi = self.forward(\n",
        "            visible_patches, masked_indices, source, target\n",
        "        )\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.criterion(reconstructed_patches, target)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
        "        return optimizer\n",
        "class Dummy3DDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_samples=1000,\n",
        "        in_channels=1,\n",
        "        depth=16,\n",
        "        height=64,\n",
        "        width=64,\n",
        "        patch_size=(4, 16, 16)\n",
        "    ):\n",
        "        super(Dummy3DDataset, self).__init__()\n",
        "        self.num_samples = num_samples\n",
        "        self.in_channels = in_channels\n",
        "        self.depth = depth\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Generate random 3D data\n",
        "            visible_patches = torch.randn(self.in_channels, self.depth, self.height, self.width)\n",
        "\n",
        "            # Masked indices can be random for dummy data\n",
        "            masked_indices = torch.randint(0, 2, (1,))  # Placeholder\n",
        "\n",
        "            # Source can be a random tensor\n",
        "            source = torch.randn(1)\n",
        "\n",
        "            # After PatchEmbedding with patch_size=(4,16,16):\n",
        "            # D'=16//4=4, H'=64//16=4, W'=64//16=4\n",
        "            # p =4*4*4=64\n",
        "            # For encoder with num_stages=3, final p_new=1\n",
        "            # Therefore, target should be [E] = [768]\n",
        "            target = torch.randn(768)  # Single sample target without batch dimension\n",
        "\n",
        "            return visible_patches, masked_indices, source, target\n",
        "        except Exception as e:\n",
        "            print(f\"Error in __getitem__ at index {idx}: {e}\")\n",
        "            raise e\n",
        "# Instantiate the dataset\n",
        "dataset = Dummy3DDataset(\n",
        "    num_samples=100,  # Reduced number for quick testing\n",
        "    in_channels=1,\n",
        "    depth=16,\n",
        "    height=64,\n",
        "    width=64,\n",
        "    patch_size=(4, 16, 16)\n",
        ")\n",
        "\n",
        "# Create DataLoader with num_workers=0 for debugging\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,  # Must align with target's first dimension\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # Set to 0 to debug\n",
        "    pin_memory=True  # Improve performance on CUDA\n",
        ")\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = VMAEProModel(\n",
        "    in_channels=1,\n",
        "    embed_dim=768,\n",
        "    num_heads=12,\n",
        "    num_encoder_layers=6,\n",
        "    num_decoder_layers=6,\n",
        "    mlp_ratio=4.0,\n",
        "    dropout=0.1,\n",
        "    num_stages=3,\n",
        "    patch_size=(4, 16, 16),\n",
        "    lr=1e-4\n",
        ")\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "from pytorch_lightning import Trainer\n",
        "# Check if CUDA is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using device: {'cuda' if use_cuda else 'cpu'}\")\n",
        "\n",
        "# Initialize Trainer with updated arguments\n",
        "trainer = Trainer(\n",
        "    max_epochs=10,\n",
        "    accelerator='gpu' if use_cuda else 'cpu',\n",
        "    devices=1 if use_cuda else None,\n",
        "    callbacks=[ModelSummary(max_depth=3)],\n",
        "    log_every_n_steps=20  # Replaces 'progress_bar_refresh_rate'\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.fit(model, dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fb84bff4a66d4bc78985192206732cd4",
            "253dc774b2434c47aa20db295891bf6b",
            "b74f01fbc17f4570a72c07d3cceede84",
            "8c4e397ae80141e7b4d488bf5b7c1c62",
            "cbd3ba8b0d8c4485a4b93727bbf06451",
            "287d77b6e0b848cb864f9356a6c4bdf9",
            "7e889a30ebf4430199db401e0a96a48b",
            "572b134637eb46298ac3d523407d46f4",
            "fe0421e2047e4971a7f5b20650a66283",
            "467eb8df001a4d15ba044d2c842c1d96",
            "3c3b8c6839004147a5c5b2c37ebb2938"
          ]
        },
        "id": "vyb6S3hWnWDP",
        "outputId": "4ffdc0c3-850b-4c39-e78f-8531f3a668e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "   | Name                                  | Type                   | Params | Mode \n",
            "------------------------------------------------------------------------------------------\n",
            "0  | patch_embedding                       | PatchEmbedding         | 788 K  | train\n",
            "1  | patch_embedding.proj                  | Conv3d                 | 787 K  | train\n",
            "2  | patch_embedding.norm                  | LayerNorm              | 1.5 K  | train\n",
            "3  | encoder                               | HierarchicalViTEncoder | 137 M  | train\n",
            "4  | encoder.transformer_layers            | ModuleList             | 127 M  | train\n",
            "5  | encoder.transformer_layers.0          | TransformerBlock       | 7.1 M  | train\n",
            "6  | encoder.transformer_layers.1          | TransformerBlock       | 7.1 M  | train\n",
            "7  | encoder.transformer_layers.2          | TransformerBlock       | 7.1 M  | train\n",
            "8  | encoder.transformer_layers.3          | TransformerBlock       | 7.1 M  | train\n",
            "9  | encoder.transformer_layers.4          | TransformerBlock       | 7.1 M  | train\n",
            "10 | encoder.transformer_layers.5          | TransformerBlock       | 7.1 M  | train\n",
            "11 | encoder.transformer_layers.6          | TransformerBlock       | 7.1 M  | train\n",
            "12 | encoder.transformer_layers.7          | TransformerBlock       | 7.1 M  | train\n",
            "13 | encoder.transformer_layers.8          | TransformerBlock       | 7.1 M  | train\n",
            "14 | encoder.transformer_layers.9          | TransformerBlock       | 7.1 M  | train\n",
            "15 | encoder.transformer_layers.10         | TransformerBlock       | 7.1 M  | train\n",
            "16 | encoder.transformer_layers.11         | TransformerBlock       | 7.1 M  | train\n",
            "17 | encoder.transformer_layers.12         | TransformerBlock       | 7.1 M  | train\n",
            "18 | encoder.transformer_layers.13         | TransformerBlock       | 7.1 M  | train\n",
            "19 | encoder.transformer_layers.14         | TransformerBlock       | 7.1 M  | train\n",
            "20 | encoder.transformer_layers.15         | TransformerBlock       | 7.1 M  | train\n",
            "21 | encoder.transformer_layers.16         | TransformerBlock       | 7.1 M  | train\n",
            "22 | encoder.transformer_layers.17         | TransformerBlock       | 7.1 M  | train\n",
            "23 | encoder.downsamples                   | ModuleList             | 9.4 M  | train\n",
            "24 | encoder.downsamples.0                 | Conv3d                 | 4.7 M  | train\n",
            "25 | encoder.downsamples.1                 | Conv3d                 | 4.7 M  | train\n",
            "26 | encoder.norm                          | LayerNorm              | 1.5 K  | train\n",
            "27 | decoder                               | ViT_MAE_Decoder        | 43.1 M | train\n",
            "28 | decoder.transformer_layers            | ModuleList             | 42.5 M | train\n",
            "29 | decoder.transformer_layers.0          | TransformerBlock       | 7.1 M  | train\n",
            "30 | decoder.transformer_layers.1          | TransformerBlock       | 7.1 M  | train\n",
            "31 | decoder.transformer_layers.2          | TransformerBlock       | 7.1 M  | train\n",
            "32 | decoder.transformer_layers.3          | TransformerBlock       | 7.1 M  | train\n",
            "33 | decoder.transformer_layers.4          | TransformerBlock       | 7.1 M  | train\n",
            "34 | decoder.transformer_layers.5          | TransformerBlock       | 7.1 M  | train\n",
            "35 | decoder.norm                          | LayerNorm              | 1.5 K  | train\n",
            "36 | decoder.output_layer                  | Linear                 | 590 K  | train\n",
            "37 | registration_head                     | RegistrationHead       | 31.9 M | train\n",
            "38 | registration_head.conv1               | Conv3d                 | 15.9 M | train\n",
            "39 | registration_head.relu1               | ReLU                   | 0      | train\n",
            "40 | registration_head.conv2               | Conv3d                 | 15.9 M | train\n",
            "41 | registration_head.relu2               | ReLU                   | 0      | train\n",
            "42 | registration_head.conv3               | Conv3d                 | 2.3 K  | train\n",
            "43 | spatial_transformer                   | SpatialTransformer3D   | 15.9 M | train\n",
            "44 | spatial_transformer.conv              | Conv3d                 | 15.9 M | train\n",
            "45 | spatial_transformer.norm              | LayerNorm              | 1.5 K  | train\n",
            "46 | criterion                             | VMAEProLoss            | 0      | train\n",
            "47 | criterion.mse                         | MSELoss                | 0      | train\n",
            "48 | teacher_encoder                       | HierarchicalViTEncoder | 137 M  | train\n",
            "49 | teacher_encoder.transformer_layers    | ModuleList             | 127 M  | train\n",
            "50 | teacher_encoder.transformer_layers.0  | TransformerBlock       | 7.1 M  | train\n",
            "51 | teacher_encoder.transformer_layers.1  | TransformerBlock       | 7.1 M  | train\n",
            "52 | teacher_encoder.transformer_layers.2  | TransformerBlock       | 7.1 M  | train\n",
            "53 | teacher_encoder.transformer_layers.3  | TransformerBlock       | 7.1 M  | train\n",
            "54 | teacher_encoder.transformer_layers.4  | TransformerBlock       | 7.1 M  | train\n",
            "55 | teacher_encoder.transformer_layers.5  | TransformerBlock       | 7.1 M  | train\n",
            "56 | teacher_encoder.transformer_layers.6  | TransformerBlock       | 7.1 M  | train\n",
            "57 | teacher_encoder.transformer_layers.7  | TransformerBlock       | 7.1 M  | train\n",
            "58 | teacher_encoder.transformer_layers.8  | TransformerBlock       | 7.1 M  | train\n",
            "59 | teacher_encoder.transformer_layers.9  | TransformerBlock       | 7.1 M  | train\n",
            "60 | teacher_encoder.transformer_layers.10 | TransformerBlock       | 7.1 M  | train\n",
            "61 | teacher_encoder.transformer_layers.11 | TransformerBlock       | 7.1 M  | train\n",
            "62 | teacher_encoder.transformer_layers.12 | TransformerBlock       | 7.1 M  | train\n",
            "63 | teacher_encoder.transformer_layers.13 | TransformerBlock       | 7.1 M  | train\n",
            "64 | teacher_encoder.transformer_layers.14 | TransformerBlock       | 7.1 M  | train\n",
            "65 | teacher_encoder.transformer_layers.15 | TransformerBlock       | 7.1 M  | train\n",
            "66 | teacher_encoder.transformer_layers.16 | TransformerBlock       | 7.1 M  | train\n",
            "67 | teacher_encoder.transformer_layers.17 | TransformerBlock       | 7.1 M  | train\n",
            "68 | teacher_encoder.downsamples           | ModuleList             | 9.4 M  | train\n",
            "69 | teacher_encoder.downsamples.0         | Conv3d                 | 4.7 M  | train\n",
            "70 | teacher_encoder.downsamples.1         | Conv3d                 | 4.7 M  | train\n",
            "71 | teacher_encoder.norm                  | LayerNorm              | 1.5 K  | train\n",
            "------------------------------------------------------------------------------------------\n",
            "365 M     Trainable params\n",
            "0         Non-trainable params\n",
            "365 M     Total params\n",
            "1,462.936 Total estimated model params size (MB)\n",
            "492       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb84bff4a66d4bc78985192206732cd4"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}