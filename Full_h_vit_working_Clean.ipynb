{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/Thesis/blob/main/Full_h_vit_working_Clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t21Fu_cr5LUu",
        "outputId": "35c86b09-853b-49f6-ad51-cbe3e4b4805a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
            "Collecting lightning\n",
            "  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Collecting monai\n",
            "  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.44)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.5.0)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.10.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.10/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.12)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.11)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Downloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monai-1.4.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, monai, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.5.0.post0 lightning-utilities-0.11.9 monai-1.4.0 pytorch-lightning-2.5.0.post0 torchmetrics-1.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install einops timm lightning wandb monai gitpython\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB_pza05MnPC"
      },
      "source": [
        "#Correct Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TWQOkLxnODWy"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        # Create handlers\n",
        "        console_handler = logging.StreamHandler()\n",
        "\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        file_handler = logging.FileHandler(os.path.join(save_dir, \"logfile.log\"))\n",
        "\n",
        "        # Create formatters and add it to handlers\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        console_handler.setFormatter(formatter)\n",
        "        file_handler.setFormatter(formatter)\n",
        "\n",
        "        # Add handlers to the logger\n",
        "        self.logger.addHandler(console_handler)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def info(self, message):\n",
        "        self.logger.info(message)\n",
        "\n",
        "    def warning(self, message):\n",
        "        self.logger.warning(message)\n",
        "\n",
        "    def error(self, message):\n",
        "        self.logger.error(message)\n",
        "\n",
        "    def debug(self, message):\n",
        "        self.logger.debug(message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EFOgJQp95FaV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import yaml\n",
        "import glob\n",
        "import pickle\n",
        "import random\n",
        "import logging\n",
        "from functools import reduce\n",
        "from typing import Tuple, Dict, Any, List, Set, Optional, Union, Callable, Type\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import lightning as L\n",
        "from lightning import LightningModule, Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "from einops import rearrange\n",
        "import timm\n",
        "import wandb\n",
        "import monai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U5KUXjQG6J5E"
      },
      "outputs": [],
      "source": [
        "# Model Components: Blocks and Transformer Layers\n",
        "\n",
        "# Drop Path (Stochastic Depth) Implementation\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
        "    if keep_prob > 0.0 and scale_by_keep:\n",
        "        random_tensor.div_(keep_prob)\n",
        "    return x * random_tensor\n",
        "\n",
        "class timm_DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
        "        super(timm_DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.scale_by_keep = scale_by_keep\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
        "\n",
        "# Truncated Normal Initialization\n",
        "def _trunc_normal_(tensor, mean, std, a, b):\n",
        "    def norm_cdf(x):\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    l = norm_cdf((a - mean) / std)\n",
        "    u = norm_cdf((b - mean) / std)\n",
        "\n",
        "    tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "    tensor.erfinv_()\n",
        "    tensor.mul_(std * math.sqrt(2.))\n",
        "    tensor.add_(mean)\n",
        "    tensor.clamp_(min=a, max=b)\n",
        "    return tensor\n",
        "\n",
        "def timm_trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    with torch.no_grad():\n",
        "        return _trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "# Convolutional Block with ReLU and Normalization\n",
        "class Conv3dReLU(nn.Sequential):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            padding=0,\n",
        "            stride=1,\n",
        "            use_batchnorm=True,\n",
        "    ):\n",
        "        conv = nn.Conv3d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            bias=False,\n",
        "        )\n",
        "        relu = nn.LeakyReLU(inplace=True)\n",
        "        if use_batchnorm:\n",
        "            nm = nn.BatchNorm3d(out_channels)\n",
        "        else:\n",
        "            nm = nn.InstanceNorm3d(out_channels)\n",
        "        super(Conv3dReLU, self).__init__(conv, nm, relu)\n",
        "\n",
        "# Normalization and Activation Getters\n",
        "def get_norm(name, **kwargs):\n",
        "    if name.lower() == 'batchnorm2d'.lower():\n",
        "        BatchNorm = getattr(nn, f'BatchNorm{ndims}d')\n",
        "        return BatchNorm(**kwargs)\n",
        "    elif name.lower() == 'instance':\n",
        "        InstanceNorm = getattr(nn, f'InstanceNorm{ndims}d')\n",
        "        return InstanceNorm(**kwargs)\n",
        "    elif name.lower() == 'none'.lower():\n",
        "        return nn.Identity()\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Normalization '{name}' not implemented.\")\n",
        "\n",
        "def get_activation(name, **kwargs):\n",
        "    if name.lower() == 'relu':\n",
        "        return nn.ReLU()\n",
        "    elif name.lower() == 'gelu':\n",
        "        return nn.GELU()\n",
        "    elif name.lower() == 'none':\n",
        "        return nn.Identity()\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Activation '{name}' not implemented.\")\n",
        "\n",
        "def prod_func(Vec):\n",
        "    return reduce(lambda x, y: x*y, Vec)\n",
        "\n",
        "def downsampler_fn(data, out_size):\n",
        "    \"\"\"\n",
        "    Trilinear downsampling\n",
        "    \"\"\"\n",
        "    return nn.functional.interpolate(data,\n",
        "                                     size=out_size,\n",
        "                                     mode='trilinear',\n",
        "                                     align_corners=False)\n",
        "\n",
        "# Spatial Transformer\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    N-D Spatial Transformer\n",
        "    Obtained from https://github.com/voxelmorph/voxelmorph\n",
        "    \"\"\"\n",
        "    def __init__(self, size, mode='bilinear'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        # create sampling grid\n",
        "        vectors = [torch.arange(0, s) for s in size]\n",
        "        grids = torch.meshgrid(vectors)\n",
        "        grid = torch.stack(grids)\n",
        "        grid = torch.unsqueeze(grid, 0)\n",
        "        grid = grid.type(torch.FloatTensor)\n",
        "\n",
        "        # Register the grid as a buffer\n",
        "        self.register_buffer('grid', grid)\n",
        "\n",
        "    def forward(self, src, flow):\n",
        "        # new locations\n",
        "        new_locs = self.grid + flow\n",
        "        shape = flow.shape[2:]\n",
        "\n",
        "        # normalize grid values to [-1, 1]\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n",
        "\n",
        "        # move channels dim to last position and reverse if necessary\n",
        "        if len(shape) == 2:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 1)\n",
        "            new_locs = new_locs[..., [1, 0]]\n",
        "        elif len(shape) == 3:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 4, 1)\n",
        "            new_locs = new_locs[..., [2, 1, 0]]\n",
        "\n",
        "        return F.grid_sample(src, new_locs, align_corners=False, mode=self.mode)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwIqeAy0C_fL"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iL9NTPkb6NC5"
      },
      "outputs": [],
      "source": [
        "# Model Components: HViT and Related Classes\n",
        "from torch import Tensor\n",
        "\n",
        "ndims = 3  # Spatial dimensions\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention module for hierarchical vision transformer.\n",
        "    Implements both local and global attention mechanisms.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        patch_size: Union[int, List[int]],\n",
        "        attention_type: str = \"local\",\n",
        "        qkv_bias: bool = True,\n",
        "        qk_scale: Optional[float] = None,\n",
        "        attn_drop: float = 0.,\n",
        "        proj_drop: float = 0.\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.patch_size = [patch_size] * ndims if isinstance(patch_size, int) else patch_size\n",
        "        self.attention_type = attention_type\n",
        "\n",
        "        assert dim % num_heads == 0, \"Dimension must be divisible by number of heads\"\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or self.head_dim ** -0.5\n",
        "\n",
        "        if self.attention_type == \"local\":\n",
        "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        elif self.attention_type == \"global\":\n",
        "            self.qkv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Attention type '{self.attention_type}' not implemented.\")\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x: Tensor, q_ms: Optional[Tensor] = None) -> Tensor:\n",
        "        B_, N, C = x.size()\n",
        "\n",
        "        if self.attention_type == \"local\":\n",
        "            qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "            q = q * self.scale\n",
        "        else:\n",
        "            B = q_ms.size()[0]\n",
        "            kv = self.qkv(x).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "            k, v = kv[0], kv[1]\n",
        "            q = self._process_global_query(q_ms, B, B_, N, C)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _process_global_query(self, q_ms: Tensor, B: int, B_: int, N: int, C: int) -> Tensor:\n",
        "        q_tmp = q_ms.reshape(B, self.num_heads, N, C // self.num_heads)\n",
        "        div_, rem_ = divmod(B_, B)\n",
        "        q_tmp = q_tmp.repeat(div_, 1, 1, 1)\n",
        "        q_tmp = q_tmp.reshape(B * div_, self.num_heads, N, C // self.num_heads)\n",
        "\n",
        "        q = torch.zeros(B_, self.num_heads, N, C // self.num_heads, device=q_ms.device)\n",
        "        q[:B*div_] = q_tmp\n",
        "        if rem_ > 0:\n",
        "            q[B*div_:] = q_tmp[:rem_]\n",
        "\n",
        "        return q * self.scale\n",
        "\n",
        "def get_patches(x: Tensor, patch_size: int) -> Tuple[Tensor, int, int, int]:\n",
        "    \"\"\"\n",
        "    Divide the input tensor into patches and reshape them for processing.\n",
        "    \"\"\"\n",
        "    B, H, W, D, C = x.size()\n",
        "    nh = H / patch_size\n",
        "    nw = W / patch_size\n",
        "    nd = D / patch_size\n",
        "\n",
        "    down_req = (nh - int(nh)) + (nw - int(nw)) + (nd - int(nd))\n",
        "    if down_req > 0:\n",
        "        new_dims = [int(nh) * patch_size, int(nw) * patch_size, int(nd) * patch_size]\n",
        "        x = downsampler_fn(x.permute(0, 4, 1, 2, 3), new_dims).permute(0, 2, 3, 4, 1)\n",
        "        B, H, W, D, C = x.size()\n",
        "\n",
        "    x = x.view(B, H // patch_size, patch_size,\n",
        "               W // patch_size, patch_size,\n",
        "               D // patch_size, patch_size,\n",
        "               C)\n",
        "\n",
        "    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, patch_size, patch_size, patch_size, C)\n",
        "\n",
        "    return windows, H, W, D\n",
        "\n",
        "def get_image(windows: Tensor, patch_size: int, Hatt: int, Watt: int, Datt: int, H: int, W: int, D: int) -> Tensor:\n",
        "    \"\"\"\n",
        "    Reconstruct the image from windows (patches).\n",
        "    \"\"\"\n",
        "    B = int(windows.size(0) / ((Hatt * Watt * Datt) // (patch_size ** 3)))\n",
        "\n",
        "    x = windows.view(B,\n",
        "                    Hatt // patch_size,\n",
        "                    Watt // patch_size,\n",
        "                    Datt // patch_size,\n",
        "                    patch_size, patch_size, patch_size, -1)\n",
        "    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, Hatt, Watt, Datt, -1)\n",
        "\n",
        "    if H != Hatt or W != Watt or D != Datt:\n",
        "        x = downsampler_fn(x.permute(0, 4, 1, 2, 3), [H, W, D]).permute(0, 2, 3, 4, 1)\n",
        "    return x\n",
        "\n",
        "class ViTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer Block.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 embed_dim: int,\n",
        "                 input_dims: List[int],\n",
        "                 num_heads: int,\n",
        "                 mlp_type: str,\n",
        "                 patch_size: int,\n",
        "                 mlp_ratio: float,\n",
        "                 qkv_bias: bool,\n",
        "                 qk_scale: Optional[float],\n",
        "                 drop: float,\n",
        "                 attn_drop: float,\n",
        "                 drop_path: float,\n",
        "                 act_layer: str,\n",
        "                 attention_type: str,\n",
        "                 norm_layer: Callable[..., nn.Module],\n",
        "                 layer_scale: Optional[float]):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_windows = prod_func([d // patch_size for d in input_dims])\n",
        "\n",
        "        self.norm1 = norm_layer(embed_dim)\n",
        "        self.attn = Attention(\n",
        "            dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            patch_size=patch_size,\n",
        "            attention_type=attention_type,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop,\n",
        "        )\n",
        "\n",
        "        self.drop_path = timm_DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(embed_dim)\n",
        "\n",
        "        self.mlp = Conv3dReLU(\n",
        "            in_channels=embed_dim,\n",
        "            out_channels=int(embed_dim * mlp_ratio),\n",
        "            kernel_size=3,  # Assuming kernel_size=3 for MLP\n",
        "            padding=1,\n",
        "            stride=1,\n",
        "            use_batchnorm=True,\n",
        "        )\n",
        "\n",
        "        # Add projection layer to ensure output channels match embed_dim\n",
        "        self.proj = nn.Conv3d(\n",
        "            in_channels=int(embed_dim * mlp_ratio),\n",
        "            out_channels=embed_dim,\n",
        "            kernel_size=1\n",
        "        )\n",
        "        self.layer_scale = layer_scale is not None and isinstance(layer_scale, (int, float))\n",
        "        if self.layer_scale:\n",
        "            self.gamma1 = nn.Parameter(layer_scale * torch.ones(embed_dim), requires_grad=True)\n",
        "            self.gamma2 = nn.Parameter(layer_scale * torch.ones(embed_dim), requires_grad=True)\n",
        "        else:\n",
        "            self.gamma1 = 1.0\n",
        "            self.gamma2 = 1.0\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor, q_ms: Optional[Tensor]) -> Tensor:\n",
        "        B, H, W, D, C = x.size()\n",
        "        shortcut = x\n",
        "\n",
        "        # Normalize and compute attention\n",
        "        x = self.norm1(x)\n",
        "        x_windows, Hatt, Watt, Datt = get_patches(x, self.patch_size)\n",
        "        x_windows = x_windows.view(-1, self.patch_size ** 3, C)\n",
        "\n",
        "        # Compute attention and reconstruct image\n",
        "        attn_windows = self.attn(x_windows, q_ms)\n",
        "        x = get_image(attn_windows, self.patch_size, Hatt, Watt, Datt, H, W, D)\n",
        "\n",
        "        # Apply shortcut and drop path\n",
        "        x = shortcut + self.drop_path(self.gamma1 * x)\n",
        "\n",
        "        # Apply MLP\n",
        "        x_mlp_input = self.norm2(x).permute(0, 4, 1, 2, 3)\n",
        "        print(f\"MLP input shape after permute: {x_mlp_input.shape}\")  # Debug print\n",
        "        x_mlp_output = self.mlp(x_mlp_input)\n",
        "        x_mlp_output = self.proj(x_mlp_output).permute(0, 2, 3, 4, 1)\n",
        "\n",
        "        # Add MLP output with drop path and gamma scaling\n",
        "        x = x + self.drop_path(self.gamma2 * x_mlp_output)\n",
        "        return x\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch Embedding layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chans: int = 3, out_chans: int = 32,\n",
        "                 drop_rate: float = 0,\n",
        "                 kernel_size: int = 3,\n",
        "                 stride: int = 1, padding: int = 1,\n",
        "                 dilation: int = 1, groups: int = 1, bias: bool = False) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        Convnd = getattr(nn, f\"Conv{ndims}d\")\n",
        "        self.proj = Convnd(in_channels=in_chans, out_channels=out_chans,\n",
        "                           kernel_size=kernel_size,\n",
        "                           stride=stride, padding=padding,\n",
        "                           dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "        self.drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.drop(self.proj(x))\n",
        "        return x\n",
        "\n",
        "class ViTLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer Layer.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_type: str,\n",
        "        dim: int,\n",
        "        dim_out: int,\n",
        "        depth: int,\n",
        "        input_dims: List[int],\n",
        "        num_heads: int,\n",
        "        patch_size: int,\n",
        "        mlp_type: str,\n",
        "        mlp_ratio: float,\n",
        "        qkv_bias: bool,\n",
        "        qk_scale: Optional[float],\n",
        "        drop: float,\n",
        "        attn_drop: float,\n",
        "        drop_path: Union[float, List[float]],\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        norm_type: str,\n",
        "        layer_scale: Optional[float],\n",
        "        act_layer: str\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = dim\n",
        "        self.input_dims = input_dims\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ViTBlock(\n",
        "                embed_dim=dim,\n",
        "                input_dims=input_dims,\n",
        "                num_heads=num_heads,\n",
        "                mlp_type=mlp_type,\n",
        "                patch_size=patch_size,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                attention_type=attention_type,\n",
        "                drop=drop,\n",
        "                attn_drop=attn_drop,\n",
        "                drop_path=drop_path[k] if isinstance(drop_path, list) else drop_path,\n",
        "                act_layer=act_layer,\n",
        "                norm_layer=norm_layer,\n",
        "                layer_scale=layer_scale\n",
        "            )\n",
        "            for k in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, inp: Tensor, q_ms: Optional[Tensor], CONCAT_ok: bool) -> Tensor:\n",
        "        x = inp.clone()\n",
        "        x = rearrange(x, 'b c h w d -> b h w d c')\n",
        "\n",
        "        if q_ms is not None:\n",
        "            q_ms = rearrange(q_ms, 'b c h w d -> b h w d c')\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            if q_ms is None:\n",
        "                x = blk(x, None)\n",
        "            else:\n",
        "                q_ms_patches, _, _, _ = get_patches(q_ms, self.patch_size)\n",
        "                q_ms_patches = q_ms_patches.view(-1, self.patch_size ** ndims, x.size()[-1])\n",
        "                x = blk(x, q_ms_patches)\n",
        "\n",
        "        x = rearrange(x, 'b h w d c -> b c h w d')\n",
        "\n",
        "        if CONCAT_ok:\n",
        "            x = torch.cat((inp, x), dim=-1)\n",
        "        else:\n",
        "            x = inp + x\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) module for hierarchical feature processing.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 PYR_SCALES=None,\n",
        "                 feats_num=None,\n",
        "                 hid_dim=None,\n",
        "                 depths=None,\n",
        "                 patch_size=None,\n",
        "                 mlp_ratio=None,\n",
        "                 num_heads=None,\n",
        "                 mlp_type=None,\n",
        "                 norm_type=None,\n",
        "                 act_layer=None,\n",
        "                 drop_path_rate: float = 0.2,\n",
        "                 qkv_bias: bool = True,\n",
        "                 qk_scale: bool = None,\n",
        "                 drop_rate: float = 0.,\n",
        "                 attn_drop_rate: float = 0.,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 layer_scale=None,\n",
        "                 img_size=None,\n",
        "                 NUM_CROSS_ATT=-1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Determine the number of levels for processing\n",
        "        num_levels = len(feats_num)\n",
        "        num_levels = min(num_levels, NUM_CROSS_ATT) if NUM_CROSS_ATT > 0 else num_levels\n",
        "        # WO_SELF_ATT is defined globally; set to False as per code\n",
        "        global WO_SELF_ATT\n",
        "        if WO_SELF_ATT:\n",
        "            num_levels -= 1\n",
        "\n",
        "        # Ensure patch_size is a list\n",
        "        patch_size = patch_size if isinstance(patch_size, list) else [patch_size for _ in range(num_levels)]\n",
        "        hwd = img_size[-1]\n",
        "\n",
        "        # Create patch embedding layers\n",
        "        self.patch_embed = nn.ModuleList([\n",
        "            PatchEmbed(\n",
        "                in_chans=feats_num[i],\n",
        "                out_chans=hid_dim,\n",
        "                drop_rate=drop_rate\n",
        "            ) for i in range(num_levels)\n",
        "        ])\n",
        "\n",
        "        # Generate drop path rate for each layer\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        # Create ViT layers\n",
        "        self.levels = nn.ModuleList()\n",
        "        for i in range(num_levels):\n",
        "            level = ViTLayer(\n",
        "                dim=hid_dim,\n",
        "                dim_out=hid_dim,\n",
        "                depth=depths[i],\n",
        "                num_heads=num_heads[i],\n",
        "                patch_size=patch_size[i],\n",
        "                mlp_type=mlp_type,\n",
        "                attention_type=\"local\" if i == 0 else \"global\",\n",
        "                drop_path=dpr[sum(depths[:i]):sum(depths[:i+1])],\n",
        "                input_dims=img_size[i],\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                norm_layer=norm_layer,\n",
        "                layer_scale=layer_scale,\n",
        "                norm_type=norm_type,\n",
        "                act_layer=act_layer\n",
        "            )\n",
        "            self.levels.append(level)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        \"\"\"Initialize the weights of the module.\"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            timm_trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        \"\"\"Return keywords for no weight decay.\"\"\"\n",
        "        return {'rpb'}\n",
        "\n",
        "    def forward(self, KQs, CONCAT_ok: bool = False):\n",
        "        \"\"\"\n",
        "        Forward pass of the ViT module.\n",
        "        \"\"\"\n",
        "        for i, (patch_embed_, level) in enumerate(zip(self.patch_embed, self.levels)):\n",
        "            if i == 0:\n",
        "                # First level: process input without cross-attention\n",
        "                Q = patch_embed_(KQs[i])\n",
        "                x = level(Q, None, CONCAT_ok=CONCAT_ok)\n",
        "                Q = patch_embed_(x)\n",
        "            else:\n",
        "                # Subsequent levels: process with cross-attention\n",
        "                K = patch_embed_(KQs[i])\n",
        "                x = level(Q, K, CONCAT_ok=CONCAT_ok)\n",
        "                Q = x.clone()\n",
        "\n",
        "        return x\n",
        "\n",
        "class EncoderCnnBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional block for the encoder part of the network.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        stride,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "        affine=True,\n",
        "        eps=1e-05\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # First convolutional block\n",
        "        conv_block_1 = [\n",
        "            nn.Conv3d(\n",
        "                in_channels=in_channels, out_channels=out_channels,\n",
        "                kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                bias=bias\n",
        "            ),\n",
        "            nn.InstanceNorm3d(num_features=out_channels, affine=affine, eps=eps),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Second convolutional block\n",
        "        conv_block_2 = [\n",
        "            nn.Conv3d(\n",
        "                in_channels=out_channels, out_channels=out_channels,\n",
        "                kernel_size=kernel_size, stride=1, padding=padding,\n",
        "                bias=bias\n",
        "            ),\n",
        "            nn.InstanceNorm3d(num_features=out_channels, affine=affine, eps=eps),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Combine both blocks\n",
        "        self._block = nn.Sequential(\n",
        "            *conv_block_1,\n",
        "            *conv_block_2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of the EncoderCnnBlock.\"\"\"\n",
        "        return self._block(x)\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) module for hierarchical feature processing.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 PYR_SCALES=None,\n",
        "                 feats_num=None,\n",
        "                 hid_dim=None,\n",
        "                 depths=None,\n",
        "                 patch_size=None,\n",
        "                 mlp_ratio=None,\n",
        "                 num_heads=None,\n",
        "                 mlp_type=None,\n",
        "                 norm_type=None,\n",
        "                 act_layer=None,\n",
        "                 drop_path_rate: float = 0.2,\n",
        "                 qkv_bias: bool = True,\n",
        "                 qk_scale: bool = None,\n",
        "                 drop_rate: float = 0.,\n",
        "                 attn_drop_rate: float = 0.,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 layer_scale=None,\n",
        "                 img_size=None,\n",
        "                 WO_SELF_ATT=False,  # Added WO_SELF_ATT parameter\n",
        "\n",
        "                 NUM_CROSS_ATT=-1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Determine the number of levels for processing\n",
        "        num_levels = len(feats_num)\n",
        "        num_levels = min(num_levels, NUM_CROSS_ATT) if NUM_CROSS_ATT > 0 else num_levels\n",
        "        if WO_SELF_ATT:\n",
        "            num_levels -= 1\n",
        "\n",
        "        # Ensure patch_size is a list\n",
        "        patch_size = patch_size if isinstance(patch_size, list) else [patch_size for _ in range(num_levels)]\n",
        "        hwd = img_size[-1]\n",
        "\n",
        "        # Create patch embedding layers\n",
        "        self.patch_embed = nn.ModuleList([\n",
        "            PatchEmbed(\n",
        "                in_chans=feats_num[i],\n",
        "                out_chans=hid_dim,\n",
        "                drop_rate=drop_rate\n",
        "            ) for i in range(num_levels)\n",
        "        ])\n",
        "\n",
        "        # Generate drop path rate for each layer\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        # Create ViT layers\n",
        "        self.levels = nn.ModuleList()\n",
        "        for i in range(num_levels):\n",
        "            level = ViTLayer(\n",
        "                dim=hid_dim,\n",
        "                dim_out=hid_dim,\n",
        "                depth=depths[i],\n",
        "                num_heads=num_heads[i],\n",
        "                patch_size=patch_size[i],\n",
        "                mlp_type=mlp_type,\n",
        "                attention_type=\"local\" if i == 0 else \"global\",\n",
        "                drop_path=dpr[sum(depths[:i]):sum(depths[:i+1])],\n",
        "                input_dims=img_size[i],\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                norm_layer=norm_layer,\n",
        "                layer_scale=layer_scale,\n",
        "                norm_type=norm_type,\n",
        "                act_layer=act_layer\n",
        "            )\n",
        "            self.levels.append(level)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        \"\"\"Initialize the weights of the module.\"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            timm_trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        \"\"\"Return keywords for no weight decay.\"\"\"\n",
        "        return {'rpb'}\n",
        "\n",
        "    def forward(self, KQs, CONCAT_ok: bool = False):\n",
        "        \"\"\"\n",
        "        Forward pass of the ViT module.\n",
        "        \"\"\"\n",
        "        for i, (patch_embed_, level) in enumerate(zip(self.patch_embed, self.levels)):\n",
        "            if i == 0:\n",
        "                # First level: process input without cross-attention\n",
        "                Q = patch_embed_(KQs[i])\n",
        "                x = level(Q, None, CONCAT_ok=CONCAT_ok)\n",
        "                Q = patch_embed_(x)\n",
        "            else:\n",
        "                # Subsequent levels: process with cross-attention\n",
        "                K = patch_embed_(KQs[i])\n",
        "                x = level(Q, K, CONCAT_ok=CONCAT_ok)\n",
        "                Q = x.clone()\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder module for the hierarchical vision transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "        self._num_stages: int = config['num_stages']\n",
        "        self.use_seg: bool = config['use_seg_loss']\n",
        "\n",
        "        # Determine channels of encoder feature maps\n",
        "        encoder_out_channels: torch.Tensor = torch.tensor([config['start_channels'] * 2**stage for stage in range(self._num_stages)])\n",
        "        self._NUM_CROSS_ATT=config.get('NUM_CROSS_ATT', -1)\n",
        "        # Estimate required stages\n",
        "        required_stages: Set[int] = set(int(fmap[-1]) for fmap in config['out_fmaps'])\n",
        "        self._required_stages: Set[int] = required_stages\n",
        "\n",
        "        earliest_required_stage: int = min(required_stages)\n",
        "\n",
        "        # Lateral connections\n",
        "        lateral_in_channels: torch.Tensor = encoder_out_channels[earliest_required_stage:]\n",
        "        lateral_out_channels: torch.Tensor = lateral_in_channels.clip(max=config['fpn_channels'])\n",
        "\n",
        "        self._lateral: nn.ModuleList = nn.ModuleList([\n",
        "            nn.Conv3d(in_channels=in_ch, out_channels=out_ch, kernel_size=1)\n",
        "            for in_ch, out_ch in zip(lateral_in_channels, lateral_out_channels)\n",
        "        ])\n",
        "        self._lateral_levels: int = len(self._lateral)\n",
        "\n",
        "        # Output layers\n",
        "        out_in_channels: List[int] = [lateral_out_channels[-self._num_stages + required_stage].item() for required_stage in required_stages]\n",
        "        out_out_channels: List[int] = [int(config['fpn_channels'])] * len(out_in_channels)\n",
        "        out_out_channels[0] = int(config['fpn_channels'])\n",
        "\n",
        "        self._out: nn.ModuleList = nn.ModuleList([\n",
        "            nn.Conv3d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1)\n",
        "            for in_ch, out_ch in zip(out_in_channels, out_out_channels)\n",
        "        ])\n",
        "\n",
        "        # Upsampling layers\n",
        "        self._up: nn.ModuleList = nn.ModuleList([\n",
        "            nn.ConvTranspose3d(\n",
        "                in_channels=list(reversed(lateral_out_channels))[level],\n",
        "                out_channels=list(reversed(lateral_out_channels))[level+1],\n",
        "                kernel_size=list(reversed(config['strides']))[level],\n",
        "                stride=list(reversed(config['strides']))[level]\n",
        "            )\n",
        "            for level in range(len(lateral_out_channels)-1)\n",
        "        ])\n",
        "\n",
        "        # Multi-scale attention\n",
        "        self.hierarchical_dec: nn.ModuleList = self._create_hierarchical_layers(config, out_out_channels)\n",
        "\n",
        "        if self.use_seg:\n",
        "            self._seg_head: nn.ModuleList = nn.ModuleList([\n",
        "                nn.Conv3d(out_ch, config['num_organs'] + 1, kernel_size=1, stride=1)\n",
        "                for out_ch in out_out_channels\n",
        "            ])\n",
        "    def _create_hierarchical_layers(self, config: Dict[str, Any], out_out_channels: List[int]) -> nn.ModuleList:\n",
        "        \"\"\"Create hierarchical layers for multi-scale attention.\"\"\"\n",
        "        out: nn.ModuleList = nn.ModuleList()\n",
        "        img_size: List[List[int]] = []\n",
        "        feats_num: List[int] = []\n",
        "\n",
        "        num_levels = len(out_out_channels)  # Ensure `num_levels` matches the length of `out_out_channels`\n",
        "\n",
        "        for k, out_ch in enumerate(out_out_channels):\n",
        "            img_size.append([int(item / (2 ** (self._num_stages - k - 1))) for item in config['data_size']])\n",
        "            feats_num.append(out_ch)\n",
        "            n: int = len(feats_num)\n",
        "\n",
        "            if k == 0:\n",
        "                out.append(nn.Identity())\n",
        "            else:\n",
        "                # Ensure depths and num_heads have enough entries\n",
        "                depths = config.get('depths', [1] * num_levels)\n",
        "                num_heads = config.get('num_heads', [32] * num_levels)\n",
        "\n",
        "                # Use k or level-based indexing\n",
        "                out.append(\n",
        "                    ViT(\n",
        "                        NUM_CROSS_ATT=config.get('NUM_CROSS_ATT', self._NUM_CROSS_ATT),\n",
        "                        PYR_SCALES=[1.],\n",
        "                        feats_num=feats_num,\n",
        "                        hid_dim=int(config.get('fpn_channels', 64)),\n",
        "                        depths=depths,  # Use the list directly\n",
        "                        patch_size=config.get('patch_size', [2] * n),  # Fixed line\n",
        "                        mlp_ratio=int(config.get('mlp_ratio', 2)),\n",
        "                        num_heads=num_heads,  # Use the list directly\n",
        "                        mlp_type='basic',\n",
        "                        norm_type='BatchNorm2d',\n",
        "                        act_layer='gelu',\n",
        "                        drop_path_rate=config.get('drop_path_rate', 0.2),\n",
        "                        qkv_bias=config.get('qkv_bias', True),\n",
        "                        qk_scale=None,\n",
        "                        drop_rate=config.get('drop_rate', 0.),\n",
        "                        attn_drop_rate=config.get('attn_drop_rate', 0.),\n",
        "                        norm_layer=nn.LayerNorm,\n",
        "                        layer_scale=1e-5,\n",
        "                        img_size=img_size\n",
        "                    )\n",
        "                )\n",
        "        return out\n",
        "\n",
        "\n",
        "    def forward(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
        "        \"\"\"Forward pass of the Decoder.\"\"\"\n",
        "        lateral_out: List[Tensor] = [lateral(fmap) for lateral, fmap in zip(self._lateral, list(x.values())[-self._lateral_levels:])]\n",
        "\n",
        "        up_out: List[Tensor] = []\n",
        "        for idx, x in enumerate(reversed(lateral_out)):\n",
        "            if idx != 0:\n",
        "                x = x + up\n",
        "\n",
        "            if idx < self._lateral_levels - 1:\n",
        "                up = self._up[idx](x)\n",
        "\n",
        "            up_out.append(x)\n",
        "\n",
        "        cnn_outputs: Dict[int, Tensor] = {stage: self._out[idx](fmap) for idx, (fmap, stage) in enumerate(zip(reversed(up_out), self._required_stages))}\n",
        "        return self._forward_hierarchical(cnn_outputs)\n",
        "\n",
        "    def _forward_hierarchical(self, cnn_outputs: Dict[int, Tensor]) -> Dict[str, Tensor]:\n",
        "        \"\"\"Forward pass through the hierarchical decoder.\"\"\"\n",
        "        xs: List[Tensor] = [cnn_outputs[key].clone() for key in range(max(cnn_outputs.keys()), min(cnn_outputs.keys())-1, -1)]\n",
        "\n",
        "        out_dict: Dict[str, Tensor] = {}\n",
        "        QK: List[Tensor] = []\n",
        "        for i, key in enumerate(range(max(cnn_outputs.keys()), min(cnn_outputs.keys())-1, -1)):\n",
        "            QK = [xs[i]] + QK\n",
        "            if i == 0:\n",
        "                Pi = QK[0]\n",
        "            else:\n",
        "                Pi = self.hierarchical_dec[i](QK)\n",
        "            QK[0] = Pi\n",
        "            out_dict[f'P{key}'] = Pi\n",
        "\n",
        "            if self.use_seg:\n",
        "                Pi_seg = self._seg_head[i](Pi)\n",
        "                out_dict[f'S{key}'] = Pi_seg\n",
        "\n",
        "        return out_dict\n",
        "\n",
        "\n",
        "\n",
        "class HierarchicalViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Hierarchical Vision Transformer (HViT) for image processing tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "\n",
        "        # Configuration parameters\n",
        "        self.backbone = config['backbone_net']\n",
        "        in_channels = 2 * config.get('in_channels', 1)  # source + target\n",
        "        kernel_size = config.get('kernel_size', 3)\n",
        "        emb_dim = config.get('start_channels', 32)\n",
        "        data_size = config.get('data_size', [160, 192, 224])\n",
        "        self.out_fmaps = config.get('out_fmaps', ['P4', 'P3', 'P2', 'P1'])\n",
        "\n",
        "        # Calculate number of stages\n",
        "        num_stages = min(int(math.log2(min(data_size))) - 1,\n",
        "                         max(int(fmap[-1]) for fmap in self.out_fmaps) + 1)\n",
        "\n",
        "        strides = [1] + [2] * (num_stages - 1)\n",
        "        kernel_sizes = [kernel_size] * num_stages\n",
        "\n",
        "        config['num_stages'] = num_stages\n",
        "        config['strides'] = strides\n",
        "\n",
        "        # Build encoder\n",
        "        self._encoder = nn.ModuleList()\n",
        "        if self.backbone.lower() in ['fpn', 'fpn']:\n",
        "            for k in range(num_stages):\n",
        "                blk = EncoderCnnBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=emb_dim,\n",
        "                    kernel_size=kernel_sizes[k],\n",
        "                    stride=strides[k]\n",
        "                )\n",
        "                self._encoder.append(blk)\n",
        "\n",
        "                in_channels = emb_dim\n",
        "                emb_dim *= 2\n",
        "\n",
        "        # Build decoder\n",
        "        if self.backbone.lower() in ['fpn', 'fpn']:\n",
        "            self._decoder = Decoder(config)\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize model weights.\"\"\"\n",
        "        for m in self.modules():\n",
        "            self._init_weights(m)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        \"\"\"Initialize the weights of the module.\"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            timm_trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x: Tensor, verbose: bool = False) -> Dict[str, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the HierarchicalViT model.\n",
        "        \"\"\"\n",
        "        down = {}\n",
        "        if self.backbone.lower() in ['fpn', 'fpn']:\n",
        "            for stage_id, module in enumerate(self._encoder):\n",
        "                x = module(x)\n",
        "                down[f'C{stage_id}'] = x\n",
        "            up = self._decoder(down)\n",
        "\n",
        "        if verbose:\n",
        "            for key, item in down.items():\n",
        "                print(f'down {key}', item.shape)\n",
        "            for key, item in up.items():\n",
        "                print(f'up {key}', item.shape)\n",
        "        return up\n",
        "\n",
        "class RegistrationHead(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Registration head for generating displacement fields.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
        "        super().__init__()\n",
        "        conv3d = nn.Conv3d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2\n",
        "        )\n",
        "        # Initialize weights with small random values\n",
        "        conv3d.weight = nn.Parameter(torch.zeros_like(conv3d.weight).normal_(0, 1e-5))\n",
        "        conv3d.bias = nn.Parameter(torch.zeros(conv3d.bias.shape))\n",
        "        self.add_module('conv3d', conv3d)\n",
        "\n",
        "class HierarchicalViT_Light(nn.Module):\n",
        "    \"\"\"\n",
        "    Light Hierarchical Vision Transformer (HViT) model for image registration.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: dict):\n",
        "        super(HierarchicalViT_Light, self).__init__()\n",
        "        self.upsample_df = config.get('upsample_df', False)\n",
        "        self.upsample_scale_factor = config.get('upsample_scale_factor', 2)\n",
        "        self.scale_level_df = config.get('scale_level_df', 'P1')\n",
        "        self.ndims = config.get('ndims', 3)\n",
        "        self._NUM_CROSS_ATT = config.get('NUM_CROSS_ATT', -1)\n",
        "        self.deformable = HierarchicalViT(config)\n",
        "        self.avg_pool = nn.AvgPool3d(3, stride=2, padding=1)\n",
        "        self.spatial_trans = SpatialTransformer(config['data_size'])\n",
        "        self.reg_head = RegistrationHead(\n",
        "            in_channels=config.get('fpn_channels', 64),\n",
        "            out_channels=ndims,\n",
        "            kernel_size=ndims,\n",
        "        )\n",
        "\n",
        "    def forward(self, source: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the HViT model.\n",
        "        \"\"\"\n",
        "        x = torch.cat((source, target), dim=1)\n",
        "        x_dec = self.deformable(x)\n",
        "\n",
        "        # Extract features at the specified scale level\n",
        "        x_dec = x_dec[self.scale_level_df]\n",
        "        flow = self.reg_head(x_dec)\n",
        "\n",
        "        if self.upsample_df:\n",
        "            flow = nn.Upsample(scale_factor=self.upsample_scale_factor,\n",
        "                               mode='trilinear',\n",
        "                               align_corners=False)(flow)\n",
        "\n",
        "        moved = self.spatial_trans(source, flow)\n",
        "        return moved, flow\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N8ByTBe_C4Vm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download DS"
      ],
      "metadata": {
        "id": "DKLXlFBGSoJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cloud.imi.uni-luebeck.de/s/xcZrLSQYtK68em8/download/OASIS.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZInA0cASqCf",
        "outputId": "ea49a357-6487-4dcf-dfb7-bd48a54774c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-10 12:58:35--  https://cloud.imi.uni-luebeck.de/s/xcZrLSQYtK68em8/download/OASIS.zip\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1444971437 (1.3G) [application/zip]\n",
            "Saving to: ‘OASIS.zip.1’\n",
            "\n",
            "OASIS.zip.1         100%[===================>]   1.35G  14.8MB/s    in 99s     \n",
            "\n",
            "2025-01-10 13:00:15 (14.0 MB/s) - ‘OASIS.zip.1’ saved [1444971437/1444971437]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: extract downloadd zip\n",
        "\n",
        "!unzip -q OASIS.zip\n"
      ],
      "metadata": {
        "id": "FxuexOk3S4H2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/OASIS/OASIS_dataset.json /content/"
      ],
      "metadata": {
        "id": "EMAbSVZHL2tK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "json_path = \"OASIS/OASIS_dataset.json\"\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Update paths dynamically\n",
        "base_path = \"OASIS\"\n",
        "for entry in data[\"training\"]:\n",
        "    entry[\"image\"] = os.path.join(base_path, entry[\"image\"].lstrip(\"./\"))\n",
        "    entry[\"label\"] = os.path.join(base_path, entry[\"label\"].lstrip(\"./\"))\n",
        "    entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "for entry in data[\"test\"]:\n",
        "    entry[\"image\"] = os.path.join(base_path, entry[\"image\"].lstrip(\"./\"))\n",
        "    entry[\"label\"] = os.path.join(base_path, entry[\"label\"].lstrip(\"./\"))\n",
        "    entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "for entry in data[\"registration_test\"]:\n",
        "    entry[\"fixed\"] = os.path.join(base_path, entry[\"fixed\"].lstrip(\"./\"))\n",
        "    entry[\"moving\"] = os.path.join(base_path, entry[\"moving\"].lstrip(\"./\"))\n",
        "    # entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "for entry in data[\"registration_val\"]:\n",
        "    entry[\"fixed\"] = os.path.join(base_path, entry[\"fixed\"].lstrip(\"./\"))\n",
        "    entry[\"moving\"] = os.path.join(base_path, entry[\"moving\"].lstrip(\"./\"))\n",
        "    # entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "\n",
        "# Save updated JSON\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(data, f, indent=4)"
      ],
      "metadata": {
        "id": "M7F7Nwa_d_gI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## new oasis dataset test"
      ],
      "metadata": {
        "id": "gKW-tdiFr7r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from monai.transforms import Resize\n",
        "\n",
        "class OASIS_Dataset(Dataset):\n",
        "    def __init__(self, json_path, mode=\"training\", input_dim=(128, 128, 128), is_pair=False):\n",
        "        self.input_dim = input_dim\n",
        "        self.is_pair = is_pair\n",
        "\n",
        "        # Load JSON data\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if mode == \"training\":\n",
        "            self.samples = data[\"training\"]\n",
        "        elif mode == \"test\":\n",
        "            self.samples = data[\"test\"]\n",
        "        elif mode == \"registration_val\":\n",
        "            self.samples = data[\"registration_val\"]\n",
        "        elif mode == \"registration_test\":\n",
        "            self.samples = data[\"registration_test\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "        self.transforms_image = Resize(spatial_size=input_dim)\n",
        "        self.transforms_mask = Resize(spatial_size=input_dim, mode=\"nearest\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_pair:\n",
        "            # Load fixed and moving images for registration\n",
        "            sample = self.samples[index]\n",
        "            fixed_path = sample[\"fixed\"]\n",
        "            moving_path = sample[\"moving\"]\n",
        "\n",
        "            fixed = nib.load(fixed_path).get_fdata()\n",
        "            moving = nib.load(moving_path).get_fdata()\n",
        "\n",
        "            fixed = self.transforms_image(torch.from_numpy(fixed).unsqueeze(0).float())\n",
        "            moving = self.transforms_image(torch.from_numpy(moving).unsqueeze(0).float())\n",
        "\n",
        "            # Load segmentation masks if available\n",
        "            # Assuming mask paths are provided; adjust accordingly\n",
        "            fixed_mask_path = sample.get(\"fixed_mask\", None)\n",
        "            moving_mask_path = sample.get(\"moving_mask\", None)\n",
        "\n",
        "            if fixed_mask_path and moving_mask_path:\n",
        "                fixed_mask = nib.load(fixed_mask_path).get_fdata()\n",
        "                moving_mask = nib.load(moving_mask_path).get_fdata()\n",
        "\n",
        "                fixed_mask = self.transforms_mask(torch.from_numpy(fixed_mask).unsqueeze(0).long())\n",
        "                moving_mask = self.transforms_mask(torch.from_numpy(moving_mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                # If masks are not available, return dummy masks\n",
        "                fixed_mask = torch.zeros_like(fixed)\n",
        "                moving_mask = torch.zeros_like(moving)\n",
        "\n",
        "            return fixed, moving, fixed_mask, moving_mask\n",
        "        else:\n",
        "            # Load unpaired data (image, label, mask)\n",
        "            sample = self.samples[index]\n",
        "            image_path = sample[\"image\"]\n",
        "            label_path = sample[\"label\"]\n",
        "            mask_path = sample.get(\"mask\", None)\n",
        "\n",
        "            image = nib.load(image_path).get_fdata()\n",
        "            label = nib.load(label_path).get_fdata()\n",
        "\n",
        "            image = self.transforms_image(torch.from_numpy(image).unsqueeze(0).float())\n",
        "            label = self.transforms_mask(torch.from_numpy(label).unsqueeze(0).long())\n",
        "\n",
        "            if mask_path:\n",
        "                mask = nib.load(mask_path).get_fdata()\n",
        "                mask = self.transforms_mask(torch.from_numpy(mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                mask = torch.zeros_like(label)\n",
        "\n",
        "            return image, label, mask\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# For training (paired data)\n",
        "train_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"training\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=True  # Set to True for paired data\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=4  # Adjust based on your system\n",
        ")\n",
        "\n",
        "# For validation\n",
        "val_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"registration_val\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=True  # Set to True for paired data\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n"
      ],
      "metadata": {
        "id": "OXUhE5U1ldo5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: summerize json attributes\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "json_path = \"OASIS/OASIS_dataset.json\"\n",
        "with open(json_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def summarize_json(data):\n",
        "    summary = {}\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, list):\n",
        "            summary[key] = {\n",
        "                \"count\": len(value),\n",
        "                \"example\": value[0] if value else None  # Example item\n",
        "            }\n",
        "        elif isinstance(value, dict):\n",
        "            summary[key] = summarize_json(value) # Recursive call for nested dicts\n",
        "        else:\n",
        "            summary[key] = value\n",
        "    return summary\n",
        "\n",
        "summary = summarize_json(data)\n",
        "print(json.dumps(summary, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E07T0bLrl9Tv",
        "outputId": "03842a0b-61ac-4309-ad12-4d224241efdf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"OASIS\",\n",
            "  \"release\": \"1.1\",\n",
            "  \"description\": \"OASIS task of Learn2Reg Dataset. Please see https://learn2reg.grand-challenge.org/ for more information. These data were prepared by Andrew Hoopes and Adrian V. Dalca for the following HyperMorph paper. If you use this collection please cite the following and refer to the OASIS Data Use Agreement. \",\n",
            "  \"licence\": \"Open Access Series of Imaging Studies (OASIS): Cross-Sectional MRI Data in Young, Middle Aged, Nondemented, and Demented Older Adults. Marcus DS, Wang TH, Parker J, Csernansky JG, Morris JC, Buckner RL. Journal of Cognitive Neuroscience, 19, 1498-1507.\",\n",
            "  \"reference\": \"\",\n",
            "  \"pairings\": \"unpaired\",\n",
            "  \"provided_data\": {\n",
            "    \"0\": {\n",
            "      \"count\": 3,\n",
            "      \"example\": \"image\"\n",
            "    }\n",
            "  },\n",
            "  \"registration_direction\": {\n",
            "    \"fixed\": 0,\n",
            "    \"moving\": 0\n",
            "  },\n",
            "  \"modality\": {\n",
            "    \"0\": \"MR\"\n",
            "  },\n",
            "  \"img_shift\": {\n",
            "    \"fixed\": \"Patient A\",\n",
            "    \"moving\": \"Patient B\"\n",
            "  },\n",
            "  \"labels\": {\n",
            "    \"0\": {}\n",
            "  },\n",
            "  \"tensorImageSize\": {\n",
            "    \"0\": \"3D\"\n",
            "  },\n",
            "  \"tensorImageShape\": {\n",
            "    \"0\": {\n",
            "      \"count\": 3,\n",
            "      \"example\": 160\n",
            "    }\n",
            "  },\n",
            "  \"numTraining\": 414,\n",
            "  \"numTest\": 39,\n",
            "  \"training\": {\n",
            "    \"count\": 414,\n",
            "    \"example\": {\n",
            "      \"image\": \"OASIS/imagesTr/OASIS_0001_0000.nii.gz\",\n",
            "      \"label\": \"OASIS/labelsTr/OASIS_0001_0000.nii.gz\",\n",
            "      \"mask\": \"OASIS/masksTr/OASIS_0001_0000.nii.gz\"\n",
            "    }\n",
            "  },\n",
            "  \"test\": {\n",
            "    \"count\": 39,\n",
            "    \"example\": {\n",
            "      \"image\": \"OASIS/imagesTs/OASIS_0415_0000.csv\",\n",
            "      \"label\": \"OASIS/labelsTs/OASIS_0415_0000.csv\",\n",
            "      \"mask\": \"OASIS/masksTs/OASIS_0415_0000.csv\"\n",
            "    }\n",
            "  },\n",
            "  \"numPairedTraining\": 0,\n",
            "  \"training_paired_images\": {\n",
            "    \"count\": 0,\n",
            "    \"example\": null\n",
            "  },\n",
            "  \"numPairedTest\": 0,\n",
            "  \"test_paired_images\": {\n",
            "    \"count\": 0,\n",
            "    \"example\": null\n",
            "  },\n",
            "  \"numRegistration_val\": 19,\n",
            "  \"registration_val\": {\n",
            "    \"count\": 19,\n",
            "    \"example\": {\n",
            "      \"fixed\": \"OASIS/imagesTr/OASIS_0395_0000.nii.gz\",\n",
            "      \"moving\": \"OASIS/imagesTr/OASIS_0396_0000.nii.gz\"\n",
            "    }\n",
            "  },\n",
            "  \"numRegistration_test\": 38,\n",
            "  \"registration_test\": {\n",
            "    \"count\": 38,\n",
            "    \"example\": {\n",
            "      \"fixed\": \"OASIS/imagesTs/OASIS_0415_0000.nii.gz\",\n",
            "      \"moving\": \"OASIS/imagesTs/OASIS_0416_0000.nii.gz\"\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mzSHxEK5y7Re"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Import Necessary Modules\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import yaml\n",
        "import glob\n",
        "import pickle\n",
        "import random\n",
        "import json\n",
        "from functools import reduce\n",
        "from typing import Tuple, Dict, Any, List, Set, Optional, Union, Callable, Type\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import lightning as L\n",
        "from lightning import LightningModule, Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "from einops import rearrange\n",
        "import timm\n",
        "import wandb\n",
        "import monai\n",
        "import nibabel as nib\n",
        "import warnings\n",
        "import monai.transforms as transforms\n",
        "import logging\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "wandb.login()\n",
        "wandb_logger = WandbLogger(project=\"hvit_test2\")  # Replace with your project name\n",
        "\n",
        "# 3. Define Utility Classes and Functions\n",
        "# 3.1. Logger Class (Ensure only this definition exists)\n",
        "import logging\n",
        "import os\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        # Create handlers\n",
        "        console_handler = logging.StreamHandler()\n",
        "\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        file_handler = logging.FileHandler(os.path.join(save_dir, \"logfile.log\"))\n",
        "\n",
        "        # Create formatters and add to handlers\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        console_handler.setFormatter(formatter)\n",
        "        file_handler.setFormatter(formatter)\n",
        "\n",
        "        # Add handlers to the logger\n",
        "        self.logger.addHandler(console_handler)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def info(self, message):\n",
        "        self.logger.info(message)\n",
        "\n",
        "    def warning(self, message):\n",
        "        self.logger.warning(message)\n",
        "\n",
        "    def error(self, message):\n",
        "        self.logger.error(message)\n",
        "\n",
        "    def debug(self, message):\n",
        "        self.logger.debug(message)\n",
        "\n",
        "# 3.2. Utility Functions\n",
        "def read_yaml_file(file_path):\n",
        "    \"\"\"\n",
        "    Reads a YAML file and returns the content as a dictionary.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        try:\n",
        "            content = yaml.safe_load(file)\n",
        "            return content\n",
        "        except yaml.YAMLError as e:\n",
        "            print(f\"Error reading YAML file: {e}\")\n",
        "            return None\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def get_one_hot(inp_seg, num_labels):\n",
        "    B, C, H, W, D = inp_seg.shape\n",
        "    inp_onehot = nn.functional.one_hot(inp_seg.long(), num_classes=num_labels)\n",
        "    inp_onehot = inp_onehot.squeeze(dim=1)\n",
        "    inp_onehot = inp_onehot.permute(0, 4, 1, 2, 3).contiguous()\n",
        "    return inp_onehot\n",
        "\n",
        "def DiceScore(y_pred, y_true, num_class):\n",
        "    y_true = nn.functional.one_hot(y_true, num_classes=num_class)\n",
        "    y_true = torch.squeeze(y_true, 1)\n",
        "    y_true = y_true.permute(0, 4, 1, 2, 3).contiguous()\n",
        "    intersection = y_pred * y_true\n",
        "    intersection = intersection.sum(dim=[2, 3, 4])\n",
        "    union = torch.pow(y_pred, 2).sum(dim=[2, 3, 4]) + torch.pow(y_true, 2).sum(dim=[2, 3, 4])\n",
        "    dsc = (2.*intersection) / (union + 1e-5)\n",
        "    return dsc\n",
        "\n",
        "# 3.3. Loss Functions\n",
        "class Grad3D(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    N-D gradient loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, penalty='l1', loss_mult=None):\n",
        "        super().__init__()\n",
        "        self.penalty = penalty\n",
        "        self.loss_mult = loss_mult\n",
        "\n",
        "    def forward(self, y_pred):\n",
        "        dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])\n",
        "        dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])\n",
        "        dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])\n",
        "\n",
        "        if self.penalty == 'l2':\n",
        "            dy = dy * dy\n",
        "            dx = dx * dx\n",
        "            dz = dz * dz\n",
        "\n",
        "        d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)\n",
        "        grad = d / 3.0\n",
        "\n",
        "        if self.loss_mult is not None:\n",
        "            grad *= self.loss_mult\n",
        "        return grad\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"Dice loss\"\"\"\n",
        "    def __init__(self, num_class=36):\n",
        "        super().__init__()\n",
        "        self.num_class = num_class\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_true = nn.functional.one_hot(y_true, num_classes=self.num_class)\n",
        "        y_true = torch.squeeze(y_true, 1)\n",
        "        y_true = y_true.permute(0, 4, 1, 2, 3).contiguous()\n",
        "        intersection = y_pred * y_true\n",
        "        intersection = intersection.sum(dim=[2, 3, 4])\n",
        "        union = torch.pow(y_pred, 2).sum(dim=[2, 3, 4]) + torch.pow(y_true, 2).sum(dim=[2, 3, 4])\n",
        "        dsc = (2.*intersection) / (union + 1e-5)\n",
        "        dsc_loss = (1-torch.mean(dsc))\n",
        "        return dsc_loss\n",
        "\n",
        "loss_functions = {\n",
        "    \"mse\": nn.MSELoss(),\n",
        "    \"dice\": DiceLoss(num_class=36),\n",
        "    \"grad\": Grad3D(penalty='l2')\n",
        "}\n",
        "\n",
        "# 4. Define the Dataset Class\n",
        "class OASIS_Dataset(Dataset):\n",
        "    def __init__(self, json_path, mode=\"training\", input_dim=(128, 128, 128), is_pair=False):\n",
        "        self.input_dim = input_dim\n",
        "        self.is_pair = is_pair\n",
        "\n",
        "        # Load JSON data\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if mode == \"training\":\n",
        "            self.samples = data[\"training\"]\n",
        "        elif mode == \"test\":\n",
        "            self.samples = data[\"test\"]\n",
        "        elif mode == \"registration_val\":\n",
        "            self.samples = data[\"registration_val\"]\n",
        "        elif mode == \"registration_test\":\n",
        "            self.samples = data[\"registration_test\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "        self.transforms_image = transforms.Resize(spatial_size=input_dim)\n",
        "        self.transforms_mask = transforms.Resize(spatial_size=input_dim, mode=\"nearest\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_pair:\n",
        "            # Load fixed and moving images for registration\n",
        "            sample = self.samples[index]\n",
        "            fixed_path = sample[\"fixed\"]\n",
        "            moving_path = sample[\"moving\"]\n",
        "\n",
        "            # Load images\n",
        "            fixed = nib.load(fixed_path).get_fdata()\n",
        "            moving = nib.load(moving_path).get_fdata()\n",
        "\n",
        "            # Apply transforms\n",
        "            fixed = self.transforms_image(torch.from_numpy(fixed).unsqueeze(0).float())\n",
        "            moving = self.transforms_image(torch.from_numpy(moving).unsqueeze(0).float())\n",
        "\n",
        "            # Load segmentation masks if available\n",
        "            fixed_mask_path = sample.get(\"fixed_mask\", None)\n",
        "            moving_mask_path = sample.get(\"moving_mask\", None)\n",
        "\n",
        "            if fixed_mask_path and moving_mask_path:\n",
        "                fixed_mask = nib.load(fixed_mask_path).get_fdata()\n",
        "                moving_mask = nib.load(moving_mask_path).get_fdata()\n",
        "\n",
        "                fixed_mask = self.transforms_mask(torch.from_numpy(fixed_mask).unsqueeze(0).long())\n",
        "                moving_mask = self.transforms_mask(torch.from_numpy(moving_mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                # If masks are not available, return dummy masks\n",
        "                fixed_mask = torch.zeros_like(fixed)\n",
        "                moving_mask = torch.zeros_like(moving)\n",
        "\n",
        "            return fixed, moving, fixed_mask, moving_mask\n",
        "        else:\n",
        "            # Load two random images for unpaired registration\n",
        "            selected_samples = random.sample(self.samples, 2)\n",
        "\n",
        "            # Load source image\n",
        "            src_sample = selected_samples[0]\n",
        "            src_path = src_sample[\"image\"]\n",
        "            src = nib.load(src_path).get_fdata()\n",
        "            src = self.transforms_image(torch.from_numpy(src).unsqueeze(0).float())\n",
        "\n",
        "            # Load source mask if available\n",
        "            src_mask_path = src_sample.get(\"mask\", None)\n",
        "            if src_mask_path:\n",
        "                src_mask = nib.load(src_mask_path).get_fdata()\n",
        "                src_mask = self.transforms_mask(torch.from_numpy(src_mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                src_mask = torch.zeros_like(src)\n",
        "\n",
        "            # Load target image\n",
        "            tgt_sample = selected_samples[1]\n",
        "            tgt_path = tgt_sample[\"image\"]\n",
        "            tgt = nib.load(tgt_path).get_fdata()\n",
        "            tgt = self.transforms_image(torch.from_numpy(tgt).unsqueeze(0).float())\n",
        "\n",
        "            # Load target mask if available\n",
        "            tgt_mask_path = tgt_sample.get(\"mask\", None)\n",
        "            if tgt_mask_path:\n",
        "                tgt_mask = nib.load(tgt_mask_path).get_fdata()\n",
        "                tgt_mask = self.transforms_mask(torch.from_numpy(tgt_mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                tgt_mask = torch.zeros_like(tgt)\n",
        "\n",
        "            return src, tgt, src_mask, tgt_mask\n",
        "\n",
        "# 5. Define the Model Components\n",
        "# (Assuming all model classes are defined correctly as per your initial code)\n",
        "\n",
        "# 6. Define the LightningModule\n",
        "class LiTHViT(LightningModule):\n",
        "    def __init__(self, args, config, wandb_logger=None, save_model_every_n_epochs=10):\n",
        "        super().__init__()\n",
        "        self.automatic_optimization = False\n",
        "        self.args = args\n",
        "        self.config = config\n",
        "        self.best_val_loss = 1e8\n",
        "        self.save_model_every_n_epochs = save_model_every_n_epochs\n",
        "        self.lr = args.lr\n",
        "        self.last_epoch = 0\n",
        "        self.tgt2src_reg = args.tgt2src_reg\n",
        "        self.hvit_light = args.hvit_light\n",
        "        self.precision = args.precision\n",
        "\n",
        "        # Initialize logger\n",
        "        self.custom_logger = Logger(save_dir=\"./logs\")\n",
        "\n",
        "        self.hvit = HierarchicalViT_Light(config) if self.hvit_light else HierarchicalViT(config)\n",
        "\n",
        "        self.loss_weights = {\n",
        "            \"mse\": self.args.mse_weights,\n",
        "            \"dice\": self.args.dice_weights,\n",
        "            \"grad\": self.args.grad_weights\n",
        "        }\n",
        "        self.wandb_logger = wandb_logger\n",
        "        self.test_step_outputs = []\n",
        "\n",
        "    def _forward(self, batch, calc_score: bool = False, tgt2src_reg: bool = False):\n",
        "        _loss = {}\n",
        "        _score = 0.\n",
        "\n",
        "        dtype_map = {\n",
        "            'bf16': torch.bfloat16,\n",
        "            'fp32': torch.float32,\n",
        "            'fp16': torch.float16\n",
        "        }\n",
        "        dtype_ = dtype_map.get(self.precision, torch.float32)\n",
        "\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=dtype_):\n",
        "            if tgt2src_reg:\n",
        "                target, source = batch[0].to(dtype=dtype_), batch[1].to(dtype=dtype_)\n",
        "                tgt_seg, src_seg = batch[2], batch[3]\n",
        "            else:\n",
        "                source, target = batch[0].to(dtype=dtype_), batch[1].to(dtype=dtype_)\n",
        "                src_seg, tgt_seg = batch[2], batch[3]\n",
        "\n",
        "            moved, flow = self.hvit(source, target)\n",
        "\n",
        "            if calc_score:\n",
        "                moved_seg = self._get_one_hot_from_src(src_seg, flow, self.args.num_labels)\n",
        "                _score = DiceScore(moved_seg, tgt_seg.long(), self.args.num_labels)\n",
        "\n",
        "            _loss = {}\n",
        "            for key, weight in self.loss_weights.items():\n",
        "                if key == \"mse\":\n",
        "                    _loss[key] = weight * loss_functions[key](moved, target)\n",
        "                elif key == \"dice\":\n",
        "                    moved_seg = self._get_one_hot_from_src(src_seg, flow, self.args.num_labels)\n",
        "                    _loss[key] = weight * loss_functions[key](moved_seg, tgt_seg.long())\n",
        "                elif key == \"grad\":\n",
        "                    _loss[key] = weight * loss_functions[key](flow)\n",
        "\n",
        "            _loss[\"avg_loss\"] = sum(_loss.values()) / len(_loss)\n",
        "        return _loss, _score\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.hvit.train()\n",
        "        opt = self.optimizers()\n",
        "\n",
        "        loss1, _ = self._forward(batch, calc_score=False)\n",
        "        self.manual_backward(loss1[\"avg_loss\"])\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if self.tgt2src_reg:\n",
        "            loss2, _ = self._forward(batch, tgt2src_reg=True, calc_score=False)\n",
        "            self.manual_backward(loss2[\"avg_loss\"])\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        total_loss = {\n",
        "            key: (loss1[key].item() + loss2[key].item()) / 2 if self.tgt2src_reg and key in loss2 else loss1[key].item()\n",
        "            for key in loss1.keys()\n",
        "        }\n",
        "\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics(total_loss, step=self.global_step)\n",
        "        self.custom_logger.info(f\"Batch {batch_idx} - Loss: {total_loss}\")\n",
        "        return total_loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        if self.current_epoch % self.save_model_every_n_epochs == 0:\n",
        "            checkpoints_dir = f\"./checkpoints/{self.current_epoch}\"\n",
        "            os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "            checkpoint_path = f\"{checkpoints_dir}/model_epoch_{self.current_epoch}.ckpt\"\n",
        "            self.trainer.save_checkpoint(checkpoint_path)\n",
        "            self.custom_logger.info(f\"Saved model at epoch {self.current_epoch}\")  # Use custom_logger\n",
        "\n",
        "        current_lr = self.optimizers().param_groups[0]['lr']\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"learning_rate\": current_lr}, step=self.global_step)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        with torch.no_grad():\n",
        "            self.hvit.eval()\n",
        "            _loss, _score = self._forward(batch, calc_score=True)\n",
        "\n",
        "        # Log each component of the validation loss\n",
        "        for loss_name, loss_value in _loss.items():\n",
        "            self.log(f\"val_{loss_name}\", loss_value, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        # Log the mean validation score if available\n",
        "        if _score is not None:\n",
        "            self.log(\"val_score\", _score.mean(), on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        # Log to wandb\n",
        "        if self.wandb_logger:\n",
        "            log_dict = {f\"val_{k}\": v.item() for k, v in _loss.items()}\n",
        "            log_dict.update({\n",
        "                \"val_score_mean\": _score.mean().item() if _score is not None else None,\n",
        "            })\n",
        "            self.wandb_logger.log_metrics({k: v for k, v in log_dict.items() if v is not None}, step=self.global_step)\n",
        "\n",
        "        return {\"val_loss\": _loss[\"avg_loss\"], \"val_score\": _score.mean().item()}\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Callback method called at the end of the validation epoch.\n",
        "        Saves the best model based on validation loss and logs metrics.\n",
        "        \"\"\"\n",
        "        val_loss = self.trainer.callback_metrics.get(\"val_avg_loss\")\n",
        "        if val_loss is not None and self.current_epoch > 0:\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                checkpoints_dir = f\"./checkpoints/{self.current_epoch}\"\n",
        "                os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "                best_model_path = f\"{checkpoints_dir}/best_model.ckpt\"\n",
        "                self.trainer.save_checkpoint(best_model_path)\n",
        "                if self.wandb_logger:\n",
        "                    self.wandb_logger.experiment.log({\n",
        "                        \"best_model_saved\": best_model_path,\n",
        "                        \"best_val_loss\": self.best_val_loss.item()\n",
        "                    })\n",
        "                self.custom_logger.info(f\"New best model saved with validation loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Performs a single test step on a batch of data.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.hvit.eval()\n",
        "            _, _score = self._forward(batch, calc_score=True)\n",
        "\n",
        "        _score = _score.mean() if isinstance(_score, torch.Tensor) else torch.tensor(_score).mean()\n",
        "\n",
        "        self.test_step_outputs.append(_score)\n",
        "\n",
        "        # Log to wandb only if the logger is available\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"test_dice\": _score.item()}, step=self.global_step)\n",
        "\n",
        "        # Return as a dict with tensor values\n",
        "        return {\"test_dice\": _score}\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Callback method called at the end of the test epoch.\n",
        "        Computes and logs the average test Dice score.\n",
        "        \"\"\"\n",
        "        # Calculate the average Dice score across all test steps\n",
        "        avg_test_dice = torch.stack(self.test_step_outputs).mean()\n",
        "\n",
        "        # Log the average test Dice score\n",
        "        self.log(\"avg_test_dice\", avg_test_dice, prog_bar=True)\n",
        "\n",
        "        # Log to wandb if available\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"total_test_dice_avg\": avg_test_dice.item()})\n",
        "\n",
        "        # Clear the test step outputs list for the next test epoch\n",
        "        self.test_step_outputs.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configures the optimizer and learning rate scheduler for the model.\n",
        "        \"\"\"\n",
        "        optimizer = torch.optim.Adam(self.hvit.parameters(), lr=self.lr, weight_decay=0, amsgrad=True)\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=self.lr_lambda)\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"epoch\",\n",
        "                \"frequency\": 1,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def lr_lambda(self, epoch):\n",
        "        \"\"\"\n",
        "        Defines the learning rate schedule.\n",
        "        \"\"\"\n",
        "        return math.pow(1 - epoch / self.trainer.max_epochs, 0.9)\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_checkpoint(cls, checkpoint_path, args=None, wandb_logger=None):\n",
        "        \"\"\"\n",
        "        Loads a model from a checkpoint file.\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "        args = args or checkpoint.get('hyper_parameters', {}).get('args')\n",
        "        config = checkpoint.get('hyper_parameters', {}).get('config')\n",
        "\n",
        "        model = cls(args, config, wandb_logger)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "        if 'hyper_parameters' in checkpoint:\n",
        "            hyper_params = checkpoint['hyper_parameters']\n",
        "            for attr in ['lr', 'best_val_loss', 'last_epoch']:\n",
        "                setattr(model, attr, hyper_params.get(attr, getattr(model, attr)))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        \"\"\"\n",
        "        Callback to save additional information in the checkpoint.\n",
        "        \"\"\"\n",
        "        checkpoint['hyper_parameters'] = {\n",
        "            'config': self.config,\n",
        "            'lr': self.lr,\n",
        "            'best_val_loss': self.best_val_loss,\n",
        "            'last_epoch': self.current_epoch\n",
        "        }\n",
        "\n",
        "    def _get_one_hot_from_src(self, src_seg, flow, num_labels):\n",
        "        \"\"\"\n",
        "        Converts source segmentation to one-hot encoding and applies deformation.\n",
        "        \"\"\"\n",
        "        src_seg_onehot = get_one_hot(src_seg, self.args.num_labels)\n",
        "        deformed_segs = [\n",
        "            self.hvit.spatial_trans(src_seg_onehot[:, i:i+1, ...].float(), flow.float())\n",
        "            for i in range(num_labels)\n",
        "        ]\n",
        "        return torch.cat(deformed_segs, dim=1)\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize the dataset and dataloaders\n",
        "\n",
        "# For training (unpaired data, paired on-the-fly)\n",
        "train_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"training\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=False  # Set to False to enable on-the-fly pairing\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    num_workers=1  # Adjust based on your system\n",
        ")\n",
        "\n",
        "# For validation\n",
        "val_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"registration_val\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=True  # Set to True for validation paired data\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=1\n",
        ")\n",
        "# Define training arguments\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.lr = 0.001\n",
        "        self.mse_weights = 1.0\n",
        "        self.dice_weights = 1.0\n",
        "        self.grad_weights = 1.0\n",
        "        self.tgt2src_reg = False\n",
        "        self.hvit_light = True\n",
        "        self.precision = 'fp32'  # Training precision (e.g., 'bf16', 'fp16', 'fp32')\n",
        "        self.num_labels = 36  # Update based on your dataset\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# 3. Define configuration\n",
        "config = {\n",
        "    'WO_SELF_ATT': False,\n",
        "    '_NUM_CROSS_ATT': -1,\n",
        "    'out_fmaps': ['P4', 'P3', 'P2', 'P1'],  # Number of levels = 4\n",
        "    'scale_level_df': 'P1',\n",
        "    'upsample_df': True,\n",
        "    'upsample_scale_factor': 2,\n",
        "    'fpn_channels': 64,\n",
        "    'start_channels': 32,\n",
        "    'patch_size': [2, 2, 2, 2],  # Matches number of levels\n",
        "    'backbone_net': 'fpn',\n",
        "    'in_channels': 1,\n",
        "\n",
        "    # **Debugged Lines: Update 'data_size' and add 'img_size' to match input_dim**\n",
        "    'data_size': [128, 128, 128],  # Updated from [40, 48, 56]\n",
        "    'img_size': [128, 128, 128],   # Added to align with input_dim\n",
        "\n",
        "    'bias': True,\n",
        "    'norm_type': 'instance',\n",
        "    'kernel_size': 3,\n",
        "    'depths': [1, 1, 1, 1],  # Matches number of levels\n",
        "    'mlp_ratio': 2,\n",
        "    'num_heads': [4, 8, 16, 32],  # Matches number of levels\n",
        "    'drop_path_rate': 0.,\n",
        "    'qkv_bias': True,\n",
        "    'drop_rate': 0.,\n",
        "    'attn_drop_rate': 0.,\n",
        "    'use_seg_loss': False,\n",
        "    'use_seg_proxy_loss': False,\n",
        "    'num_organs': 36,  # Updated to match DiceLoss\n",
        "}\n",
        "# Initialize WandB logger (optional, replace with None if not using WandB)\n",
        "wandb_logger = WandbLogger(project=\"hvit_test\")  # Replace with your project name\n",
        "\n",
        "# Instantiate the Lightning module\n",
        "lit_model = LiTHViT(args, config, wandb_logger=wandb_logger)\n",
        "\n",
        "# Define the PyTorch Lightning Trainer\n",
        "trainer = Trainer(\n",
        "    max_epochs=5,  # Number of epochs\n",
        "    logger=wandb_logger,  # Log training metrics\n",
        "    enable_checkpointing=False,  # Disable checkpointing for testing\n",
        "    devices=1,  # Number of GPUs (set to 0 for CPU)\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",  # Use GPU if available\n",
        "    precision=16 if args.precision == 'fp16' else 32  # Set precision\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.fit(lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2cb50298ddeb4710829e023d04b50ea9",
            "29c70c47e19b4bfea1bdf943f87cb7c3",
            "68e5ef4e452b4ccea5eecc9faa5d2dea",
            "0ceeccb5a742467a97e68573aeef2920",
            "2b4efe4b0d1d4e4c95de7b5bf6867177",
            "8c20936cd9574d598c29a0846a370881",
            "6b2610c39ecd41a7ba25ebe9b42816d6",
            "a7a09b287180453984154055e3f07bba",
            "24b67787b1c7409f8a000600ed2569a7",
            "21d9bdfdd3194b2b83684081afadd8d1",
            "40ce786947e4477b9bb806e10cb7378e",
            "0d19bc550b60450abb51084eb372fda7",
            "a3d921539a6f4c9f8afe53c8580b32b7",
            "c205ff7890f346079fbc8d70c0af07c8",
            "dd9b6de80b6349e782ef345456001b61",
            "3a0c2fd0d9784bcd8dd669e4e2f15727",
            "7ab87cf7b9ce4e44b7444bebbd901d73",
            "0e21705b16734601a0e8f4be95b4f556",
            "b53b7d5516ee4842a1405455e5865e39",
            "20a7d6d275574306af18f6a3c1f71a0b",
            "bf9094e614f7426dbec4e419ceb40a32",
            "53207ea3465042c6a96b82d4d6dca0ef"
          ]
        },
        "id": "x47GIP0GJwFY",
        "outputId": "adfe3e68-1d0a-44d0-d8d2-4e35766ef338"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malim98barnet\u001b[0m (\u001b[33malim98barnet-university-of-tehran\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20250110_130647-ug937y0f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_test/runs/ug937y0f' target=\"_blank\">giddy-durian-1</a></strong> to <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_test' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/hvit_test</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_test/runs/ug937y0f' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/hvit_test/runs/ug937y0f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name | Type                  | Params | Mode \n",
            "-------------------------------------------------------\n",
            "0 | hvit | HierarchicalViT_Light | 17.9 M | train\n",
            "-------------------------------------------------------\n",
            "17.9 M    Trainable params\n",
            "0         Non-trainable params\n",
            "17.9 M    Total params\n",
            "71.731    Total estimated model params size (MB)\n",
            "244       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name | Type                  | Params | Mode \n",
            "-------------------------------------------------------\n",
            "0 | hvit | HierarchicalViT_Light | 17.9 M | train\n",
            "-------------------------------------------------------\n",
            "17.9 M    Trainable params\n",
            "0         Non-trainable params\n",
            "17.9 M    Total params\n",
            "71.731    Total estimated model params size (MB)\n",
            "244       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cb50298ddeb4710829e023d04b50ea9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d19bc550b60450abb51084eb372fda7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-10 13:06:59,021 - __main__ - INFO - Batch 0 - Loss: {'mse': 0.003875497728586197, 'dice': 0.9468063712120056, 'grad': 1.6333691732484112e-08, 'avg_loss': 0.31689396500587463}\n",
            "INFO:__main__:Batch 0 - Loss: {'mse': 0.003875497728586197, 'dice': 0.9468063712120056, 'grad': 1.6333691732484112e-08, 'avg_loss': 0.31689396500587463}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-10 13:07:01,549 - __main__ - INFO - Batch 1 - Loss: {'mse': 0.0029408661648631096, 'dice': 0.9466022253036499, 'grad': 0.0011872303439304233, 'avg_loss': 0.31691011786460876}\n",
            "INFO:__main__:Batch 1 - Loss: {'mse': 0.0029408661648631096, 'dice': 0.9466022253036499, 'grad': 0.0011872303439304233, 'avg_loss': 0.31691011786460876}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-10 13:07:04,097 - __main__ - INFO - Batch 2 - Loss: {'mse': 0.004398035351186991, 'dice': 0.9476045370101929, 'grad': 0.00596154248341918, 'avg_loss': 0.3193213939666748}\n",
            "INFO:__main__:Batch 2 - Loss: {'mse': 0.004398035351186991, 'dice': 0.9476045370101929, 'grad': 0.00596154248341918, 'avg_loss': 0.3193213939666748}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-10 13:07:06,634 - __main__ - INFO - Batch 3 - Loss: {'mse': 0.004532233811914921, 'dice': 0.9477837681770325, 'grad': 0.006855541840195656, 'avg_loss': 0.31972384452819824}\n",
            "INFO:__main__:Batch 3 - Loss: {'mse': 0.004532233811914921, 'dice': 0.9477837681770325, 'grad': 0.006855541840195656, 'avg_loss': 0.31972384452819824}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: \n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
            "INFO:lightning.pytorch.utilities.rank_zero:\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/manual.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# no loop to break at this level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/manual.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# manually capture logged metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m  \u001b[0;31m# release the batch from memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-d4937ae2b2fd>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         total_loss = {\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt2src_reg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-d4937ae2b2fd>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    335\u001b[0m         total_loss = {\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt2src_reg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/data/meta_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;31m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_default_nowrap_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d4937ae2b2fd>\u001b[0m in \u001b[0;36m<cell line: 593>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZB_pza05MnPC",
        "PwIqeAy0C_fL"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2cb50298ddeb4710829e023d04b50ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29c70c47e19b4bfea1bdf943f87cb7c3",
              "IPY_MODEL_68e5ef4e452b4ccea5eecc9faa5d2dea",
              "IPY_MODEL_0ceeccb5a742467a97e68573aeef2920"
            ],
            "layout": "IPY_MODEL_2b4efe4b0d1d4e4c95de7b5bf6867177"
          }
        },
        "29c70c47e19b4bfea1bdf943f87cb7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c20936cd9574d598c29a0846a370881",
            "placeholder": "​",
            "style": "IPY_MODEL_6b2610c39ecd41a7ba25ebe9b42816d6",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "68e5ef4e452b4ccea5eecc9faa5d2dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7a09b287180453984154055e3f07bba",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24b67787b1c7409f8a000600ed2569a7",
            "value": 2
          }
        },
        "0ceeccb5a742467a97e68573aeef2920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21d9bdfdd3194b2b83684081afadd8d1",
            "placeholder": "​",
            "style": "IPY_MODEL_40ce786947e4477b9bb806e10cb7378e",
            "value": " 2/2 [00:03&lt;00:00,  0.63it/s]"
          }
        },
        "2b4efe4b0d1d4e4c95de7b5bf6867177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "8c20936cd9574d598c29a0846a370881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2610c39ecd41a7ba25ebe9b42816d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7a09b287180453984154055e3f07bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b67787b1c7409f8a000600ed2569a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21d9bdfdd3194b2b83684081afadd8d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40ce786947e4477b9bb806e10cb7378e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d19bc550b60450abb51084eb372fda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3d921539a6f4c9f8afe53c8580b32b7",
              "IPY_MODEL_c205ff7890f346079fbc8d70c0af07c8",
              "IPY_MODEL_dd9b6de80b6349e782ef345456001b61"
            ],
            "layout": "IPY_MODEL_3a0c2fd0d9784bcd8dd669e4e2f15727"
          }
        },
        "a3d921539a6f4c9f8afe53c8580b32b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ab87cf7b9ce4e44b7444bebbd901d73",
            "placeholder": "​",
            "style": "IPY_MODEL_0e21705b16734601a0e8f4be95b4f556",
            "value": "Epoch 0:   0%"
          }
        },
        "c205ff7890f346079fbc8d70c0af07c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b53b7d5516ee4842a1405455e5865e39",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20a7d6d275574306af18f6a3c1f71a0b",
            "value": 0
          }
        },
        "dd9b6de80b6349e782ef345456001b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf9094e614f7426dbec4e419ceb40a32",
            "placeholder": "​",
            "style": "IPY_MODEL_53207ea3465042c6a96b82d4d6dca0ef",
            "value": " 0/414 [00:00&lt;?, ?it/s]"
          }
        },
        "3a0c2fd0d9784bcd8dd669e4e2f15727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "7ab87cf7b9ce4e44b7444bebbd901d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e21705b16734601a0e8f4be95b4f556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b53b7d5516ee4842a1405455e5865e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20a7d6d275574306af18f6a3c1f71a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf9094e614f7426dbec4e419ceb40a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53207ea3465042c6a96b82d4d6dca0ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}