{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCHYZADeVcUDfBWjzUA/au",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db1256b7a7a641998d4904221666c845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7108a8dc1411426c8d7c422af0d5864d",
              "IPY_MODEL_a6463491ec3444d28cc0fd3d5a1a2540",
              "IPY_MODEL_b23d51f544c147b1b153e029815e56cc"
            ],
            "layout": "IPY_MODEL_c4b46d7508084e6d93102a7e0bae40d2"
          }
        },
        "7108a8dc1411426c8d7c422af0d5864d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdef3703853e40ab9ce2f9ec8489d0b3",
            "placeholder": "​",
            "style": "IPY_MODEL_67c6dadd23514d15bc24efd494d88131",
            "value": "Epoch 7: 100%"
          }
        },
        "a6463491ec3444d28cc0fd3d5a1a2540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d272704e994e4d2798793dc8349260fe",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57cf2c6d07184dcd96a2afec5036e535",
            "value": 100
          }
        },
        "b23d51f544c147b1b153e029815e56cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20348541f4b5407f8ffab8400b9c883c",
            "placeholder": "​",
            "style": "IPY_MODEL_8727a34574514ea4ba0ef752320d5ae9",
            "value": " 100/100 [01:59&lt;00:00,  0.84it/s, v_num=0, train_loss_step=0.183, train_loss_epoch=0.182]"
          }
        },
        "c4b46d7508084e6d93102a7e0bae40d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "cdef3703853e40ab9ce2f9ec8489d0b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c6dadd23514d15bc24efd494d88131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d272704e994e4d2798793dc8349260fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57cf2c6d07184dcd96a2afec5036e535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20348541f4b5407f8ffab8400b9c883c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8727a34574514ea4ba0ef752320d5ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/Thesis/blob/main/VMAEPro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZYI889qNGxu",
        "outputId": "8cc9c0ad-84f6-4f92-d519-caa13b39eb71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip -q install torch torchvision torchaudio\n",
        "!pip -q install einops\n",
        "!pip -q install monai\n",
        "!pip -q install torchmetrics\n",
        "!pip -q install timm\n",
        "!pip -q install pytorch-lightning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: This is a simplified version of the decoder. Incorporating generative and diffusion-based enhancements like Latent Diffusion Models (LDM) requires additional implementation, which can be complex. For demonstration purposes, we'll keep it basic, but you should consider integrating libraries like Diffusers or custom implementations for LDM."
      ],
      "metadata": {
        "id": "C_-cvWIJNZpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pytorch_lightning as pl\n",
        "# from pytorch_lightning import Trainer\n",
        "# from pytorch_lightning.callbacks import ModelSummary"
      ],
      "metadata": {
        "id": "QVarrqcZeXS_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from einops import rearrange\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from monai.transforms import (\n",
        "     Compose, LoadImaged, ScaleIntensityd, EnsureTyped, Resized\n",
        ")\n",
        "from monai.data import CacheDataset, load_decathlon_datalist\n",
        "from torchmetrics import Dice\n",
        "import timm\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelSummary"
      ],
      "metadata": {
        "id": "iOACnZ2teJze"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 3. Dummy Dataset\n",
        "# ============================\n",
        "\n",
        "class DummyMedicalImageRegistrationDataset(Dataset):\n",
        "    def __init__(self, num_samples=100, volume_size=(64, 64, 64), patch_size=(16,16,16), mask_ratio=0.75):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_samples (int): Number of samples in the dataset.\n",
        "            volume_size (tuple): Size of the 3D volume (D, H, W).\n",
        "            patch_size (tuple): Size of each 3D patch (P_D, P_H, P_W).\n",
        "            mask_ratio (float): Ratio of patches to mask.\n",
        "        \"\"\"\n",
        "        self.num_samples = num_samples\n",
        "        self.volume_size = volume_size\n",
        "        self.patch_size = patch_size\n",
        "        self.mask_ratio = mask_ratio\n",
        "\n",
        "        # Calculate number of patches per volume\n",
        "        self.num_patches = (volume_size[0] // patch_size[0]) * \\\n",
        "                           (volume_size[1] // patch_size[1]) * \\\n",
        "                           (volume_size[2] // patch_size[2])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Generate random source and target volumes\n",
        "        source = np.random.rand(*self.volume_size).astype(np.float32)\n",
        "        target = np.random.rand(*self.volume_size).astype(np.float32)\n",
        "\n",
        "        # Concatenate along channel dimension\n",
        "        concatenated = np.stack([source, target], axis=0)  # Shape: (2, D, H, W)\n",
        "\n",
        "        # Divide into patches\n",
        "        patches = self.divide_into_patches(concatenated, self.patch_size)  # Shape: (N_patches, 2, P_D, P_H, P_W)\n",
        "\n",
        "        # Masking\n",
        "        N_visible = int((1 - self.mask_ratio) * self.num_patches)\n",
        "        indices = np.random.permutation(self.num_patches)\n",
        "        visible_indices = indices[:N_visible]\n",
        "        masked_indices = indices[N_visible:]\n",
        "\n",
        "        visible_patches = patches[visible_indices]  # Shape: (N_visible, 2, P_D, P_H, P_W)\n",
        "        masked_patches = patches[masked_indices]    # Shape: (N_masked, 2, P_D, P_H, P_W)\n",
        "\n",
        "        sample = {\n",
        "            'visible_patches': torch.tensor(visible_patches, dtype=torch.float32),  # (N_visible, 2, P_D, P_H, P_W)\n",
        "            'masked_patches': torch.tensor(masked_patches, dtype=torch.float32),    # (N_masked, 2, P_D, P_H, P_W)\n",
        "            'masked_indices': torch.tensor(masked_indices, dtype=torch.long),      # (N_masked,)\n",
        "            'source': torch.tensor(source, dtype=torch.float32),                    # (D, H, W)\n",
        "            'target': torch.tensor(target, dtype=torch.float32)                     # (D, H, W)\n",
        "        }\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def divide_into_patches(self, volume, patch_size):\n",
        "        \"\"\"\n",
        "        Divide a 3D volume into non-overlapping patches.\n",
        "        Args:\n",
        "            volume (np.array): 3D volume with shape (C, D, H, W)\n",
        "            patch_size (tuple): Size of each patch (P_D, P_H, P_W)\n",
        "        Returns:\n",
        "            patches (np.array): Array of patches with shape (N_patches, C, P_D, P_H, P_W)\n",
        "        \"\"\"\n",
        "        C, D, H, W = volume.shape\n",
        "        P_D, P_H, P_W = patch_size\n",
        "        assert D % P_D == 0 and H % P_H == 0 and W % P_W == 0, \"Volume dimensions must be divisible by patch size.\"\n",
        "        patches = volume.reshape(\n",
        "            C,\n",
        "            D//P_D, P_D,\n",
        "            H//P_H, P_H,\n",
        "            W//P_W, P_W\n",
        "        )\n",
        "        patches = patches.transpose(1,2,3,4,5,6,0)\n",
        "        patches = patches.reshape(-1, C, P_D, P_H, P_W)\n",
        "        return patches\n",
        "# ============================\n",
        "# 4. Initialize Dataset and DataLoader\n",
        "# ============================\n",
        "\n",
        "# Initialize dataset and dataloader\n",
        "num_samples = 100\n",
        "volume_size = (48, 48, 48)       # Depth, Height, Width\n",
        "patch_size = (16, 16, 16)        # Patch size\n",
        "mask_ratio = 19/27               # 75% patches masked\n",
        "\n",
        "dataset = DummyMedicalImageRegistrationDataset(\n",
        "    num_samples=num_samples,\n",
        "    volume_size=volume_size,\n",
        "    patch_size=patch_size,\n",
        "    mask_ratio=mask_ratio\n",
        ")\n",
        "\n",
        "batch_size = 1  # Set to 1 to ensure N_patches_new is a perfect cube (64)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "# ============================\n",
        "# 5. Patch Embedding\n",
        "# ============================\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=2, embed_dim=768, patch_size=(16,16,16)):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (B * N_visible, C, P_D, P_H, P_W)\n",
        "        Returns:\n",
        "            embeddings: (B * N_visible, embed_dim)\n",
        "        \"\"\"\n",
        "        B_N, C, P_D, P_H, P_W = x.shape\n",
        "        x = self.proj(x)  # (B * N_visible, embed_dim, 1, 1, 1)\n",
        "        x = x.view(B_N, self.embed_dim)  # (B * N_visible, embed_dim)\n",
        "        return x\n",
        "# ============================\n",
        "# 6. Hierarchical Vision Transformer (H-ViT) Encoder\n",
        "# ============================\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4., dropout=0.):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (S, B, embed_dim)\n",
        "        x2 = self.norm1(x)\n",
        "        attn_output, _ = self.attn(x2, x2, x2)  # Self-attention\n",
        "        x = x + self.dropout1(attn_output)\n",
        "        x2 = self.norm2(x)\n",
        "        x = x + self.mlp(x2)\n",
        "        return x\n",
        "\n",
        "class HierarchicalViTEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=768, num_heads=12, num_layers=6, mlp_ratio=4., dropout=0.,\n",
        "                 num_stages=1, patch_size=(16,16,16)):\n",
        "        super(HierarchicalViTEncoder, self).__init__()\n",
        "        self.num_stages = num_stages\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Define transformer layers for each stage\n",
        "        self.transformer_layers = nn.ModuleList()\n",
        "        for stage in range(num_stages):\n",
        "            for _ in range(num_layers):\n",
        "                self.transformer_layers.append(\n",
        "                    TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "                )\n",
        "\n",
        "        # Downsampling layers between stages (if any)\n",
        "        self.downsamples = nn.ModuleList()\n",
        "        for stage in range(num_stages - 1):\n",
        "            self.downsamples.append(\n",
        "                nn.Conv3d(embed_dim, embed_dim, kernel_size=2, stride=2)\n",
        "            )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (B, N_patches, embed_dim)\n",
        "        Returns:\n",
        "            features: list of feature maps at different scales\n",
        "            x: (B * N_patches_new, embed_dim)\n",
        "        \"\"\"\n",
        "        features = []\n",
        "        B, N_patches, E = x.shape\n",
        "        x = x.view(B * N_patches, E)  # Flatten patches across batch\n",
        "\n",
        "        for stage in range(self.num_stages):\n",
        "            # Transformer layers\n",
        "            for _ in range(len(self.transformer_layers) // self.num_stages):\n",
        "                layer = self.transformer_layers.pop(0)\n",
        "                # Prepare input for MultiheadAttention: (S, B, E)\n",
        "                # Here, treat each patch as a sequence element\n",
        "                x = x.unsqueeze(0)  # (1, B * N_patches, E)\n",
        "                x = layer(x)         # (1, B * N_patches, E)\n",
        "                x = x.squeeze(0)     # (B * N_patches, E)\n",
        "\n",
        "            # Collect features\n",
        "            features.append(x.clone())\n",
        "\n",
        "            if stage < self.num_stages -1:\n",
        "                # Compute D, H, W\n",
        "                N_patches_new = B * N_patches\n",
        "                D = H = W = int(np.ceil(N_patches_new ** (1/3)))\n",
        "                padding = D * H * W - N_patches_new\n",
        "                if padding >0:\n",
        "                    pad_tensor = torch.zeros(B * padding, E).to(x.device)\n",
        "                    x = torch.cat([x, pad_tensor], dim=0)  # (B * N_patches_new + padding, E)\n",
        "\n",
        "                # Reshape to (B, embed_dim, D, H, W) using einops\n",
        "                x = rearrange(x, '(b p) e -> b e d h w', b=B, p=D*H*W//(D*H*W))  # Correct pattern\n",
        "\n",
        "                # Downsample using the corresponding downsampling layer\n",
        "                x_downsampled = self.downsamples[stage](x)    # (B, embed_dim, D/2, H/2, W/2)\n",
        "\n",
        "                # Flatten back to patches using einops\n",
        "                x = rearrange(x_downsampled, 'b c d h w -> b (d h w) c')  # (B, N_patches_new, embed_dim)\n",
        "                x = x.view(B * x.shape[1], self.embed_dim)  # (B * N_patches_new, embed_dim)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return features, x  # Return all intermediate features and final output\n",
        "# ============================\n",
        "# 7. Vision Transformer Masked Autoencoder (ViT-MAE) Decoder\n",
        "# ============================\n",
        "\n",
        "class ViT_MAE_Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim=768, num_heads=12, num_layers=6, mlp_ratio=4., dropout=0.,\n",
        "                 patch_size=(16,16,16)):\n",
        "        super(ViT_MAE_Decoder, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.output_layer = nn.Linear(embed_dim, 2 * np.prod(patch_size))  # Reconstruct 2 channels\n",
        "\n",
        "    def forward(self, x, masked_indices):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (B, N_patches_new, embed_dim)\n",
        "            masked_indices: (B, N_masked)\n",
        "        Returns:\n",
        "            reconstructed: (B, N_masked, 2, P_D, P_H, P_W)\n",
        "        \"\"\"\n",
        "        B, N_patches_new, E = x.shape\n",
        "        N_masked = masked_indices.shape[1]\n",
        "\n",
        "        # Reshape x to (B, N_patches_new, embed_dim)\n",
        "        x = x  # (B, N_patches_new, embed_dim)\n",
        "\n",
        "        # Pass through decoder transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            # Prepare input for MultiheadAttention: (S, B, E)\n",
        "            x = x.transpose(0,1)  # (N_patches_new, B, E)\n",
        "            x = layer(x)           # (N_patches_new, B, E)\n",
        "            x = x.transpose(0,1)  # (B, N_patches_new, E)\n",
        "\n",
        "        x = self.norm(x)  # (B, N_patches_new, E)\n",
        "\n",
        "        # Generate mask tokens (learnable or random)\n",
        "        mask_tokens = torch.randn(B, N_masked, E).to(x.device)  # (B, N_masked, E)\n",
        "\n",
        "        # Concatenate visible and mask tokens\n",
        "        x = torch.cat([x, mask_tokens], dim=1)  # (B, N_patches_new + N_masked, E)\n",
        "\n",
        "        # Pass through decoder transformer layers again\n",
        "        for layer in self.transformer_layers:\n",
        "            x = x.transpose(0,1)  # (N_total, B, E)\n",
        "            x = layer(x)           # (N_total, B, E)\n",
        "            x = x.transpose(0,1)  # (B, N_total, E)\n",
        "\n",
        "        x = self.norm(x)  # (B, N_total, E)\n",
        "\n",
        "        # Split visible and masked tokens\n",
        "        visible = x[:, :N_patches_new, :]  # (B, N_patches_new, E)\n",
        "        masked = x[:, N_patches_new:, :]   # (B, N_masked, E)\n",
        "\n",
        "        # Reconstruct masked patches\n",
        "        reconstructed = self.output_layer(masked)  # (B, N_masked, 2 * P_D * P_H * P_W)\n",
        "        reconstructed = reconstructed.view(B, N_masked, 2, *self.patch_size)  # (B, N_masked, 2, P_D, P_H, P_W)\n",
        "\n",
        "        return reconstructed\n",
        "# ============================\n",
        "# 8. Registration Head\n",
        "# ============================\n",
        "\n",
        "class RegistrationHead(nn.Module):\n",
        "    def __init__(self, in_channels=768, base_channels=512):\n",
        "        super(RegistrationHead, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, base_channels, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv3d(base_channels, base_channels//2, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv3 = nn.Conv3d(base_channels//2, 3, kernel_size=3, padding=1)  # 3D displacement vectors\n",
        "        self.tanh = nn.Tanh()  # To constrain displacement values, adjust as needed\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (B, C, D, H, W)\n",
        "        Returns:\n",
        "            phi: (B, 3, D, H, W)\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        phi = self.conv3(x)\n",
        "        phi = self.tanh(phi)\n",
        "        return phi\n",
        "# ============================\n",
        "# 9. Spatial Transformer\n",
        "# ============================\n",
        "class SpatialTransformer3D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialTransformer3D, self).__init__()\n",
        "\n",
        "    def forward(self, src, phi):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: (B, D_src, H_src, W_src)\n",
        "            phi: (B, 3, D_phi, H_phi, W_phi) displacement vectors\n",
        "        Returns:\n",
        "            warped_src: (B, D_src, H_src, W_src)\n",
        "        \"\"\"\n",
        "        B, D_src, H_src, W_src = src.shape\n",
        "        device = src.device\n",
        "\n",
        "        # Create grid with source's spatial dimensions\n",
        "        grid_d, grid_h, grid_w = torch.meshgrid(\n",
        "            torch.linspace(-1, 1, D_src, device=device),\n",
        "            torch.linspace(-1, 1, H_src, device=device),\n",
        "            torch.linspace(-1, 1, W_src, device=device),\n",
        "            indexing='ij'  # Specify indexing to avoid future warnings\n",
        "        )\n",
        "\n",
        "        grid = torch.stack((grid_w, grid_h, grid_d), dim=-1)  # (D_src, H_src, W_src, 3)\n",
        "        grid = grid.unsqueeze(0).repeat(B, 1, 1, 1, 1)      # (B, D_src, H_src, W_src, 3)\n",
        "\n",
        "        # Normalize displacement to [-1, 1]\n",
        "        # Rearrange phi to (B, C, D_phi, H_phi, W_phi)\n",
        "        phi_norm = rearrange(phi, 'b c d h w -> b c d h w') * 0.1  # (B, 3, D_phi, H_phi, W_phi)\n",
        "\n",
        "        # Upsample phi_norm to match source's spatial dimensions\n",
        "        phi_norm_upsampled = F.interpolate(\n",
        "            phi_norm,\n",
        "            size=(D_src, H_src, W_src),\n",
        "            mode='trilinear',\n",
        "            align_corners=True\n",
        "        )  # (B, 3, D_src, H_src, W_src)\n",
        "\n",
        "        # Rearrange to (B, D_src, H_src, W_src, 3)\n",
        "        phi_norm_upsampled = rearrange(phi_norm_upsampled, 'b c d h w -> b d h w c')  # (B, D_src, H_src, W_src, 3)\n",
        "\n",
        "        # Add displacement to grid\n",
        "        grid = grid + phi_norm_upsampled  # (B, D_src, H_src, W_src, 3)\n",
        "\n",
        "        # Sample using grid_sample\n",
        "        src = src.unsqueeze(1)  # (B, 1, D_src, H_src, W_src)\n",
        "        warped_src = F.grid_sample(\n",
        "            src,\n",
        "            grid,\n",
        "            mode='bilinear',\n",
        "            padding_mode='border',\n",
        "            align_corners=True\n",
        "        )  # (B, 1, D_src, H_src, W_src)\n",
        "\n",
        "        warped_src = warped_src.squeeze(1)  # (B, D_src, H_src, W_src)\n",
        "\n",
        "        return warped_src\n",
        "\n",
        "# ============================\n",
        "# 10. Loss Function\n",
        "# ============================\n",
        "\n",
        "class VMAEProLoss(nn.Module):\n",
        "    def __init__(self, lambda_recon=1.0, lambda_reg=1.0, lambda_smooth=0.1):\n",
        "        super(VMAEProLoss, self).__init__()\n",
        "        self.lambda_recon = lambda_recon\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.lambda_smooth = lambda_smooth\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, reconstructed, masked_patches, warped_src, target, phi):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            reconstructed: (B, N_masked, 2, P_D, P_H, P_W)\n",
        "            masked_patches: (B, N_masked, 2, P_D, P_H, P_W)\n",
        "            warped_src: (B, D, H, W)\n",
        "            target: (B, D, H, W)\n",
        "            phi: (B, 3, D, H, W)\n",
        "        Returns:\n",
        "            total_loss: scalar\n",
        "        \"\"\"\n",
        "        # Reconstruction Loss\n",
        "        recon_loss = self.mse(reconstructed, masked_patches)\n",
        "\n",
        "        # Registration Loss\n",
        "        reg_loss = self.mse(warped_src, target)\n",
        "\n",
        "        # Smoothness Loss (encouraging small displacements)\n",
        "        smooth_loss = self.mse(phi, torch.zeros_like(phi))\n",
        "\n",
        "        # Total Loss\n",
        "        total_loss = self.lambda_recon * recon_loss + \\\n",
        "                     self.lambda_reg * reg_loss + \\\n",
        "                     self.lambda_smooth * smooth_loss\n",
        "        return total_loss\n",
        "# ============================\n",
        "# 11. Complete Model\n",
        "# ============================\n",
        "\n",
        "\n",
        "class VMAEProModel(nn.Module):\n",
        "    def __init__(self, embed_dim=768, num_heads=12, num_layers=6, mlp_ratio=4., dropout=0.,\n",
        "                 patch_size=(16,16,16), mask_ratio=0.75, base_channels=512):\n",
        "        super(VMAEProModel, self).__init__()\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=2, embed_dim=embed_dim, patch_size=patch_size)\n",
        "        self.encoder = HierarchicalViTEncoder(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            dropout=dropout,\n",
        "            num_stages=1,  # Set to 1 to avoid reshaping issues\n",
        "            patch_size=patch_size\n",
        "        )\n",
        "        self.decoder = ViT_MAE_Decoder(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            dropout=dropout,\n",
        "            patch_size=patch_size\n",
        "        )\n",
        "        self.registration_head = RegistrationHead(in_channels=embed_dim, base_channels=base_channels)\n",
        "        self.spatial_transformer = SpatialTransformer3D()\n",
        "\n",
        "    def forward(self, visible_patches, masked_indices, source, target):\n",
        "        B, N_visible, C, P_D, P_H, P_W = visible_patches.shape\n",
        "\n",
        "        # Reshape visible_patches to (B * N_visible, C, P_D, P_H, P_W)\n",
        "        visible_patches = visible_patches.view(B * N_visible, C, P_D, P_H, P_W)\n",
        "\n",
        "        # Patch Embedding\n",
        "        embeddings = self.patch_embedding(visible_patches)  # (B * N_visible, embed_dim)\n",
        "\n",
        "        # Reshape embeddings back to (B, N_visible, embed_dim) using einops\n",
        "        embeddings = rearrange(embeddings, '(b n) e -> b n e', b=B, n=N_visible)  # (B, N_visible, embed_dim)\n",
        "\n",
        "        # Encoder\n",
        "        features, encoder_output = self.encoder(embeddings)  # features: list, encoder_output: (B * N_patches_new, embed_dim)\n",
        "\n",
        "        # Reshape encoder_output to (B, N_patches_new, embed_dim) using einops\n",
        "        N_patches_new = encoder_output.shape[0] // B\n",
        "        encoder_output = rearrange(encoder_output, '(b p) e -> b p e', b=B, p=N_patches_new)  # (B, N_patches_new, embed_dim)\n",
        "\n",
        "        # Decoder (Reconstruction)\n",
        "        reconstructed = self.decoder(encoder_output, masked_indices)  # (B, N_masked, 2, P_D, P_H, P_W)\n",
        "\n",
        "        # Registration Head\n",
        "        # Reshape encoder_output to (B, embed_dim, D, H, W) using einops\n",
        "        D = H = W = int(round(N_patches_new ** (1/3)))  # For N_patches_new=8, D=2\n",
        "        assert D * H * W == N_patches_new, f\"D*H*W={D*H*W} does not equal N_patches_new={N_patches_new}\"\n",
        "\n",
        "        # Corrected rearrange pattern\n",
        "        registration_input = rearrange(encoder_output, 'b (d h w) e -> b e d h w', d=D, h=H, w=W)  # (B, embed_dim, D, H, W)\n",
        "        phi = self.registration_head(registration_input)  # (B, 3, D, H, W)\n",
        "\n",
        "        # Spatial Transformer\n",
        "        warped_src = self.spatial_transformer(source, phi)  # (B, D, H, W)\n",
        "\n",
        "        return reconstructed, warped_src, phi\n",
        "\n",
        "class VMAEProLightningModule(pl.LightningModule):\n",
        "    def __init__(self, embed_dim=768, num_heads=12, num_layers=6, mlp_ratio=4., dropout=0.,\n",
        "                 patch_size=(16,16,16), mask_ratio=0.75, base_channels=512, learning_rate=1e-4):\n",
        "        super(VMAEProLightningModule, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Initialize your model components\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=2, embed_dim=embed_dim, patch_size=patch_size)\n",
        "        self.encoder = HierarchicalViTEncoder(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            dropout=dropout,\n",
        "            num_stages=1,  # Set to 1 to avoid reshaping issues\n",
        "            patch_size=patch_size\n",
        "        )\n",
        "        self.decoder = ViT_MAE_Decoder(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            dropout=dropout,\n",
        "            patch_size=patch_size\n",
        "        )\n",
        "        self.registration_head = RegistrationHead(in_channels=embed_dim, base_channels=base_channels)\n",
        "        self.spatial_transformer = SpatialTransformer3D()\n",
        "        self.criterion = VMAEProLoss(\n",
        "            lambda_recon=1.0,\n",
        "            lambda_reg=1.0,\n",
        "            lambda_smooth=0.1  # Adjust as needed\n",
        "        )\n",
        "\n",
        "    def forward(self, visible_patches, masked_indices, source, target):\n",
        "        B, N_visible, C, P_D, P_H, P_W = visible_patches.shape\n",
        "\n",
        "        # Reshape visible_patches to (B * N_visible, C, P_D, P_H, P_W)\n",
        "        visible_patches = visible_patches.view(B * N_visible, C, P_D, P_H, P_W)\n",
        "\n",
        "        # Patch Embedding\n",
        "        embeddings = self.patch_embedding(visible_patches)  # (B * N_visible, embed_dim)\n",
        "\n",
        "        # Reshape embeddings back to (B, N_visible, embed_dim) using einops\n",
        "        embeddings = rearrange(embeddings, '(b n) e -> b n e', b=B, n=N_visible)  # (B, N_visible, embed_dim)\n",
        "\n",
        "        # Encoder\n",
        "        features, encoder_output = self.encoder(embeddings)  # features: list, encoder_output: (B * N_patches_new, embed_dim)\n",
        "\n",
        "        # Reshape encoder_output to (B, N_patches_new, embed_dim) using einops\n",
        "        N_patches_new = encoder_output.shape[0] // B\n",
        "        encoder_output = rearrange(encoder_output, '(b p) e -> b p e', b=B, p=N_patches_new)  # (B, N_patches_new, embed_dim)\n",
        "\n",
        "        # Decoder (Reconstruction)\n",
        "        reconstructed = self.decoder(encoder_output, masked_indices)  # (B, N_masked, 2, P_D, P_H, P_W)\n",
        "\n",
        "        # Registration Head\n",
        "        # Reshape encoder_output to (B, embed_dim, D, H, W) using einops\n",
        "        D = H = W = int(round(N_patches_new ** (1/3)))  # For N_patches_new=8, D=2\n",
        "        assert D * H * W == N_patches_new, f\"D*H*W={D*H*W} does not equal N_patches_new={N_patches_new}\"\n",
        "\n",
        "        # Corrected rearrange pattern\n",
        "        registration_input = rearrange(encoder_output, 'b (d h w) e -> b e d h w', d=D, h=H, w=W)  # (B, embed_dim, D, H, W)\n",
        "        phi = self.registration_head(registration_input)  # (B, 3, D, H, W)\n",
        "\n",
        "        # Spatial Transformer\n",
        "        warped_src = self.spatial_transformer(source, phi)  # (B, D_src, H_src, W_src)\n",
        "\n",
        "        return reconstructed, warped_src, phi\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        visible_patches = batch['visible_patches']  # (B, N_visible, 2, P_D, P_H, P_W)\n",
        "        masked_patches = batch['masked_patches']    # (B, N_masked, 2, P_D, P_H, P_W)\n",
        "        masked_indices = batch['masked_indices']    # (B, N_masked)\n",
        "        source = batch['source']                    # (B, D_src, H_src, W_src)\n",
        "        target = batch['target']                    # (B, D_src, H_src, W_src)\n",
        "\n",
        "        # Forward pass\n",
        "        reconstructed, warped_src, phi = self.forward(visible_patches, masked_indices, source, target)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.criterion(reconstructed, masked_patches, warped_src, target, phi)\n",
        "\n",
        "        # Log loss\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "        return [optimizer], [scheduler]\n",
        "# 12. Training Loop\n",
        "# ============================\n",
        "\n",
        "def train_one_step(model, dataloader, optimizer, criterion, device, num_epochs=10, scheduler=None):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        epoch_loss = 0.0\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            visible_patches = batch['visible_patches'].to(device)  # (B, N_visible, 2, P_D, P_H, P_W)\n",
        "            masked_patches = batch['masked_patches'].to(device)    # (B, N_masked, 2, P_D, P_H, P_W)\n",
        "            masked_indices = batch['masked_indices'].to(device)    # (B, N_masked)\n",
        "            source = batch['source'].to(device)                    # (B, D, H, W)\n",
        "            target = batch['target'].to(device)                    # (B, D, H, W)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            reconstructed, warped_src, phi = model(visible_patches, masked_indices, source, target)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(reconstructed, masked_patches, warped_src, target, phi)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "# ============================\n",
        "# 13. Execution\n",
        "# ============================\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize the model\n",
        "# embed_dim = 768\n",
        "# num_heads = 12\n",
        "# num_layers = 6\n",
        "# mlp_ratio = 4.\n",
        "# dropout = 0.\n",
        "# patch_size = (16,16,16)\n",
        "# mask_ratio = 0.75\n",
        "# base_channels = 512\n",
        "\n",
        "# model = VMAEProModel(\n",
        "#     embed_dim=embed_dim,\n",
        "#     num_heads=num_heads,\n",
        "#     num_layers=num_layers,\n",
        "#     mlp_ratio=mlp_ratio,\n",
        "#     dropout=dropout,\n",
        "#     patch_size=patch_size,\n",
        "#     mask_ratio=mask_ratio,\n",
        "#     base_channels=base_channels\n",
        "# )\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "# learning_rate = 1e-4\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Decays LR by 0.1 every 5 epochs\n",
        "\n",
        "# # Initialize loss function\n",
        "# criterion = VMAEProLoss(\n",
        "#     lambda_recon=1.0,\n",
        "#     lambda_reg=1.0,\n",
        "#     lambda_smooth=0.1  # Lower weight for smoothness to prevent overpowering other losses\n",
        "# )\n",
        "\n",
        "# Start training\n",
        "# num_epochs = 10\n",
        "# train_one_step(\n",
        "#     model=model,\n",
        "#     dataloader=dataloader,\n",
        "#     optimizer=optimizer,\n",
        "#     criterion=criterion,\n",
        "#     device=device,\n",
        "#     num_epochs=num_epochs,\n",
        "#     scheduler=scheduler\n",
        "# )\n",
        "\n",
        "model = VMAEProLightningModule(\n",
        "    embed_dim=768,\n",
        "    num_heads=12,\n",
        "    num_layers=6,\n",
        "    mlp_ratio=4.,\n",
        "    dropout=0.,\n",
        "    patch_size=(16,16,16),\n",
        "    mask_ratio=19/27,  # Approximately 0.7037 to get N_visible=8 patches\n",
        "    base_channels=512,\n",
        "    learning_rate=1e-4\n",
        ")\n",
        "\n",
        "# Define callbacks\n",
        "model_summary_callback = ModelSummary(max_depth=-1)  # max_depth=-1 for full summary\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    max_epochs=10,\n",
        "    callbacks=[model_summary_callback],\n",
        "    accelerator='cpu',  # Change to 'gpu' if using CUDA\n",
        "    devices=1,          # Number of GPUs or CPUs\n",
        "    log_every_n_steps=1\n",
        ")\n",
        "# Start training\n",
        "trainer.fit(model, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "db1256b7a7a641998d4904221666c845",
            "7108a8dc1411426c8d7c422af0d5864d",
            "a6463491ec3444d28cc0fd3d5a1a2540",
            "b23d51f544c147b1b153e029815e56cc",
            "c4b46d7508084e6d93102a7e0bae40d2",
            "cdef3703853e40ab9ce2f9ec8489d0b3",
            "67c6dadd23514d15bc24efd494d88131",
            "d272704e994e4d2798793dc8349260fe",
            "57cf2c6d07184dcd96a2afec5036e535",
            "20348541f4b5407f8ffab8400b9c883c",
            "8727a34574514ea4ba0ef752320d5ae9"
          ]
        },
        "id": "_DHBR56wYMe6",
        "outputId": "9a6b9c84-dde2-48c3-ac60-d5d0d7dbcb0a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "    | Name                                       | Type                            | Params | Mode \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | patch_embedding                            | PatchEmbedding                  | 6.3 M  | train\n",
            "1   | patch_embedding.proj                       | Conv3d                          | 6.3 M  | train\n",
            "2   | encoder                                    | HierarchicalViTEncoder          | 42.5 M | train\n",
            "3   | encoder.transformer_layers                 | ModuleList                      | 42.5 M | train\n",
            "4   | encoder.transformer_layers.0               | TransformerBlock                | 7.1 M  | train\n",
            "5   | encoder.transformer_layers.0.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "6   | encoder.transformer_layers.0.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "7   | encoder.transformer_layers.0.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "8   | encoder.transformer_layers.0.dropout1      | Dropout                         | 0      | train\n",
            "9   | encoder.transformer_layers.0.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "10  | encoder.transformer_layers.0.mlp           | Sequential                      | 4.7 M  | train\n",
            "11  | encoder.transformer_layers.0.mlp.0         | Linear                          | 2.4 M  | train\n",
            "12  | encoder.transformer_layers.0.mlp.1         | GELU                            | 0      | train\n",
            "13  | encoder.transformer_layers.0.mlp.2         | Dropout                         | 0      | train\n",
            "14  | encoder.transformer_layers.0.mlp.3         | Linear                          | 2.4 M  | train\n",
            "15  | encoder.transformer_layers.0.mlp.4         | Dropout                         | 0      | train\n",
            "16  | encoder.transformer_layers.1               | TransformerBlock                | 7.1 M  | train\n",
            "17  | encoder.transformer_layers.1.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "18  | encoder.transformer_layers.1.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "19  | encoder.transformer_layers.1.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "20  | encoder.transformer_layers.1.dropout1      | Dropout                         | 0      | train\n",
            "21  | encoder.transformer_layers.1.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "22  | encoder.transformer_layers.1.mlp           | Sequential                      | 4.7 M  | train\n",
            "23  | encoder.transformer_layers.1.mlp.0         | Linear                          | 2.4 M  | train\n",
            "24  | encoder.transformer_layers.1.mlp.1         | GELU                            | 0      | train\n",
            "25  | encoder.transformer_layers.1.mlp.2         | Dropout                         | 0      | train\n",
            "26  | encoder.transformer_layers.1.mlp.3         | Linear                          | 2.4 M  | train\n",
            "27  | encoder.transformer_layers.1.mlp.4         | Dropout                         | 0      | train\n",
            "28  | encoder.transformer_layers.2               | TransformerBlock                | 7.1 M  | train\n",
            "29  | encoder.transformer_layers.2.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "30  | encoder.transformer_layers.2.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "31  | encoder.transformer_layers.2.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "32  | encoder.transformer_layers.2.dropout1      | Dropout                         | 0      | train\n",
            "33  | encoder.transformer_layers.2.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "34  | encoder.transformer_layers.2.mlp           | Sequential                      | 4.7 M  | train\n",
            "35  | encoder.transformer_layers.2.mlp.0         | Linear                          | 2.4 M  | train\n",
            "36  | encoder.transformer_layers.2.mlp.1         | GELU                            | 0      | train\n",
            "37  | encoder.transformer_layers.2.mlp.2         | Dropout                         | 0      | train\n",
            "38  | encoder.transformer_layers.2.mlp.3         | Linear                          | 2.4 M  | train\n",
            "39  | encoder.transformer_layers.2.mlp.4         | Dropout                         | 0      | train\n",
            "40  | encoder.transformer_layers.3               | TransformerBlock                | 7.1 M  | train\n",
            "41  | encoder.transformer_layers.3.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "42  | encoder.transformer_layers.3.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "43  | encoder.transformer_layers.3.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "44  | encoder.transformer_layers.3.dropout1      | Dropout                         | 0      | train\n",
            "45  | encoder.transformer_layers.3.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "46  | encoder.transformer_layers.3.mlp           | Sequential                      | 4.7 M  | train\n",
            "47  | encoder.transformer_layers.3.mlp.0         | Linear                          | 2.4 M  | train\n",
            "48  | encoder.transformer_layers.3.mlp.1         | GELU                            | 0      | train\n",
            "49  | encoder.transformer_layers.3.mlp.2         | Dropout                         | 0      | train\n",
            "50  | encoder.transformer_layers.3.mlp.3         | Linear                          | 2.4 M  | train\n",
            "51  | encoder.transformer_layers.3.mlp.4         | Dropout                         | 0      | train\n",
            "52  | encoder.transformer_layers.4               | TransformerBlock                | 7.1 M  | train\n",
            "53  | encoder.transformer_layers.4.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "54  | encoder.transformer_layers.4.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "55  | encoder.transformer_layers.4.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "56  | encoder.transformer_layers.4.dropout1      | Dropout                         | 0      | train\n",
            "57  | encoder.transformer_layers.4.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "58  | encoder.transformer_layers.4.mlp           | Sequential                      | 4.7 M  | train\n",
            "59  | encoder.transformer_layers.4.mlp.0         | Linear                          | 2.4 M  | train\n",
            "60  | encoder.transformer_layers.4.mlp.1         | GELU                            | 0      | train\n",
            "61  | encoder.transformer_layers.4.mlp.2         | Dropout                         | 0      | train\n",
            "62  | encoder.transformer_layers.4.mlp.3         | Linear                          | 2.4 M  | train\n",
            "63  | encoder.transformer_layers.4.mlp.4         | Dropout                         | 0      | train\n",
            "64  | encoder.transformer_layers.5               | TransformerBlock                | 7.1 M  | train\n",
            "65  | encoder.transformer_layers.5.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "66  | encoder.transformer_layers.5.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "67  | encoder.transformer_layers.5.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "68  | encoder.transformer_layers.5.dropout1      | Dropout                         | 0      | train\n",
            "69  | encoder.transformer_layers.5.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "70  | encoder.transformer_layers.5.mlp           | Sequential                      | 4.7 M  | train\n",
            "71  | encoder.transformer_layers.5.mlp.0         | Linear                          | 2.4 M  | train\n",
            "72  | encoder.transformer_layers.5.mlp.1         | GELU                            | 0      | train\n",
            "73  | encoder.transformer_layers.5.mlp.2         | Dropout                         | 0      | train\n",
            "74  | encoder.transformer_layers.5.mlp.3         | Linear                          | 2.4 M  | train\n",
            "75  | encoder.transformer_layers.5.mlp.4         | Dropout                         | 0      | train\n",
            "76  | encoder.downsamples                        | ModuleList                      | 0      | train\n",
            "77  | encoder.norm                               | LayerNorm                       | 1.5 K  | train\n",
            "78  | decoder                                    | ViT_MAE_Decoder                 | 48.8 M | train\n",
            "79  | decoder.transformer_layers                 | ModuleList                      | 42.5 M | train\n",
            "80  | decoder.transformer_layers.0               | TransformerBlock                | 7.1 M  | train\n",
            "81  | decoder.transformer_layers.0.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "82  | decoder.transformer_layers.0.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "83  | decoder.transformer_layers.0.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "84  | decoder.transformer_layers.0.dropout1      | Dropout                         | 0      | train\n",
            "85  | decoder.transformer_layers.0.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "86  | decoder.transformer_layers.0.mlp           | Sequential                      | 4.7 M  | train\n",
            "87  | decoder.transformer_layers.0.mlp.0         | Linear                          | 2.4 M  | train\n",
            "88  | decoder.transformer_layers.0.mlp.1         | GELU                            | 0      | train\n",
            "89  | decoder.transformer_layers.0.mlp.2         | Dropout                         | 0      | train\n",
            "90  | decoder.transformer_layers.0.mlp.3         | Linear                          | 2.4 M  | train\n",
            "91  | decoder.transformer_layers.0.mlp.4         | Dropout                         | 0      | train\n",
            "92  | decoder.transformer_layers.1               | TransformerBlock                | 7.1 M  | train\n",
            "93  | decoder.transformer_layers.1.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "94  | decoder.transformer_layers.1.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "95  | decoder.transformer_layers.1.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "96  | decoder.transformer_layers.1.dropout1      | Dropout                         | 0      | train\n",
            "97  | decoder.transformer_layers.1.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "98  | decoder.transformer_layers.1.mlp           | Sequential                      | 4.7 M  | train\n",
            "99  | decoder.transformer_layers.1.mlp.0         | Linear                          | 2.4 M  | train\n",
            "100 | decoder.transformer_layers.1.mlp.1         | GELU                            | 0      | train\n",
            "101 | decoder.transformer_layers.1.mlp.2         | Dropout                         | 0      | train\n",
            "102 | decoder.transformer_layers.1.mlp.3         | Linear                          | 2.4 M  | train\n",
            "103 | decoder.transformer_layers.1.mlp.4         | Dropout                         | 0      | train\n",
            "104 | decoder.transformer_layers.2               | TransformerBlock                | 7.1 M  | train\n",
            "105 | decoder.transformer_layers.2.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "106 | decoder.transformer_layers.2.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "107 | decoder.transformer_layers.2.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "108 | decoder.transformer_layers.2.dropout1      | Dropout                         | 0      | train\n",
            "109 | decoder.transformer_layers.2.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "110 | decoder.transformer_layers.2.mlp           | Sequential                      | 4.7 M  | train\n",
            "111 | decoder.transformer_layers.2.mlp.0         | Linear                          | 2.4 M  | train\n",
            "112 | decoder.transformer_layers.2.mlp.1         | GELU                            | 0      | train\n",
            "113 | decoder.transformer_layers.2.mlp.2         | Dropout                         | 0      | train\n",
            "114 | decoder.transformer_layers.2.mlp.3         | Linear                          | 2.4 M  | train\n",
            "115 | decoder.transformer_layers.2.mlp.4         | Dropout                         | 0      | train\n",
            "116 | decoder.transformer_layers.3               | TransformerBlock                | 7.1 M  | train\n",
            "117 | decoder.transformer_layers.3.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "118 | decoder.transformer_layers.3.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "119 | decoder.transformer_layers.3.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "120 | decoder.transformer_layers.3.dropout1      | Dropout                         | 0      | train\n",
            "121 | decoder.transformer_layers.3.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "122 | decoder.transformer_layers.3.mlp           | Sequential                      | 4.7 M  | train\n",
            "123 | decoder.transformer_layers.3.mlp.0         | Linear                          | 2.4 M  | train\n",
            "124 | decoder.transformer_layers.3.mlp.1         | GELU                            | 0      | train\n",
            "125 | decoder.transformer_layers.3.mlp.2         | Dropout                         | 0      | train\n",
            "126 | decoder.transformer_layers.3.mlp.3         | Linear                          | 2.4 M  | train\n",
            "127 | decoder.transformer_layers.3.mlp.4         | Dropout                         | 0      | train\n",
            "128 | decoder.transformer_layers.4               | TransformerBlock                | 7.1 M  | train\n",
            "129 | decoder.transformer_layers.4.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "130 | decoder.transformer_layers.4.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "131 | decoder.transformer_layers.4.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "132 | decoder.transformer_layers.4.dropout1      | Dropout                         | 0      | train\n",
            "133 | decoder.transformer_layers.4.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "134 | decoder.transformer_layers.4.mlp           | Sequential                      | 4.7 M  | train\n",
            "135 | decoder.transformer_layers.4.mlp.0         | Linear                          | 2.4 M  | train\n",
            "136 | decoder.transformer_layers.4.mlp.1         | GELU                            | 0      | train\n",
            "137 | decoder.transformer_layers.4.mlp.2         | Dropout                         | 0      | train\n",
            "138 | decoder.transformer_layers.4.mlp.3         | Linear                          | 2.4 M  | train\n",
            "139 | decoder.transformer_layers.4.mlp.4         | Dropout                         | 0      | train\n",
            "140 | decoder.transformer_layers.5               | TransformerBlock                | 7.1 M  | train\n",
            "141 | decoder.transformer_layers.5.norm1         | LayerNorm                       | 1.5 K  | train\n",
            "142 | decoder.transformer_layers.5.attn          | MultiheadAttention              | 2.4 M  | train\n",
            "143 | decoder.transformer_layers.5.attn.out_proj | NonDynamicallyQuantizableLinear | 590 K  | train\n",
            "144 | decoder.transformer_layers.5.dropout1      | Dropout                         | 0      | train\n",
            "145 | decoder.transformer_layers.5.norm2         | LayerNorm                       | 1.5 K  | train\n",
            "146 | decoder.transformer_layers.5.mlp           | Sequential                      | 4.7 M  | train\n",
            "147 | decoder.transformer_layers.5.mlp.0         | Linear                          | 2.4 M  | train\n",
            "148 | decoder.transformer_layers.5.mlp.1         | GELU                            | 0      | train\n",
            "149 | decoder.transformer_layers.5.mlp.2         | Dropout                         | 0      | train\n",
            "150 | decoder.transformer_layers.5.mlp.3         | Linear                          | 2.4 M  | train\n",
            "151 | decoder.transformer_layers.5.mlp.4         | Dropout                         | 0      | train\n",
            "152 | decoder.norm                               | LayerNorm                       | 1.5 K  | train\n",
            "153 | decoder.output_layer                       | Linear                          | 6.3 M  | train\n",
            "154 | registration_head                          | RegistrationHead                | 14.2 M | train\n",
            "155 | registration_head.conv1                    | Conv3d                          | 10.6 M | train\n",
            "156 | registration_head.relu1                    | ReLU                            | 0      | train\n",
            "157 | registration_head.conv2                    | Conv3d                          | 3.5 M  | train\n",
            "158 | registration_head.relu2                    | ReLU                            | 0      | train\n",
            "159 | registration_head.conv3                    | Conv3d                          | 20.7 K | train\n",
            "160 | registration_head.tanh                     | Tanh                            | 0      | train\n",
            "161 | spatial_transformer                        | SpatialTransformer3D            | 0      | train\n",
            "162 | criterion                                  | VMAEProLoss                     | 0      | train\n",
            "163 | criterion.mse                              | MSELoss                         | 0      | train\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "111 M     Trainable params\n",
            "0         Non-trainable params\n",
            "111 M     Total params\n",
            "447.307   Total estimated model params size (MB)\n",
            "164       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db1256b7a7a641998d4904221666c845"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mon_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_lightning_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitoring_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Callback]{callback.state_key}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36mon_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_every_n_epochs\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_every_n_epochs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_topk_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_last_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_topk_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_none_monitor_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_none_monitor_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mprevious\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, trainer, filepath)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainer.save_checkpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, checkpoint, filepath, storage_options)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_global_zero\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_fabric/plugins/io/torch_io.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, checkpoint, path, storage_options)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0m_atomic_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_fabric/utilities/cloud_io.py\u001b[0m in \u001b[0;36m_atomic_save\u001b[0;34m(checkpoint, filepath)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransaction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytesbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2d054171780f>\u001b[0m in \u001b[0;36m<cell line: 674>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    672\u001b[0m )\n\u001b[1;32m    673\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# class VMAEProLightningModule(pl.LightningModule):\n",
        "#     def __init__(self, embed_dim=768, num_heads=12, num_layers=6, mlp_ratio=4., dropout=0.,\n",
        "#                  patch_size=(16,16,16), mask_ratio=0.75, base_channels=512, learning_rate=1e-4):\n",
        "#         super(VMAEProLightningModule, self).__init__()\n",
        "#         self.save_hyperparameters()\n",
        "\n",
        "#         # Initialize your model components\n",
        "#         self.patch_embedding = PatchEmbedding(in_channels=2, embed_dim=embed_dim, patch_size=patch_size)\n",
        "#         self.encoder = HierarchicalViTEncoder(\n",
        "#             embed_dim=embed_dim,\n",
        "#             num_heads=num_heads,\n",
        "#             num_layers=num_layers,\n",
        "#             mlp_ratio=mlp_ratio,\n",
        "#             dropout=dropout,\n",
        "#             num_stages=1,  # Set to 1 to avoid reshaping issues\n",
        "#             patch_size=patch_size\n",
        "#         )\n",
        "#         self.decoder = ViT_MAE_Decoder(\n",
        "#             embed_dim=embed_dim,\n",
        "#             num_heads=num_heads,\n",
        "#             num_layers=num_layers,\n",
        "#             mlp_ratio=mlp_ratio,\n",
        "#             dropout=dropout,\n",
        "#             patch_size=patch_size\n",
        "#         )\n",
        "#         self.registration_head = RegistrationHead(in_channels=embed_dim, base_channels=base_channels)\n",
        "#         self.spatial_transformer = SpatialTransformer3D()\n",
        "#         self.criterion = VMAEProLoss(\n",
        "#             lambda_recon=1.0,\n",
        "#             lambda_reg=1.0,\n",
        "#             lambda_smooth=0.1  # Adjust as needed\n",
        "#         )\n",
        "\n",
        "#     def forward(self, visible_patches, masked_indices, source, target):\n",
        "#         B, N_visible, C, P_D, P_H, P_W = visible_patches.shape\n",
        "\n",
        "#         # Reshape visible_patches to (B * N_visible, C, P_D, P_H, P_W)\n",
        "#         visible_patches = visible_patches.view(B * N_visible, C, P_D, P_H, P_W)\n",
        "\n",
        "#         # Patch Embedding\n",
        "#         embeddings = self.patch_embedding(visible_patches)  # (B * N_visible, embed_dim)\n",
        "\n",
        "#         # Reshape embeddings back to (B, N_visible, embed_dim) using einops\n",
        "#         embeddings = rearrange(embeddings, '(b n) e -> b n e', b=B, n=N_visible)  # (B, N_visible, embed_dim)\n",
        "\n",
        "#         # Encoder\n",
        "#         features, encoder_output = self.encoder(embeddings)  # features: list, encoder_output: (B * N_patches_new, embed_dim)\n",
        "\n",
        "#         # Reshape encoder_output to (B, N_patches_new, embed_dim) using einops\n",
        "#         N_patches_new = encoder_output.shape[0] // B\n",
        "#         encoder_output = rearrange(encoder_output, '(b p) e -> b p e', b=B, p=N_patches_new)  # (B, N_patches_new, embed_dim)\n",
        "\n",
        "#         # Decoder (Reconstruction)\n",
        "#         reconstructed = self.decoder(encoder_output, masked_indices)  # (B, N_masked, 2, P_D, P_H, P_W)\n",
        "\n",
        "#         # Registration Head\n",
        "#         # Reshape encoder_output to (B, embed_dim, D, H, W) using einops\n",
        "#         D = H = W = int(round(N_patches_new ** (1/3)))  # For N_patches_new=8, D=2\n",
        "#         assert D * H * W == N_patches_new, f\"D*H*W={D*H*W} does not equal N_patches_new={N_patches_new}\"\n",
        "\n",
        "#         # Corrected rearrange pattern\n",
        "#         registration_input = rearrange(encoder_output, 'b (d h w) e -> b e d h w', d=D, h=H, w=W)  # (B, embed_dim, D, H, W)\n",
        "#         phi = self.registration_head(registration_input)  # (B, 3, D, H, W)\n",
        "\n",
        "#         # Spatial Transformer\n",
        "#         warped_src = self.spatial_transformer(source, phi)  # (B, D_src, H_src, W_src)\n",
        "\n",
        "#         return reconstructed, warped_src, phi\n",
        "\n",
        "#     def training_step(self, batch, batch_idx):\n",
        "#         visible_patches = batch['visible_patches']  # (B, N_visible, 2, P_D, P_H, P_W)\n",
        "#         masked_patches = batch['masked_patches']    # (B, N_masked, 2, P_D, P_H, P_W)\n",
        "#         masked_indices = batch['masked_indices']    # (B, N_masked)\n",
        "#         source = batch['source']                    # (B, D_src, H_src, W_src)\n",
        "#         target = batch['target']                    # (B, D_src, H_src, W_src)\n",
        "\n",
        "#         # Forward pass\n",
        "#         reconstructed, warped_src, phi = self.forward(visible_patches, masked_indices, source, target)\n",
        "\n",
        "#         # Compute loss\n",
        "#         loss = self.criterion(reconstructed, masked_patches, warped_src, target, phi)\n",
        "\n",
        "#         # Log loss\n",
        "#         self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         return loss\n",
        "\n",
        "#     def configure_optimizers(self):\n",
        "#         optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
        "#         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "#         return [optimizer], [scheduler]\n",
        "# # Initialize the model\n",
        "# model = VMAEProLightningModule(\n",
        "#     embed_dim=768,\n",
        "#     num_heads=12,\n",
        "#     num_layers=6,\n",
        "#     mlp_ratio=4.,\n",
        "#     dropout=0.,\n",
        "#     patch_size=(16,16,16),\n",
        "#     mask_ratio=19/27,  # Approximately 0.7037 to get N_visible=8 patches\n",
        "#     base_channels=512,\n",
        "#     learning_rate=1e-4\n",
        "# )\n",
        "\n",
        "# # Define callbacks\n",
        "# model_summary_callback = ModelSummary(max_depth=-1)  # max_depth=-1 for full summary\n",
        "\n",
        "# # Initialize the Trainer\n",
        "# trainer = Trainer(\n",
        "#     max_epochs=10,\n",
        "#     callbacks=[model_summary_callback],\n",
        "#     accelerator='cpu',  # Change to 'gpu' if using CUDA\n",
        "#     devices=1,          # Number of GPUs or CPUs\n",
        "#     log_every_n_steps=1\n",
        "# )\n",
        "# # Start training\n",
        "# trainer.fit(model, dataloader)\n"
      ],
      "metadata": {
        "id": "HoWb3U7jcbth"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}