{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/Thesis/blob/main/h_vit_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t21Fu_cr5LUu",
        "outputId": "742ac99f-3bd1-44a3-bc68-97028998a2bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
            "Requirement already satisfied: lightning in /usr/local/lib/python3.10/dist-packages (2.5.0.post0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: monai in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.44)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.5.0)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.10.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.9)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.0.post0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.10/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.12)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.11)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install einops timm lightning wandb monai gitpython\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB_pza05MnPC"
      },
      "source": [
        "#Correct Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWQOkLxnODWy"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        # Create handlers\n",
        "        console_handler = logging.StreamHandler()\n",
        "\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        file_handler = logging.FileHandler(os.path.join(save_dir, \"logfile.log\"))\n",
        "\n",
        "        # Create formatters and add it to handlers\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        console_handler.setFormatter(formatter)\n",
        "        file_handler.setFormatter(formatter)\n",
        "\n",
        "        # Add handlers to the logger\n",
        "        self.logger.addHandler(console_handler)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def info(self, message):\n",
        "        self.logger.info(message)\n",
        "\n",
        "    def warning(self, message):\n",
        "        self.logger.warning(message)\n",
        "\n",
        "    def error(self, message):\n",
        "        self.logger.error(message)\n",
        "\n",
        "    def debug(self, message):\n",
        "        self.logger.debug(message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFOgJQp95FaV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import yaml\n",
        "import glob\n",
        "import pickle\n",
        "import random\n",
        "import logging\n",
        "from functools import reduce\n",
        "from typing import Tuple, Dict, Any, List, Set, Optional, Union, Callable, Type\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import lightning as L\n",
        "from lightning import LightningModule, Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "from einops import rearrange\n",
        "import timm\n",
        "import wandb\n",
        "import monai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzLtA0xC6BnX"
      },
      "outputs": [],
      "source": [
        "# Utility Functions and Classes\n",
        "\n",
        "def read_yaml_file(file_path):\n",
        "    \"\"\"\n",
        "    Reads a YAML file and returns the content as a dictionary.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        try:\n",
        "            content = yaml.safe_load(file)\n",
        "            return content\n",
        "        except yaml.YAMLError as e:\n",
        "            print(f\"Error reading YAML file: {e}\")\n",
        "            return None\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        # Create handlers\n",
        "        console_handler = logging.StreamHandler()\n",
        "\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        file_handler = logging.FileHandler(os.path.join(save_dir, \"logfile.log\"))\n",
        "\n",
        "        # Create formatters and add it to handlers\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        console_handler.setFormatter(formatter)\n",
        "        file_handler.setFormatter(formatter)\n",
        "\n",
        "        # Add handlers to the logger\n",
        "        self.logger.addHandler(console_handler)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def info(self, message):\n",
        "        self.logger.info(message)\n",
        "\n",
        "    def warning(self, message):\n",
        "        self.logger.warning(message)\n",
        "\n",
        "    def error(self, message):\n",
        "        self.logger.error(message)\n",
        "\n",
        "    def debug(self, message):\n",
        "        self.logger.debug(message)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def get_one_hot(inp_seg, num_labels):\n",
        "    B, C, H, W, D = inp_seg.shape\n",
        "    inp_onehot = nn.functional.one_hot(inp_seg.long(), num_classes=num_labels)\n",
        "    inp_onehot = inp_onehot.squeeze(dim=1)\n",
        "    inp_onehot = inp_onehot.permute(0, 4, 1, 2, 3).contiguous()\n",
        "    return inp_onehot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHOqFBEl6F4Q"
      },
      "outputs": [],
      "source": [
        "# Loss Functions\n",
        "\n",
        "class Grad3D(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    N-D gradient loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, penalty='l1', loss_mult=None):\n",
        "        super().__init__()\n",
        "        self.penalty = penalty\n",
        "        self.loss_mult = loss_mult\n",
        "\n",
        "    def forward(self, y_pred):\n",
        "        dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])\n",
        "        dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])\n",
        "        dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])\n",
        "\n",
        "        if self.penalty == 'l2':\n",
        "            dy = dy * dy\n",
        "            dx = dx * dx\n",
        "            dz = dz * dz\n",
        "\n",
        "        d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)\n",
        "        grad = d / 3.0\n",
        "\n",
        "        if self.loss_mult is not None:\n",
        "            grad *= self.loss_mult\n",
        "        return grad\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"Dice loss\"\"\"\n",
        "    def __init__(self, num_class=36):\n",
        "        super().__init__()\n",
        "        self.num_class = num_class\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_true = nn.functional.one_hot(y_true, num_classes=self.num_class)\n",
        "        y_true = torch.squeeze(y_true, 1)\n",
        "        y_true = y_true.permute(0, 4, 1, 2, 3).contiguous()\n",
        "        intersection = y_pred * y_true\n",
        "        intersection = intersection.sum(dim=[2, 3, 4])\n",
        "        union = torch.pow(y_pred, 2).sum(dim=[2, 3, 4]) + torch.pow(y_true, 2).sum(dim=[2, 3, 4])\n",
        "        dsc = (2.*intersection) / (union + 1e-5)\n",
        "        dsc_loss = (1-torch.mean(dsc))\n",
        "        return dsc_loss\n",
        "\n",
        "def DiceScore(y_pred, y_true, num_class):\n",
        "    y_true = nn.functional.one_hot(y_true, num_classes=num_class)\n",
        "    y_true = torch.squeeze(y_true, 1)\n",
        "    y_true = y_true.permute(0, 4, 1, 2, 3).contiguous()\n",
        "    intersection = y_pred * y_true\n",
        "    intersection = intersection.sum(dim=[2, 3, 4])\n",
        "    union = torch.pow(y_pred, 2).sum(dim=[2, 3, 4]) + torch.pow(y_true, 2).sum(dim=[2, 3, 4])\n",
        "    dsc = (2.*intersection) / (union + 1e-5)\n",
        "    return dsc\n",
        "\n",
        "loss_functions = {\n",
        "    \"mse\": nn.MSELoss(),\n",
        "    \"dice\": DiceLoss(),\n",
        "    \"grad\": Grad3D(penalty='l2')\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6W2tdKm6IM-"
      },
      "outputs": [],
      "source": [
        "# Data Loading\n",
        "\n",
        "class OASIS_Dataset(Dataset):\n",
        "    def __init__(self, input_dim, data_path, num_steps=1000, is_pair: bool = False, ext=\"pkl\"):\n",
        "        self.paths = glob.glob(os.path.join(data_path, f\"*.{ext}\"))\n",
        "        self.num_steps = num_steps\n",
        "        self.input_dim = input_dim\n",
        "        self.is_pair = is_pair\n",
        "\n",
        "        self.transforms_mask = monai.transforms.Compose([\n",
        "            monai.transforms.Resize(spatial_size=input_dim, mode=\"nearest\")\n",
        "        ])\n",
        "        self.transforms_image = monai.transforms.Compose([\n",
        "            monai.transforms.Resize(spatial_size=input_dim)\n",
        "        ])\n",
        "\n",
        "    def _pkload(self, filename: str) -> tuple:\n",
        "        \"\"\"\n",
        "        Load a pickled file and return its contents.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(filename, 'rb') as file:\n",
        "                return pickle.load(file)\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"The file {filename} was not found.\")\n",
        "        except pickle.UnpicklingError:\n",
        "            raise pickle.UnpicklingError(f\"Error unpickling the file {filename}.\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_pair:\n",
        "            src, tgt, src_lbl, tgt_lbl = self._pkload(self.paths[index])\n",
        "        else:\n",
        "            selected_items = random.sample(list(self.paths), 2)\n",
        "            src, src_lbl = self._pkload(selected_items[0])\n",
        "            tgt, tgt_lbl = self._pkload(selected_items[1])\n",
        "\n",
        "        src = torch.from_numpy(src).float().unsqueeze(0)\n",
        "        src_lbl = torch.from_numpy(src_lbl).long().unsqueeze(0)\n",
        "        tgt = torch.from_numpy(tgt).float().unsqueeze(0)\n",
        "        tgt_lbl = torch.from_numpy(tgt_lbl).long().unsqueeze(0)\n",
        "\n",
        "        src = self.transforms_image(src)\n",
        "        tgt = self.transforms_image(tgt)\n",
        "        src_lbl = self.transforms_mask(src_lbl)\n",
        "        tgt_lbl = self.transforms_mask(tgt_lbl)\n",
        "\n",
        "        return src, tgt, src_lbl, tgt_lbl\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_steps if not self.is_pair else len(self.paths)\n",
        "\n",
        "def get_dataloader(data_path, input_dim, batch_size, shuffle: bool = True, is_pair: bool = False):\n",
        "    ds = OASIS_Dataset(input_dim=input_dim, data_path=data_path, is_pair=is_pair)\n",
        "    dataloader = DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=4)\n",
        "    return dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5KUXjQG6J5E"
      },
      "outputs": [],
      "source": [
        "# Model Components: Blocks and Transformer Layers\n",
        "\n",
        "# Drop Path (Stochastic Depth) Implementation\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
        "    if keep_prob > 0.0 and scale_by_keep:\n",
        "        random_tensor.div_(keep_prob)\n",
        "    return x * random_tensor\n",
        "\n",
        "class timm_DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
        "        super(timm_DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.scale_by_keep = scale_by_keep\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
        "\n",
        "# Truncated Normal Initialization\n",
        "def _trunc_normal_(tensor, mean, std, a, b):\n",
        "    def norm_cdf(x):\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    l = norm_cdf((a - mean) / std)\n",
        "    u = norm_cdf((b - mean) / std)\n",
        "\n",
        "    tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "    tensor.erfinv_()\n",
        "    tensor.mul_(std * math.sqrt(2.))\n",
        "    tensor.add_(mean)\n",
        "    tensor.clamp_(min=a, max=b)\n",
        "    return tensor\n",
        "\n",
        "def timm_trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    with torch.no_grad():\n",
        "        return _trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "# Convolutional Block with ReLU and Normalization\n",
        "class Conv3dReLU(nn.Sequential):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            padding=0,\n",
        "            stride=1,\n",
        "            use_batchnorm=True,\n",
        "    ):\n",
        "        conv = nn.Conv3d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            bias=False,\n",
        "        )\n",
        "        relu = nn.LeakyReLU(inplace=True)\n",
        "        if use_batchnorm:\n",
        "            nm = nn.BatchNorm3d(out_channels)\n",
        "        else:\n",
        "            nm = nn.InstanceNorm3d(out_channels)\n",
        "        super(Conv3dReLU, self).__init__(conv, nm, relu)\n",
        "\n",
        "# Normalization and Activation Getters\n",
        "def get_norm(name, **kwargs):\n",
        "    if name.lower() == 'batchnorm2d'.lower():\n",
        "        BatchNorm = getattr(nn, f'BatchNorm{ndims}d')\n",
        "        return BatchNorm(**kwargs)\n",
        "    elif name.lower() == 'instance':\n",
        "        InstanceNorm = getattr(nn, f'InstanceNorm{ndims}d')\n",
        "        return InstanceNorm(**kwargs)\n",
        "    elif name.lower() == 'none'.lower():\n",
        "        return nn.Identity()\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Normalization '{name}' not implemented.\")\n",
        "\n",
        "def get_activation(name, **kwargs):\n",
        "    if name.lower() == 'relu':\n",
        "        return nn.ReLU()\n",
        "    elif name.lower() == 'gelu':\n",
        "        return nn.GELU()\n",
        "    elif name.lower() == 'none':\n",
        "        return nn.Identity()\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Activation '{name}' not implemented.\")\n",
        "\n",
        "def prod_func(Vec):\n",
        "    return reduce(lambda x, y: x*y, Vec)\n",
        "\n",
        "def downsampler_fn(data, out_size):\n",
        "    \"\"\"\n",
        "    Trilinear downsampling\n",
        "    \"\"\"\n",
        "    return nn.functional.interpolate(data,\n",
        "                                     size=out_size,\n",
        "                                     mode='trilinear',\n",
        "                                     align_corners=False)\n",
        "\n",
        "# Spatial Transformer\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    N-D Spatial Transformer\n",
        "    Obtained from https://github.com/voxelmorph/voxelmorph\n",
        "    \"\"\"\n",
        "    def __init__(self, size, mode='bilinear'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        # create sampling grid\n",
        "        vectors = [torch.arange(0, s) for s in size]\n",
        "        grids = torch.meshgrid(vectors)\n",
        "        grid = torch.stack(grids)\n",
        "        grid = torch.unsqueeze(grid, 0)\n",
        "        grid = grid.type(torch.FloatTensor)\n",
        "\n",
        "        # Register the grid as a buffer\n",
        "        self.register_buffer('grid', grid)\n",
        "\n",
        "    def forward(self, src, flow):\n",
        "        # new locations\n",
        "        new_locs = self.grid + flow\n",
        "        shape = flow.shape[2:]\n",
        "\n",
        "        # normalize grid values to [-1, 1]\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n",
        "\n",
        "        # move channels dim to last position and reverse if necessary\n",
        "        if len(shape) == 2:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 1)\n",
        "            new_locs = new_locs[..., [1, 0]]\n",
        "        elif len(shape) == 3:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 4, 1)\n",
        "            new_locs = new_locs[..., [2, 1, 0]]\n",
        "\n",
        "        return F.grid_sample(src, new_locs, align_corners=False, mode=self.mode)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwIqeAy0C_fL"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL9NTPkb6NC5"
      },
      "outputs": [],
      "source": [
        "# Model Components: HViT and Related Classes\n",
        "from torch import Tensor\n",
        "\n",
        "ndims = 3  # Spatial dimensions\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention module for hierarchical vision transformer.\n",
        "    Implements both local and global attention mechanisms.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        patch_size: Union[int, List[int]],\n",
        "        attention_type: str = \"local\",\n",
        "        qkv_bias: bool = True,\n",
        "        qk_scale: Optional[float] = None,\n",
        "        attn_drop: float = 0.,\n",
        "        proj_drop: float = 0.\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.patch_size = [patch_size] * ndims if isinstance(patch_size, int) else patch_size\n",
        "        self.attention_type = attention_type\n",
        "\n",
        "        assert dim % num_heads == 0, \"Dimension must be divisible by number of heads\"\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or self.head_dim ** -0.5\n",
        "\n",
        "        if self.attention_type == \"local\":\n",
        "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        elif self.attention_type == \"global\":\n",
        "            self.qkv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Attention type '{self.attention_type}' not implemented.\")\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x: Tensor, q_ms: Optional[Tensor] = None) -> Tensor:\n",
        "        B_, N, C = x.size()\n",
        "\n",
        "        if self.attention_type == \"local\":\n",
        "            qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "            q = q * self.scale\n",
        "        else:\n",
        "            B = q_ms.size()[0]\n",
        "            kv = self.qkv(x).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "            k, v = kv[0], kv[1]\n",
        "            q = self._process_global_query(q_ms, B, B_, N, C)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _process_global_query(self, q_ms: Tensor, B: int, B_: int, N: int, C: int) -> Tensor:\n",
        "        q_tmp = q_ms.reshape(B, self.num_heads, N, C // self.num_heads)\n",
        "        div_, rem_ = divmod(B_, B)\n",
        "        q_tmp = q_tmp.repeat(div_, 1, 1, 1)\n",
        "        q_tmp = q_tmp.reshape(B * div_, self.num_heads, N, C // self.num_heads)\n",
        "\n",
        "        q = torch.zeros(B_, self.num_heads, N, C // self.num_heads, device=q_ms.device)\n",
        "        q[:B*div_] = q_tmp\n",
        "        if rem_ > 0:\n",
        "            q[B*div_:] = q_tmp[:rem_]\n",
        "\n",
        "        return q * self.scale\n",
        "\n",
        "def get_patches(x: Tensor, patch_size: int) -> Tuple[Tensor, int, int, int]:\n",
        "    \"\"\"\n",
        "    Divide the input tensor into patches and reshape them for processing.\n",
        "    \"\"\"\n",
        "    B, H, W, D, C = x.size()\n",
        "    nh = H / patch_size\n",
        "    nw = W / patch_size\n",
        "    nd = D / patch_size\n",
        "\n",
        "    down_req = (nh - int(nh)) + (nw - int(nw)) + (nd - int(nd))\n",
        "    if down_req > 0:\n",
        "        new_dims = [int(nh) * patch_size, int(nw) * patch_size, int(nd) * patch_size]\n",
        "        x = downsampler_fn(x.permute(0, 4, 1, 2, 3), new_dims).permute(0, 2, 3, 4, 1)\n",
        "        B, H, W, D, C = x.size()\n",
        "\n",
        "    x = x.view(B, H // patch_size, patch_size,\n",
        "               W // patch_size, patch_size,\n",
        "               D // patch_size, patch_size,\n",
        "               C)\n",
        "\n",
        "    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, patch_size, patch_size, patch_size, C)\n",
        "\n",
        "    return windows, H, W, D\n",
        "\n",
        "def get_image(windows: Tensor, patch_size: int, Hatt: int, Watt: int, Datt: int, H: int, W: int, D: int) -> Tensor:\n",
        "    \"\"\"\n",
        "    Reconstruct the image from windows (patches).\n",
        "    \"\"\"\n",
        "    B = int(windows.size(0) / ((Hatt * Watt * Datt) // (patch_size ** 3)))\n",
        "\n",
        "    x = windows.view(B,\n",
        "                    Hatt // patch_size,\n",
        "                    Watt // patch_size,\n",
        "                    Datt // patch_size,\n",
        "                    patch_size, patch_size, patch_size, -1)\n",
        "    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, Hatt, Watt, Datt, -1)\n",
        "\n",
        "    if H != Hatt or W != Watt or D != Datt:\n",
        "        x = downsampler_fn(x.permute(0, 4, 1, 2, 3), [H, W, D]).permute(0, 2, 3, 4, 1)\n",
        "    return x\n",
        "\n",
        "class ViTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer Block.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 embed_dim: int,\n",
        "                 input_dims: List[int],\n",
        "                 num_heads: int,\n",
        "                 mlp_type: str,\n",
        "                 patch_size: int,\n",
        "                 mlp_ratio: float,\n",
        "                 qkv_bias: bool,\n",
        "                 qk_scale: Optional[float],\n",
        "                 drop: float,\n",
        "                 attn_drop: float,\n",
        "                 drop_path: float,\n",
        "                 act_layer: str,\n",
        "                 attention_type: str,\n",
        "                 norm_layer: Callable[..., nn.Module],\n",
        "                 layer_scale: Optional[float]):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_windows = prod_func([d // patch_size for d in input_dims])\n",
        "\n",
        "        self.norm1 = norm_layer(embed_dim)\n",
        "        self.attn = Attention(\n",
        "            dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            patch_size=patch_size,\n",
        "            attention_type=attention_type,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop,\n",
        "        )\n",
        "\n",
        "        self.drop_path = timm_DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(embed_dim)\n",
        "\n",
        "        self.mlp = Conv3dReLU(\n",
        "            in_channels=embed_dim,\n",
        "            out_channels=int(embed_dim * mlp_ratio),\n",
        "            kernel_size=3,  # Assuming kernel_size=3 for MLP\n",
        "            padding=1,\n",
        "            stride=1,\n",
        "            use_batchnorm=True,\n",
        "        )\n",
        "\n",
        "        # Add projection layer to ensure output channels match embed_dim\n",
        "        self.proj = nn.Conv3d(\n",
        "            in_channels=int(embed_dim * mlp_ratio),\n",
        "            out_channels=embed_dim,\n",
        "            kernel_size=1\n",
        "        )\n",
        "        self.layer_scale = layer_scale is not None and isinstance(layer_scale, (int, float))\n",
        "        if self.layer_scale:\n",
        "            self.gamma1 = nn.Parameter(layer_scale * torch.ones(embed_dim), requires_grad=True)\n",
        "            self.gamma2 = nn.Parameter(layer_scale * torch.ones(embed_dim), requires_grad=True)\n",
        "        else:\n",
        "            self.gamma1 = 1.0\n",
        "            self.gamma2 = 1.0\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor, q_ms: Optional[Tensor]) -> Tensor:\n",
        "        B, H, W, D, C = x.size()\n",
        "        shortcut = x\n",
        "\n",
        "        # Normalize and compute attention\n",
        "        x = self.norm1(x)\n",
        "        x_windows, Hatt, Watt, Datt = get_patches(x, self.patch_size)\n",
        "        x_windows = x_windows.view(-1, self.patch_size ** 3, C)\n",
        "\n",
        "        # Compute attention and reconstruct image\n",
        "        attn_windows = self.attn(x_windows, q_ms)\n",
        "        x = get_image(attn_windows, self.patch_size, Hatt, Watt, Datt, H, W, D)\n",
        "\n",
        "        # Apply shortcut and drop path\n",
        "        x = shortcut + self.drop_path(self.gamma1 * x)\n",
        "\n",
        "        # Apply MLP\n",
        "        x_mlp_input = self.norm2(x).permute(0, 4, 1, 2, 3)\n",
        "        print(f\"MLP input shape after permute: {x_mlp_input.shape}\")  # Debug print\n",
        "        x_mlp_output = self.mlp(x_mlp_input)\n",
        "        x_mlp_output = self.proj(x_mlp_output).permute(0, 2, 3, 4, 1)\n",
        "\n",
        "        # Add MLP output with drop path and gamma scaling\n",
        "        x = x + self.drop_path(self.gamma2 * x_mlp_output)\n",
        "        return x\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch Embedding layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chans: int = 3, out_chans: int = 32,\n",
        "                 drop_rate: float = 0,\n",
        "                 kernel_size: int = 3,\n",
        "                 stride: int = 1, padding: int = 1,\n",
        "                 dilation: int = 1, groups: int = 1, bias: bool = False) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        Convnd = getattr(nn, f\"Conv{ndims}d\")\n",
        "        self.proj = Convnd(in_channels=in_chans, out_channels=out_chans,\n",
        "                           kernel_size=kernel_size,\n",
        "                           stride=stride, padding=padding,\n",
        "                           dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "        self.drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.drop(self.proj(x))\n",
        "        return x\n",
        "\n",
        "class ViTLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer Layer.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_type: str,\n",
        "        dim: int,\n",
        "        dim_out: int,\n",
        "        depth: int,\n",
        "        input_dims: List[int],\n",
        "        num_heads: int,\n",
        "        patch_size: int,\n",
        "        mlp_type: str,\n",
        "        mlp_ratio: float,\n",
        "        qkv_bias: bool,\n",
        "        qk_scale: Optional[float],\n",
        "        drop: float,\n",
        "        attn_drop: float,\n",
        "        drop_path: Union[float, List[float]],\n",
        "        norm_layer: Callable[..., nn.Module],\n",
        "        norm_type: str,\n",
        "        layer_scale: Optional[float],\n",
        "        act_layer: str\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = dim\n",
        "        self.input_dims = input_dims\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ViTBlock(\n",
        "                embed_dim=dim,\n",
        "                input_dims=input_dims,\n",
        "                num_heads=num_heads,\n",
        "                mlp_type=mlp_type,\n",
        "                patch_size=patch_size,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                attention_type=attention_type,\n",
        "                drop=drop,\n",
        "                attn_drop=attn_drop,\n",
        "                drop_path=drop_path[k] if isinstance(drop_path, list) else drop_path,\n",
        "                act_layer=act_layer,\n",
        "                norm_layer=norm_layer,\n",
        "                layer_scale=layer_scale\n",
        "            )\n",
        "            for k in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, inp: Tensor, q_ms: Optional[Tensor], CONCAT_ok: bool) -> Tensor:\n",
        "        x = inp.clone()\n",
        "        x = rearrange(x, 'b c h w d -> b h w d c')\n",
        "\n",
        "        if q_ms is not None:\n",
        "            q_ms = rearrange(q_ms, 'b c h w d -> b h w d c')\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            if q_ms is None:\n",
        "                x = blk(x, None)\n",
        "            else:\n",
        "                q_ms_patches, _, _, _ = get_patches(q_ms, self.patch_size)\n",
        "                q_ms_patches = q_ms_patches.view(-1, self.patch_size ** ndims, x.size()[-1])\n",
        "                x = blk(x, q_ms_patches)\n",
        "\n",
        "        x = rearrange(x, 'b h w d c -> b c h w d')\n",
        "\n",
        "        if CONCAT_ok:\n",
        "            x = torch.cat((inp, x), dim=-1)\n",
        "        else:\n",
        "            x = inp + x\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) module for hierarchical feature processing.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 PYR_SCALES=None,\n",
        "                 feats_num=None,\n",
        "                 hid_dim=None,\n",
        "                 depths=None,\n",
        "                 patch_size=None,\n",
        "                 mlp_ratio=None,\n",
        "                 num_heads=None,\n",
        "                 mlp_type=None,\n",
        "                 norm_type=None,\n",
        "                 act_layer=None,\n",
        "                 drop_path_rate: float = 0.2,\n",
        "                 qkv_bias: bool = True,\n",
        "                 qk_scale: bool = None,\n",
        "                 drop_rate: float = 0.,\n",
        "                 attn_drop_rate: float = 0.,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 layer_scale=None,\n",
        "                 img_size=None,\n",
        "                 NUM_CROSS_ATT=-1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Determine the number of levels for processing\n",
        "        num_levels = len(feats_num)\n",
        "        num_levels = min(num_levels, NUM_CROSS_ATT) if NUM_CROSS_ATT > 0 else num_levels\n",
        "        # WO_SELF_ATT is defined globally; set to False as per code\n",
        "        global WO_SELF_ATT\n",
        "        if WO_SELF_ATT:\n",
        "            num_levels -= 1\n",
        "\n",
        "        # Ensure patch_size is a list\n",
        "        patch_size = patch_size if isinstance(patch_size, list) else [patch_size for _ in range(num_levels)]\n",
        "        hwd = img_size[-1]\n",
        "\n",
        "        # Create patch embedding layers\n",
        "        self.patch_embed = nn.ModuleList([\n",
        "            PatchEmbed(\n",
        "                in_chans=feats_num[i],\n",
        "                out_chans=hid_dim,\n",
        "                drop_rate=drop_rate\n",
        "            ) for i in range(num_levels)\n",
        "        ])\n",
        "\n",
        "        # Generate drop path rate for each layer\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        # Create ViT layers\n",
        "        self.levels = nn.ModuleList()\n",
        "        for i in range(num_levels):\n",
        "            level = ViTLayer(\n",
        "                dim=hid_dim,\n",
        "                dim_out=hid_dim,\n",
        "                depth=depths[i],\n",
        "                num_heads=num_heads[i],\n",
        "                patch_size=patch_size[i],\n",
        "                mlp_type=mlp_type,\n",
        "                attention_type=\"local\" if i == 0 else \"global\",\n",
        "                drop_path=dpr[sum(depths[:i]):sum(depths[:i+1])],\n",
        "                input_dims=img_size[i],\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                norm_layer=norm_layer,\n",
        "                layer_scale=layer_scale,\n",
        "                norm_type=norm_type,\n",
        "                act_layer=act_layer\n",
        "            )\n",
        "            self.levels.append(level)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        \"\"\"Initialize the weights of the module.\"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            timm_trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        \"\"\"Return keywords for no weight decay.\"\"\"\n",
        "        return {'rpb'}\n",
        "\n",
        "    def forward(self, KQs, CONCAT_ok: bool = False):\n",
        "        \"\"\"\n",
        "        Forward pass of the ViT module.\n",
        "        \"\"\"\n",
        "        for i, (patch_embed_, level) in enumerate(zip(self.patch_embed, self.levels)):\n",
        "            if i == 0:\n",
        "                # First level: process input without cross-attention\n",
        "                Q = patch_embed_(KQs[i])\n",
        "                x = level(Q, None, CONCAT_ok=CONCAT_ok)\n",
        "                Q = patch_embed_(x)\n",
        "            else:\n",
        "                # Subsequent levels: process with cross-attention\n",
        "                K = patch_embed_(KQs[i])\n",
        "                x = level(Q, K, CONCAT_ok=CONCAT_ok)\n",
        "                Q = x.clone()\n",
        "\n",
        "        return x\n",
        "\n",
        "class EncoderCnnBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional block for the encoder part of the network.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        stride,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "        affine=True,\n",
        "        eps=1e-05\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # First convolutional block\n",
        "        conv_block_1 = [\n",
        "            nn.Conv3d(\n",
        "                in_channels=in_channels, out_channels=out_channels,\n",
        "                kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                bias=bias\n",
        "            ),\n",
        "            nn.InstanceNorm3d(num_features=out_channels, affine=affine, eps=eps),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Second convolutional block\n",
        "        conv_block_2 = [\n",
        "            nn.Conv3d(\n",
        "                in_channels=out_channels, out_channels=out_channels,\n",
        "                kernel_size=kernel_size, stride=1, padding=padding,\n",
        "                bias=bias\n",
        "            ),\n",
        "            nn.InstanceNorm3d(num_features=out_channels, affine=affine, eps=eps),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Combine both blocks\n",
        "        self._block = nn.Sequential(\n",
        "            *conv_block_1,\n",
        "            *conv_block_2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of the EncoderCnnBlock.\"\"\"\n",
        "        return self._block(x)\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) module for hierarchical feature processing.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 PYR_SCALES=None,\n",
        "                 feats_num=None,\n",
        "                 hid_dim=None,\n",
        "                 depths=None,\n",
        "                 patch_size=None,\n",
        "                 mlp_ratio=None,\n",
        "                 num_heads=None,\n",
        "                 mlp_type=None,\n",
        "                 norm_type=None,\n",
        "                 act_layer=None,\n",
        "                 drop_path_rate: float = 0.2,\n",
        "                 qkv_bias: bool = True,\n",
        "                 qk_scale: bool = None,\n",
        "                 drop_rate: float = 0.,\n",
        "                 attn_drop_rate: float = 0.,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 layer_scale=None,\n",
        "                 img_size=None,\n",
        "                 WO_SELF_ATT=False,  # Added WO_SELF_ATT parameter\n",
        "\n",
        "                 NUM_CROSS_ATT=-1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Determine the number of levels for processing\n",
        "        num_levels = len(feats_num)\n",
        "        num_levels = min(num_levels, NUM_CROSS_ATT) if NUM_CROSS_ATT > 0 else num_levels\n",
        "        if WO_SELF_ATT:\n",
        "            num_levels -= 1\n",
        "\n",
        "        # Ensure patch_size is a list\n",
        "        patch_size = patch_size if isinstance(patch_size, list) else [patch_size for _ in range(num_levels)]\n",
        "        hwd = img_size[-1]\n",
        "\n",
        "        # Create patch embedding layers\n",
        "        self.patch_embed = nn.ModuleList([\n",
        "            PatchEmbed(\n",
        "                in_chans=feats_num[i],\n",
        "                out_chans=hid_dim,\n",
        "                drop_rate=drop_rate\n",
        "            ) for i in range(num_levels)\n",
        "        ])\n",
        "\n",
        "        # Generate drop path rate for each layer\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        # Create ViT layers\n",
        "        self.levels = nn.ModuleList()\n",
        "        for i in range(num_levels):\n",
        "            level = ViTLayer(\n",
        "                dim=hid_dim,\n",
        "                dim_out=hid_dim,\n",
        "                depth=depths[i],\n",
        "                num_heads=num_heads[i],\n",
        "                patch_size=patch_size[i],\n",
        "                mlp_type=mlp_type,\n",
        "                attention_type=\"local\" if i == 0 else \"global\",\n",
        "                drop_path=dpr[sum(depths[:i]):sum(depths[:i+1])],\n",
        "                input_dims=img_size[i],\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                norm_layer=norm_layer,\n",
        "                layer_scale=layer_scale,\n",
        "                norm_type=norm_type,\n",
        "                act_layer=act_layer\n",
        "            )\n",
        "            self.levels.append(level)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        \"\"\"Initialize the weights of the module.\"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            timm_trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        \"\"\"Return keywords for no weight decay.\"\"\"\n",
        "        return {'rpb'}\n",
        "\n",
        "    def forward(self, KQs, CONCAT_ok: bool = False):\n",
        "        \"\"\"\n",
        "        Forward pass of the ViT module.\n",
        "        \"\"\"\n",
        "        for i, (patch_embed_, level) in enumerate(zip(self.patch_embed, self.levels)):\n",
        "            if i == 0:\n",
        "                # First level: process input without cross-attention\n",
        "                Q = patch_embed_(KQs[i])\n",
        "                x = level(Q, None, CONCAT_ok=CONCAT_ok)\n",
        "                Q = patch_embed_(x)\n",
        "            else:\n",
        "                # Subsequent levels: process with cross-attention\n",
        "                K = patch_embed_(KQs[i])\n",
        "                x = level(Q, K, CONCAT_ok=CONCAT_ok)\n",
        "                Q = x.clone()\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder module for the hierarchical vision transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "        self._num_stages: int = config['num_stages']\n",
        "        self.use_seg: bool = config['use_seg_loss']\n",
        "\n",
        "        # Determine channels of encoder feature maps\n",
        "        encoder_out_channels: torch.Tensor = torch.tensor([config['start_channels'] * 2**stage for stage in range(self._num_stages)])\n",
        "        self._NUM_CROSS_ATT=config.get('NUM_CROSS_ATT', -1)\n",
        "        # Estimate required stages\n",
        "        required_stages: Set[int] = set(int(fmap[-1]) for fmap in config['out_fmaps'])\n",
        "        self._required_stages: Set[int] = required_stages\n",
        "\n",
        "        earliest_required_stage: int = min(required_stages)\n",
        "\n",
        "        # Lateral connections\n",
        "        lateral_in_channels: torch.Tensor = encoder_out_channels[earliest_required_stage:]\n",
        "        lateral_out_channels: torch.Tensor = lateral_in_channels.clip(max=config['fpn_channels'])\n",
        "\n",
        "        self._lateral: nn.ModuleList = nn.ModuleList([\n",
        "            nn.Conv3d(in_channels=in_ch, out_channels=out_ch, kernel_size=1)\n",
        "            for in_ch, out_ch in zip(lateral_in_channels, lateral_out_channels)\n",
        "        ])\n",
        "        self._lateral_levels: int = len(self._lateral)\n",
        "\n",
        "        # Output layers\n",
        "        out_in_channels: List[int] = [lateral_out_channels[-self._num_stages + required_stage].item() for required_stage in required_stages]\n",
        "        out_out_channels: List[int] = [int(config['fpn_channels'])] * len(out_in_channels)\n",
        "        out_out_channels[0] = int(config['fpn_channels'])\n",
        "\n",
        "        self._out: nn.ModuleList = nn.ModuleList([\n",
        "            nn.Conv3d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1)\n",
        "            for in_ch, out_ch in zip(out_in_channels, out_out_channels)\n",
        "        ])\n",
        "\n",
        "        # Upsampling layers\n",
        "        self._up: nn.ModuleList = nn.ModuleList([\n",
        "            nn.ConvTranspose3d(\n",
        "                in_channels=list(reversed(lateral_out_channels))[level],\n",
        "                out_channels=list(reversed(lateral_out_channels))[level+1],\n",
        "                kernel_size=list(reversed(config['strides']))[level],\n",
        "                stride=list(reversed(config['strides']))[level]\n",
        "            )\n",
        "            for level in range(len(lateral_out_channels)-1)\n",
        "        ])\n",
        "\n",
        "        # Multi-scale attention\n",
        "        self.hierarchical_dec: nn.ModuleList = self._create_hierarchical_layers(config, out_out_channels)\n",
        "\n",
        "        if self.use_seg:\n",
        "            self._seg_head: nn.ModuleList = nn.ModuleList([\n",
        "                nn.Conv3d(out_ch, config['num_organs'] + 1, kernel_size=1, stride=1)\n",
        "                for out_ch in out_out_channels\n",
        "            ])\n",
        "    def _create_hierarchical_layers(self, config: Dict[str, Any], out_out_channels: List[int]) -> nn.ModuleList:\n",
        "        \"\"\"Create hierarchical layers for multi-scale attention.\"\"\"\n",
        "        out: nn.ModuleList = nn.ModuleList()\n",
        "        img_size: List[List[int]] = []\n",
        "        feats_num: List[int] = []\n",
        "\n",
        "        num_levels = len(out_out_channels)  # Ensure `num_levels` matches the length of `out_out_channels`\n",
        "\n",
        "        for k, out_ch in enumerate(out_out_channels):\n",
        "            img_size.append([int(item / (2 ** (self._num_stages - k - 1))) for item in config['data_size']])\n",
        "            feats_num.append(out_ch)\n",
        "            n: int = len(feats_num)\n",
        "\n",
        "            if k == 0:\n",
        "                out.append(nn.Identity())\n",
        "            else:\n",
        "                # Ensure depths and num_heads have enough entries\n",
        "                depths = config.get('depths', [1] * num_levels)\n",
        "                num_heads = config.get('num_heads', [32] * num_levels)\n",
        "\n",
        "                # Use k or level-based indexing\n",
        "                out.append(\n",
        "                    ViT(\n",
        "                        NUM_CROSS_ATT=config.get('NUM_CROSS_ATT', self._NUM_CROSS_ATT),\n",
        "                        PYR_SCALES=[1.],\n",
        "                        feats_num=feats_num,\n",
        "                        hid_dim=int(config.get('fpn_channels', 64)),\n",
        "                        depths=depths,  # Use the list directly\n",
        "                        patch_size=config.get('patch_size', [2] * n),  # Fixed line\n",
        "                        mlp_ratio=int(config.get('mlp_ratio', 2)),\n",
        "                        num_heads=num_heads,  # Use the list directly\n",
        "                        mlp_type='basic',\n",
        "                        norm_type='BatchNorm2d',\n",
        "                        act_layer='gelu',\n",
        "                        drop_path_rate=config.get('drop_path_rate', 0.2),\n",
        "                        qkv_bias=config.get('qkv_bias', True),\n",
        "                        qk_scale=None,\n",
        "                        drop_rate=config.get('drop_rate', 0.),\n",
        "                        attn_drop_rate=config.get('attn_drop_rate', 0.),\n",
        "                        norm_layer=nn.LayerNorm,\n",
        "                        layer_scale=1e-5,\n",
        "                        img_size=img_size\n",
        "                    )\n",
        "                )\n",
        "        return out\n",
        "\n",
        "\n",
        "    def forward(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
        "        \"\"\"Forward pass of the Decoder.\"\"\"\n",
        "        lateral_out: List[Tensor] = [lateral(fmap) for lateral, fmap in zip(self._lateral, list(x.values())[-self._lateral_levels:])]\n",
        "\n",
        "        up_out: List[Tensor] = []\n",
        "        for idx, x in enumerate(reversed(lateral_out)):\n",
        "            if idx != 0:\n",
        "                x = x + up\n",
        "\n",
        "            if idx < self._lateral_levels - 1:\n",
        "                up = self._up[idx](x)\n",
        "\n",
        "            up_out.append(x)\n",
        "\n",
        "        cnn_outputs: Dict[int, Tensor] = {stage: self._out[idx](fmap) for idx, (fmap, stage) in enumerate(zip(reversed(up_out), self._required_stages))}\n",
        "        return self._forward_hierarchical(cnn_outputs)\n",
        "\n",
        "    def _forward_hierarchical(self, cnn_outputs: Dict[int, Tensor]) -> Dict[str, Tensor]:\n",
        "        \"\"\"Forward pass through the hierarchical decoder.\"\"\"\n",
        "        xs: List[Tensor] = [cnn_outputs[key].clone() for key in range(max(cnn_outputs.keys()), min(cnn_outputs.keys())-1, -1)]\n",
        "\n",
        "        out_dict: Dict[str, Tensor] = {}\n",
        "        QK: List[Tensor] = []\n",
        "        for i, key in enumerate(range(max(cnn_outputs.keys()), min(cnn_outputs.keys())-1, -1)):\n",
        "            QK = [xs[i]] + QK\n",
        "            if i == 0:\n",
        "                Pi = QK[0]\n",
        "            else:\n",
        "                Pi = self.hierarchical_dec[i](QK)\n",
        "            QK[0] = Pi\n",
        "            out_dict[f'P{key}'] = Pi\n",
        "\n",
        "            if self.use_seg:\n",
        "                Pi_seg = self._seg_head[i](Pi)\n",
        "                out_dict[f'S{key}'] = Pi_seg\n",
        "\n",
        "        return out_dict\n",
        "\n",
        "\n",
        "\n",
        "class HierarchicalViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Hierarchical Vision Transformer (HViT) for image processing tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "\n",
        "        # Configuration parameters\n",
        "        self.backbone = config['backbone_net']\n",
        "        in_channels = 2 * config.get('in_channels', 1)  # source + target\n",
        "        kernel_size = config.get('kernel_size', 3)\n",
        "        emb_dim = config.get('start_channels', 32)\n",
        "        data_size = config.get('data_size', [160, 192, 224])\n",
        "        self.out_fmaps = config.get('out_fmaps', ['P4', 'P3', 'P2', 'P1'])\n",
        "\n",
        "        # Calculate number of stages\n",
        "        num_stages = min(int(math.log2(min(data_size))) - 1,\n",
        "                         max(int(fmap[-1]) for fmap in self.out_fmaps) + 1)\n",
        "\n",
        "        strides = [1] + [2] * (num_stages - 1)\n",
        "        kernel_sizes = [kernel_size] * num_stages\n",
        "\n",
        "        config['num_stages'] = num_stages\n",
        "        config['strides'] = strides\n",
        "\n",
        "        # Build encoder\n",
        "        self._encoder = nn.ModuleList()\n",
        "        if self.backbone.lower() in ['fpn', 'fpn']:\n",
        "            for k in range(num_stages):\n",
        "                blk = EncoderCnnBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=emb_dim,\n",
        "                    kernel_size=kernel_sizes[k],\n",
        "                    stride=strides[k]\n",
        "                )\n",
        "                self._encoder.append(blk)\n",
        "\n",
        "                in_channels = emb_dim\n",
        "                emb_dim *= 2\n",
        "\n",
        "        # Build decoder\n",
        "        if self.backbone.lower() in ['fpn', 'fpn']:\n",
        "            self._decoder = Decoder(config)\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize model weights.\"\"\"\n",
        "        for m in self.modules():\n",
        "            self._init_weights(m)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        \"\"\"Initialize the weights of the module.\"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            timm_trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x: Tensor, verbose: bool = False) -> Dict[str, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the HierarchicalViT model.\n",
        "        \"\"\"\n",
        "        down = {}\n",
        "        if self.backbone.lower() in ['fpn', 'fpn']:\n",
        "            for stage_id, module in enumerate(self._encoder):\n",
        "                x = module(x)\n",
        "                down[f'C{stage_id}'] = x\n",
        "            up = self._decoder(down)\n",
        "\n",
        "        if verbose:\n",
        "            for key, item in down.items():\n",
        "                print(f'down {key}', item.shape)\n",
        "            for key, item in up.items():\n",
        "                print(f'up {key}', item.shape)\n",
        "        return up\n",
        "\n",
        "class RegistrationHead(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Registration head for generating displacement fields.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
        "        super().__init__()\n",
        "        conv3d = nn.Conv3d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2\n",
        "        )\n",
        "        # Initialize weights with small random values\n",
        "        conv3d.weight = nn.Parameter(torch.zeros_like(conv3d.weight).normal_(0, 1e-5))\n",
        "        conv3d.bias = nn.Parameter(torch.zeros(conv3d.bias.shape))\n",
        "        self.add_module('conv3d', conv3d)\n",
        "\n",
        "class HierarchicalViT_Light(nn.Module):\n",
        "    \"\"\"\n",
        "    Light Hierarchical Vision Transformer (HViT) model for image registration.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: dict):\n",
        "        super(HierarchicalViT_Light, self).__init__()\n",
        "        self.upsample_df = config.get('upsample_df', False)\n",
        "        self.upsample_scale_factor = config.get('upsample_scale_factor', 2)\n",
        "        self.scale_level_df = config.get('scale_level_df', 'P1')\n",
        "        self.ndims = config.get('ndims', 3)\n",
        "        self._NUM_CROSS_ATT = config.get('NUM_CROSS_ATT', -1)\n",
        "        self.deformable = HierarchicalViT(config)\n",
        "        self.avg_pool = nn.AvgPool3d(3, stride=2, padding=1)\n",
        "        self.spatial_trans = SpatialTransformer(config['data_size'])\n",
        "        self.reg_head = RegistrationHead(\n",
        "            in_channels=config.get('fpn_channels', 64),\n",
        "            out_channels=ndims,\n",
        "            kernel_size=ndims,\n",
        "        )\n",
        "\n",
        "    def forward(self, source: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the HViT model.\n",
        "        \"\"\"\n",
        "        x = torch.cat((source, target), dim=1)\n",
        "        x_dec = self.deformable(x)\n",
        "\n",
        "        # Extract features at the specified scale level\n",
        "        x_dec = x_dec[self.scale_level_df]\n",
        "        flow = self.reg_head(x_dec)\n",
        "\n",
        "        if self.upsample_df:\n",
        "            flow = nn.Upsample(scale_factor=self.upsample_scale_factor,\n",
        "                               mode='trilinear',\n",
        "                               align_corners=False)(flow)\n",
        "\n",
        "        moved = self.spatial_trans(source, flow)\n",
        "        return moved, flow\n",
        "# Trainer: Lightning Module\n",
        "\n",
        "class LiTHViT(LightningModule):\n",
        "    def __init__(self, args, config, wandb_logger=None, save_model_every_n_epochs=10):\n",
        "        super().__init__()\n",
        "        self.automatic_optimization = False\n",
        "        self.args = args\n",
        "        self.config = config\n",
        "        self.best_val_loss = 1e8\n",
        "        self.save_model_every_n_epochs = save_model_every_n_epochs\n",
        "        self.lr = args.lr\n",
        "        self.last_epoch = 0\n",
        "        self.tgt2src_reg = args.tgt2src_reg\n",
        "        self.hvit_light = args.hvit_light\n",
        "        self.precision = args.precision\n",
        "\n",
        "        # Initialize logger\n",
        "        self.custom_logger = Logger(save_dir=\"./logs\")\n",
        "\n",
        "        self.hvit = HierarchicalViT_Light(config) if self.hvit_light else HierarchicalViT(config)\n",
        "\n",
        "        self.loss_weights = {\n",
        "            \"mse\": self.args.mse_weights,\n",
        "            \"dice\": self.args.dice_weights,\n",
        "            \"grad\": self.args.grad_weights\n",
        "        }\n",
        "        self.wandb_logger = wandb_logger\n",
        "        self.test_step_outputs = []\n",
        "\n",
        "    def _forward(self, batch, calc_score: bool = False, tgt2src_reg: bool = False):\n",
        "        _loss = {}\n",
        "        _score = 0.\n",
        "\n",
        "        dtype_map = {\n",
        "            'bf16': torch.bfloat16,\n",
        "            'fp32': torch.float32,\n",
        "            'fp16': torch.float16\n",
        "        }\n",
        "        dtype_ = dtype_map.get(self.precision, torch.float32)\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=dtype_):\n",
        "            if tgt2src_reg:\n",
        "                target, source = batch[0].to(dtype=dtype_), batch[1].to(dtype=dtype_)\n",
        "                tgt_seg, src_seg = batch[2], batch[3]\n",
        "            else:\n",
        "                source, target = batch[0].to(dtype=dtype_), batch[1].to(dtype=dtype_)\n",
        "                src_seg, tgt_seg = batch[2], batch[3]\n",
        "\n",
        "            moved, flow = self.hvit(source, target)\n",
        "\n",
        "            if calc_score:\n",
        "                moved_seg = self._get_one_hot_from_src(src_seg, flow, self.args.num_labels)\n",
        "                _score = DiceScore(moved_seg, tgt_seg.long(), self.args.num_labels)\n",
        "\n",
        "            _loss = {}\n",
        "            for key, weight in self.loss_weights.items():\n",
        "                if key == \"mse\":\n",
        "                    _loss[key] = weight * loss_functions[key](moved, target)\n",
        "                elif key == \"dice\":\n",
        "                    moved_seg = self._get_one_hot_from_src(src_seg, flow, self.args.num_labels)\n",
        "                    _loss[key] = weight * loss_functions[key](moved_seg, tgt_seg.long())\n",
        "                elif key == \"grad\":\n",
        "                    _loss[key] = weight * loss_functions[key](flow)\n",
        "\n",
        "            _loss[\"avg_loss\"] = sum(_loss.values()) / len(_loss)\n",
        "        return _loss, _score\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.hvit.train()\n",
        "        opt = self.optimizers()\n",
        "\n",
        "        loss1, _ = self._forward(batch, calc_score=False)\n",
        "        self.manual_backward(loss1[\"avg_loss\"])\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if self.tgt2src_reg:\n",
        "            loss2, _ = self._forward(batch, tgt2src_reg=True, calc_score=False)\n",
        "            self.manual_backward(loss2[\"avg_loss\"])\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        total_loss = {\n",
        "            key: (loss1[key].item() + loss2[key].item()) / 2 if self.tgt2src_reg and key in loss2 else loss1[key].item()\n",
        "            for key in loss1.keys()\n",
        "        }\n",
        "\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics(total_loss, step=self.global_step)\n",
        "        return total_loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        if self.current_epoch % self.save_model_every_n_epochs == 0:\n",
        "            checkpoints_dir = f\"./checkpoints/{self.current_epoch}\"\n",
        "            os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "            checkpoint_path = f\"{checkpoints_dir}/model_epoch_{self.current_epoch}.ckpt\"\n",
        "            self.trainer.save_checkpoint(checkpoint_path)\n",
        "            self.custom_logger.info(f\"Saved model at epoch {self.current_epoch}\")  # Use custom_logger\n",
        "\n",
        "        current_lr = self.optimizers().param_groups[0]['lr']\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"learning_rate\": current_lr}, step=self.global_step)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        with torch.no_grad():\n",
        "            self.hvit.eval()\n",
        "            _loss, _score = self._forward(batch, calc_score=True)\n",
        "\n",
        "        # Log each component of the validation loss\n",
        "        for loss_name, loss_value in _loss.items():\n",
        "            self.log(f\"val_{loss_name}\", loss_value, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        # Log the mean validation score if available\n",
        "        if _score is not None:\n",
        "            self.log(\"val_score\", _score.mean(), on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        # Log to wandb\n",
        "        if self.wandb_logger:\n",
        "            log_dict = {f\"val_{k}\": v.item() for k, v in _loss.items()}\n",
        "            log_dict.update({\n",
        "                \"val_score_mean\": _score.mean().item() if _score is not None else None,\n",
        "            })\n",
        "            self.wandb_logger.log_metrics({k: v for k, v in log_dict.items() if v is not None}, step=self.global_step)\n",
        "\n",
        "        return {\"val_loss\": _loss[\"avg_loss\"], \"val_score\": _score.mean().item()}\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Callback method called at the end of the validation epoch.\n",
        "        Saves the best model based on validation loss and logs metrics.\n",
        "        \"\"\"\n",
        "        val_loss = self.trainer.callback_metrics.get(\"val_loss\")\n",
        "        checkpoints_dir = f\"./checkpoints/{self.current_epoch}\"\n",
        "        if val_loss is not None and self.current_epoch > 0:\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                best_model_path = f\"{checkpoints_dir}/best_model.ckpt\"\n",
        "                self.trainer.save_checkpoint(best_model_path)\n",
        "                if self.wandb_logger:\n",
        "                    self.wandb_logger.experiment.log({\n",
        "                        \"best_model_saved\": best_model_path,\n",
        "                        \"best_val_loss\": self.best_val_loss.item()\n",
        "                    })\n",
        "                logger.info(f\"New best model saved with validation loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Performs a single test step on a batch of data.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.hvit.eval()\n",
        "            _, _score = self._forward(batch, calc_score=True)\n",
        "\n",
        "        _score = _score.mean() if isinstance(_score, torch.Tensor) else torch.tensor(_score).mean()\n",
        "\n",
        "        self.test_step_outputs.append(_score)\n",
        "\n",
        "        # Log to wandb only if the logger is available\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"test_dice\": _score.item()}, step=self.global_step)\n",
        "\n",
        "        # Return as a dict with tensor values\n",
        "        return {\"test_dice\": _score}\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Callback method called at the end of the test epoch.\n",
        "        Computes and logs the average test Dice score.\n",
        "        \"\"\"\n",
        "        # Calculate the average Dice score across all test steps\n",
        "        avg_test_dice = torch.stack(self.test_step_outputs).mean()\n",
        "\n",
        "        # Log the average test Dice score\n",
        "        self.log(\"avg_test_dice\", avg_test_dice, prog_bar=True)\n",
        "\n",
        "        # Log to wandb if available\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"total_test_dice_avg\": avg_test_dice.item()})\n",
        "\n",
        "        # Clear the test step outputs list for the next test epoch\n",
        "        self.test_step_outputs.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configures the optimizer and learning rate scheduler for the model.\n",
        "        \"\"\"\n",
        "        optimizer = torch.optim.Adam(self.hvit.parameters(), lr=self.lr, weight_decay=0, amsgrad=True)\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=self.lr_lambda)\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"epoch\",\n",
        "                \"frequency\": 1,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def lr_lambda(self, epoch):\n",
        "        \"\"\"\n",
        "        Defines the learning rate schedule.\n",
        "        \"\"\"\n",
        "        return math.pow(1 - epoch / self.trainer.max_epochs, 0.9)\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_checkpoint(cls, checkpoint_path, args=None, wandb_logger=None):\n",
        "        \"\"\"\n",
        "        Loads a model from a checkpoint file.\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "        args = args or checkpoint.get('hyper_parameters', {}).get('args')\n",
        "        config = checkpoint.get('hyper_parameters', {}).get('config')\n",
        "\n",
        "        model = cls(args, config, wandb_logger)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "        if 'hyper_parameters' in checkpoint:\n",
        "            hyper_params = checkpoint['hyper_parameters']\n",
        "            for attr in ['lr', 'best_val_loss', 'last_epoch']:\n",
        "                setattr(model, attr, hyper_params.get(attr, getattr(model, attr)))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        \"\"\"\n",
        "        Callback to save additional information in the checkpoint.\n",
        "        \"\"\"\n",
        "        checkpoint['hyper_parameters'] = {\n",
        "            'config': self.config,\n",
        "            'lr': self.lr,\n",
        "            'best_val_loss': self.best_val_loss,\n",
        "            'last_epoch': self.current_epoch\n",
        "        }\n",
        "\n",
        "    def _get_one_hot_from_src(self, src_seg, flow, num_labels):\n",
        "        \"\"\"\n",
        "        Converts source segmentation to one-hot encoding and applies deformation.\n",
        "        \"\"\"\n",
        "        src_seg_onehot = get_one_hot(src_seg, self.args.num_labels)\n",
        "        deformed_segs = [\n",
        "            self.hvit.spatial_trans(src_seg_onehot[:, i:i+1, ...].float(), flow.float())\n",
        "            for i in range(num_labels)\n",
        "        ]\n",
        "        return torch.cat(deformed_segs, dim=1)\n",
        "# Instantiate the Model and Run with Dummy Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8ByTBe_C4Vm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9SwqhLPGOcG",
        "outputId": "5aa79721-e564-4f85-97ec-cdf5b64fb653"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([1, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([1, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([1, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([1, 64, 20, 24, 28])\n",
            "moved shape: torch.Size([1, 1, 40, 48, 56]), flow shape: torch.Size([1, 3, 40, 48, 56])\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    'WO_SELF_ATT': False,\n",
        "    '_NUM_CROSS_ATT': -1,\n",
        "    'out_fmaps': ['P4', 'P3', 'P2', 'P1'],  # Number of levels = 4\n",
        "    'scale_level_df': 'P1',\n",
        "    'upsample_df': True,\n",
        "    'upsample_scale_factor': 2,\n",
        "    'fpn_channels': 64,\n",
        "    'start_channels': 32,\n",
        "    'patch_size': [2, 2, 2, 2],  # Matches number of levels\n",
        "    'backbone_net': 'fpn',\n",
        "    'in_channels': 1,\n",
        "    'data_size': [40, 48, 56],\n",
        "    'bias': True,\n",
        "    'norm_type': 'instance',\n",
        "    'kernel_size': 3,\n",
        "    'depths': [1, 1, 1, 1],  # Matches number of levels\n",
        "    'mlp_ratio': 2,\n",
        "    'num_heads': [4, 8, 16, 32],  # Matches number of levels\n",
        "    'drop_path_rate': 0.,\n",
        "    'qkv_bias': True,\n",
        "    'drop_rate': 0.,\n",
        "    'attn_drop_rate': 0.,\n",
        "    'use_seg_loss': False,\n",
        "    'use_seg_proxy_loss': False,\n",
        "    'num_organs': -1,\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize the model# Initialize the model\n",
        "model = HierarchicalViT_Light(config)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define dummy inputs\n",
        "B, C, H, W, D = 1, 1, 40, 48, 56\n",
        "source = torch.rand([B, C, H, W, D]).to(device)\n",
        "target = torch.rand([B, C, H, W, D]).to(device)\n",
        "\n",
        "# Perform a forward pass\n",
        "moved, flow = model(source, target)\n",
        "print(f'moved shape: {moved.shape}, flow shape: {flow.shape}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "53e9603b385a4bc2875a241c49c07401",
            "e73b138c5c4747fca16ee4dfdada39a1",
            "5f64cd12fe834904b2ebe458e80311d0",
            "edc0facd34734af0a04c73e5e1e5e01d",
            "493f538365344f71b6c38c58678bc885",
            "96e87d0fb39d4dc481d9f69d3b85f1bb",
            "c35ba06c29c8401fa9480a05834d7df1",
            "ba25410ac20e410a8d18b0d8e6618be3",
            "8b1460be627844e7af5e5ce1d5a7fcc0",
            "0be89667db1242aa919ba5f1c6dccb2d",
            "c8f8da0fc85e4b92a9c98c2a450eef7d"
          ]
        },
        "id": "Q54USs_nMyVm",
        "outputId": "66a36471-f187-412c-ed2b-afbeffd8bfba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: False, used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malim98barnet\u001b[0m (\u001b[33malim98barnet-university-of-tehran\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20250110_080156-mrb1un3g</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_dummy_test/runs/mrb1un3g' target=\"_blank\">jolly-meadow-2</a></strong> to <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_dummy_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_dummy_test' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/hvit_dummy_test</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_dummy_test/runs/mrb1un3g' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/hvit_dummy_test/runs/mrb1un3g</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: \n",
            "  | Name | Type                  | Params | Mode \n",
            "-------------------------------------------------------\n",
            "0 | hvit | HierarchicalViT_Light | 7.2 M  | train\n",
            "-------------------------------------------------------\n",
            "7.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "7.2 M     Total params\n",
            "28.993    Total estimated model params size (MB)\n",
            "234       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name | Type                  | Params | Mode \n",
            "-------------------------------------------------------\n",
            "0 | hvit | HierarchicalViT_Light | 7.2 M  | train\n",
            "-------------------------------------------------------\n",
            "7.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "7.2 M     Total params\n",
            "28.993    Total estimated model params size (MB)\n",
            "234       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53e9603b385a4bc2875a241c49c07401",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-10 08:02:59,619 - __main__ - INFO - Saved model at epoch 0\n",
            "2025-01-10 08:02:59,619 - __main__ - INFO - Saved model at epoch 0\n",
            "INFO:__main__:Saved model at epoch 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 10, 12, 14])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n",
            "MLP input shape after permute: torch.Size([2, 64, 20, 24, 28])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
          ]
        }
      ],
      "source": [
        "# Training arguments\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.lr = 0.001\n",
        "        self.mse_weights = 1.0\n",
        "        self.dice_weights = 1.0\n",
        "        self.grad_weights = 1.0\n",
        "        self.tgt2src_reg = False\n",
        "        self.hvit_light = True\n",
        "        self.precision = 'fp32'  # Training precision (e.g., 'bf16', 'fp16', 'fp32')\n",
        "        self.num_labels = 36\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Define a dummy dataset loader\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, num_samples=10, input_dim=(1, 40, 48, 56), num_classes=36):\n",
        "        self.num_samples = num_samples\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Create dummy source, target, and their segmentations\n",
        "        source = torch.rand(self.input_dim)\n",
        "        target = torch.rand(self.input_dim)\n",
        "        source_seg = torch.randint(0, self.num_classes, self.input_dim)\n",
        "        target_seg = torch.randint(0, self.num_classes, self.input_dim)\n",
        "        return source, target, source_seg, target_seg\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "# Instantiate dummy dataloader\n",
        "dummy_dataloader = DataLoader(DummyDataset(), batch_size=2, shuffle=True, num_workers=0)\n",
        "\n",
        "# Initialize WandB logger (optional, replace with None if not using WandB)\n",
        "wandb_logger = WandbLogger(project=\"hvit_dummy_test\")\n",
        "\n",
        "# Instantiate the Lightning module\n",
        "lit_model = LiTHViT(args, config, wandb_logger=wandb_logger)\n",
        "\n",
        "# Define the PyTorch Lightning Trainer\n",
        "trainer = Trainer(\n",
        "    max_epochs=5,  # Number of epochs\n",
        "    logger=wandb_logger,  # Log training metrics\n",
        "    enable_checkpointing=False,  # Disable checkpointing for testing\n",
        "    devices=1,  # Number of GPUs (set to 0 for CPU)\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",  # Use GPU if available\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.fit(lit_model, train_dataloaders=dummy_dataloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP7dCROZMqYT"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqFr0uwM6iUh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Define a dummy configuration\n",
        "# config = {\n",
        "#     'WO_SELF_ATT': False,  # Add this parameter\n",
        "\n",
        "#     '_NUM_CROSS_ATT': -1,\n",
        "#     'out_fmaps': ['P4', 'P3', 'P2', 'P1'],\n",
        "#     'scale_level_df': 'P1',\n",
        "#     'upsample_df': True,\n",
        "#     'upsample_scale_factor': 2,\n",
        "#     'fpn_channels': 64,\n",
        "#     'start_channels': 32,\n",
        "#     'patch_size': 2,\n",
        "#     'bspl': False,\n",
        "\n",
        "#     'backbone_net': 'fpn',\n",
        "#     'in_channels': 1,\n",
        "#     'data_size': [40, 48, 56],  # Adjusted to match the dummy data dimensions below\n",
        "#     'bias': True,\n",
        "#     'norm_type': 'instance',\n",
        "#     'cuda': 0,\n",
        "#     'kernel_size': 3,\n",
        "#     'depths': [1],\n",
        "#     'mlp_ratio': 2,\n",
        "\n",
        "#     'num_heads': [32],\n",
        "#     'drop_path_rate': 0.,\n",
        "#     'qkv_bias': True,\n",
        "#     'drop_rate': 0.,\n",
        "#     'attn_drop_rate': 0.,\n",
        "\n",
        "#     'use_seg_loss': False,\n",
        "#     'use_seg_proxy_loss': False,\n",
        "#     'num_organs': -1\n",
        "# }\n",
        "\n",
        "# # Instantiate the model\n",
        "# model = HierarchicalViT_Light(config)\n",
        "\n",
        "# # Move model to appropriate device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# # Create dummy data\n",
        "# # Assuming input dimensions as per config['data_size'] = [40, 48, 56]\n",
        "# B, C, H, W, D = 1, 1, 40, 48, 56\n",
        "# source = torch.rand([B, C, H, W, D]).to(device)\n",
        "# target = torch.rand([B, C, H, W, D]).to(device)\n",
        "\n",
        "# # Run a forward pass\n",
        "# with torch.no_grad():\n",
        "#     moved, flow = model(source, target)\n",
        "#     print(f'moved shape: {moved.shape}, flow shape: {flow.shape}')\n",
        "\n",
        "# # Check trainable parameters\n",
        "# total_params = count_parameters(model)\n",
        "# print(f\"Total trainable parameters: {total_params / 1e6:.5f} million\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMHH7QcHEVb1",
        "outputId": "01ae5195-2d20-47f6-8dc5-1151a25c204f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention output shape: torch.Size([4, 16, 64])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "device='cpu'\n",
        "# Initialize Attention\n",
        "attention = Attention(dim=64, num_heads=8, patch_size=2, attention_type=\"local\").to(device)\n",
        "x = torch.rand(4, 16, 64).to(device)  # B_, N, C\n",
        "\n",
        "# Forward pass\n",
        "output = attention(x)\n",
        "print(f'Attention output shape: {output.shape}')\n",
        "assert output.shape == (4, 16, 64), \"Attention output shape mismatch\"\n",
        "x = torch.rand(2, 40, 48, 56, 16).to(device)  # B, H, W, D, C\n",
        "patch_size = 4\n",
        "\n",
        "# Extract patches\n",
        "patches, H, W, D = get_patches(x, patch_size)\n",
        "print(f'Patches shape: {patches.shape}')\n",
        "\n",
        "# Reconstruct image\n",
        "reconstructed = get_image(patches, patch_size, H, W, D, x.shape[1], x.shape[2], x.shape[3])\n",
        "print(f'Reconstructed shape: {reconstructed.shape}')\n",
        "assert torch.allclose(x, reconstructed, atol=1e-5), \"Reconstruction mismatch\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd97_x61Egog",
        "outputId": "eb74a321-1455-4257-9a73-30f864e944e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP input shape after permute: torch.Size([2, 64, 40, 48, 56])\n",
            "ViTBlock output shape: torch.Size([2, 40, 48, 56, 64])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class ViTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer Block.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 embed_dim: int,\n",
        "                 input_dims: List[int],\n",
        "                 num_heads: int,\n",
        "                 mlp_type: str,\n",
        "                 patch_size: int,\n",
        "                 mlp_ratio: float,\n",
        "                 qkv_bias: bool,\n",
        "                 qk_scale: Optional[float],\n",
        "                 drop: float,\n",
        "                 attn_drop: float,\n",
        "                 drop_path: float,\n",
        "                 act_layer: str,\n",
        "                 attention_type: str,\n",
        "                 norm_layer: Callable[..., nn.Module],\n",
        "                 layer_scale: Optional[float]):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_windows = prod_func([d // patch_size for d in input_dims])\n",
        "\n",
        "        self.norm1 = norm_layer(embed_dim)\n",
        "        self.attn = Attention(\n",
        "            dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            patch_size=patch_size,\n",
        "            attention_type=attention_type,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop,\n",
        "        )\n",
        "\n",
        "        self.drop_path = timm_DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(embed_dim)\n",
        "\n",
        "        self.mlp = Conv3dReLU(\n",
        "            in_channels=embed_dim,\n",
        "            out_channels=int(embed_dim * mlp_ratio),\n",
        "            kernel_size=3,  # Assuming kernel_size=3 for MLP\n",
        "            padding=1,\n",
        "            stride=1,\n",
        "            use_batchnorm=True,\n",
        "        )\n",
        "\n",
        "        # Add projection layer to ensure output channels match embed_dim\n",
        "        self.proj = nn.Conv3d(\n",
        "            in_channels=int(embed_dim * mlp_ratio),\n",
        "            out_channels=embed_dim,\n",
        "            kernel_size=1\n",
        "        )\n",
        "        self.layer_scale = layer_scale is not None and isinstance(layer_scale, (int, float))\n",
        "        if self.layer_scale:\n",
        "            self.gamma1 = nn.Parameter(layer_scale * torch.ones(embed_dim), requires_grad=True)\n",
        "            self.gamma2 = nn.Parameter(layer_scale * torch.ones(embed_dim), requires_grad=True)\n",
        "        else:\n",
        "            self.gamma1 = 1.0\n",
        "            self.gamma2 = 1.0\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor, q_ms: Optional[Tensor]) -> Tensor:\n",
        "        B, H, W, D, C = x.size()\n",
        "        shortcut = x\n",
        "\n",
        "        # Normalize and compute attention\n",
        "        x = self.norm1(x)\n",
        "        x_windows, Hatt, Watt, Datt = get_patches(x, self.patch_size)\n",
        "        x_windows = x_windows.view(-1, self.patch_size ** 3, C)\n",
        "\n",
        "        # Compute attention and reconstruct image\n",
        "        attn_windows = self.attn(x_windows, q_ms)\n",
        "        x = get_image(attn_windows, self.patch_size, Hatt, Watt, Datt, H, W, D)\n",
        "\n",
        "        # Apply shortcut and drop path\n",
        "        x = shortcut + self.drop_path(self.gamma1 * x)\n",
        "\n",
        "        # Apply MLP\n",
        "        x_mlp_input = self.norm2(x).permute(0, 4, 1, 2, 3)\n",
        "        print(f\"MLP input shape after permute: {x_mlp_input.shape}\")  # Debug print\n",
        "        x_mlp_output = self.mlp(x_mlp_input)\n",
        "        x_mlp_output = self.proj(x_mlp_output).permute(0, 2, 3, 4, 1)\n",
        "\n",
        "        # Add MLP output with drop path and gamma scaling\n",
        "        x = x + self.drop_path(self.gamma2 * x_mlp_output)\n",
        "        return x\n",
        "\n",
        "\n",
        "vit_block = ViTBlock(embed_dim=64, input_dims=[40, 48, 56], num_heads=4, mlp_type=\"basic\",\n",
        "                     patch_size=4, mlp_ratio=2, qkv_bias=True, qk_scale=None, drop=0.1,\n",
        "                     attn_drop=0.1, drop_path=0.1, act_layer=\"relu\", attention_type=\"local\",\n",
        "                     norm_layer=nn.LayerNorm, layer_scale=1e-5).to(device)\n",
        "\n",
        "x = torch.rand(2, 40, 48, 56, 64).to(device)  # Correct input shape\n",
        "output = vit_block(x, q_ms=None)\n",
        "print(f'ViTBlock output shape: {output.shape}')\n",
        "assert output.shape == (2, 40, 48, 56, 64), \"ViTBlock output shape mismatch\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9Tz1oWGF7VZ",
        "outputId": "573dbf82-3090-466c-e09b-03a12c10f61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PatchEmbed output shape: torch.Size([2, 32, 40, 48, 56])\n"
          ]
        }
      ],
      "source": [
        "patch_embed = PatchEmbed(in_chans=1, out_chans=32, kernel_size=3, stride=1).to(device)\n",
        "x = torch.rand(2, 1, 40, 48, 56).to(device)  # B, C, H, W, D\n",
        "\n",
        "output = patch_embed(x)\n",
        "print(f'PatchEmbed output shape: {output.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHRhc-WbF9O5",
        "outputId": "849d34dd-b7db-4e54-da1d-e1792e3c2f46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP input shape after permute: torch.Size([2, 64, 40, 48, 56])\n",
            "MLP input shape after permute: torch.Size([2, 64, 40, 48, 56])\n",
            "ViTLayer output shape: torch.Size([2, 64, 40, 48, 56])\n"
          ]
        }
      ],
      "source": [
        "vit_layer = ViTLayer(attention_type=\"local\", dim=64, dim_out=64, depth=2, input_dims=[40, 48, 56],\n",
        "                     num_heads=4, patch_size=4, mlp_type=\"basic\", mlp_ratio=2, qkv_bias=True,\n",
        "                     qk_scale=None, drop=0.1, attn_drop=0.1, drop_path=0.1, norm_layer=nn.LayerNorm,\n",
        "                     norm_type=\"instance\", layer_scale=1e-5, act_layer=\"relu\").to(device)\n",
        "\n",
        "x = torch.rand(2, 64, 40, 48, 56).to(device)\n",
        "output = vit_layer(x, q_ms=None, CONCAT_ok=False)\n",
        "print(f'ViTLayer output shape: {output.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKovnMWvGv0S"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder module for the hierarchical vision transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "        self._num_stages: int = config['num_stages']\n",
        "        self.use_seg: bool = config['use_seg_loss']\n",
        "\n",
        "        # Determine channels of encoder feature maps\n",
        "        encoder_out_channels: torch.Tensor = torch.tensor([config['start_channels'] * 2**stage for stage in range(self._num_stages)])\n",
        "        self._NUM_CROSS_ATT=config.get('NUM_CROSS_ATT', -1)\n",
        "        # Estimate required stages\n",
        "        required_stages: Set[int] = set(int(fmap[-1]) for fmap in config['out_fmaps'])\n",
        "        self._required_stages: Set[int] = required_stages\n",
        "\n",
        "        earliest_required_stage: int = min(required_stages)\n",
        "\n",
        "        # Lateral connections\n",
        "        lateral_in_channels: torch.Tensor = encoder_out_channels[earliest_required_stage:]\n",
        "        lateral_out_channels: torch.Tensor = lateral_in_channels.clip(max=config['fpn_channels'])\n",
        "\n",
        "        self._lateral: nn.ModuleList = nn.ModuleList([\n",
        "            nn.Conv3d(in_channels=in_ch, out_channels=out_ch, kernel_size=1)\n",
        "            for in_ch, out_ch in zip(lateral_in_channels, lateral_out_channels)\n",
        "        ])\n",
        "        self._lateral_levels: int = len(self._lateral)\n",
        "\n",
        "        # Output layers\n",
        "        out_in_channels: List[int] = [lateral_out_channels[-self._num_stages + required_stage].item() for required_stage in required_stages]\n",
        "        out_out_channels: List[int] = [int(config['fpn_channels'])] * len(out_in_channels)\n",
        "        out_out_channels[0] = int(config['fpn_channels'])\n",
        "\n",
        "        self._out: nn.ModuleList = nn.ModuleList([\n",
        "            nn.Conv3d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1)\n",
        "            for in_ch, out_ch in zip(out_in_channels, out_out_channels)\n",
        "        ])\n",
        "\n",
        "        # Upsampling layers\n",
        "        self._up: nn.ModuleList = nn.ModuleList([\n",
        "            nn.ConvTranspose3d(\n",
        "                in_channels=list(reversed(lateral_out_channels))[level],\n",
        "                out_channels=list(reversed(lateral_out_channels))[level+1],\n",
        "                kernel_size=list(reversed(config['strides']))[level],\n",
        "                stride=list(reversed(config['strides']))[level]\n",
        "            )\n",
        "            for level in range(len(lateral_out_channels)-1)\n",
        "        ])\n",
        "\n",
        "        # Multi-scale attention\n",
        "        self.hierarchical_dec: nn.ModuleList = self._create_hierarchical_layers(config, out_out_channels)\n",
        "\n",
        "        if self.use_seg:\n",
        "            self._seg_head: nn.ModuleList = nn.ModuleList([\n",
        "                nn.Conv3d(out_ch, config['num_organs'] + 1, kernel_size=1, stride=1)\n",
        "                for out_ch in out_out_channels\n",
        "            ])\n",
        "    def _create_hierarchical_layers(self, config: Dict[str, Any], out_out_channels: List[int]) -> nn.ModuleList:\n",
        "        \"\"\"Create hierarchical layers for multi-scale attention.\"\"\"\n",
        "        out: nn.ModuleList = nn.ModuleList()\n",
        "        img_size: List[List[int]] = []\n",
        "        feats_num: List[int] = []\n",
        "\n",
        "        num_levels = len(out_out_channels)  # Ensure `num_levels` matches the length of `out_out_channels`\n",
        "\n",
        "        for k, out_ch in enumerate(out_out_channels):\n",
        "            img_size.append([int(item / (2 ** (self._num_stages - k - 1))) for item in config['data_size']])\n",
        "            feats_num.append(out_ch)\n",
        "            n: int = len(feats_num)\n",
        "\n",
        "            if k == 0:\n",
        "                out.append(nn.Identity())\n",
        "            else:\n",
        "                # Ensure depths and num_heads have enough entries\n",
        "                depths = config.get('depths', [1] * num_levels)\n",
        "                num_heads = config.get('num_heads', [32] * num_levels)\n",
        "\n",
        "                # Use k or level-based indexing\n",
        "                out.append(\n",
        "                    ViT(\n",
        "                        NUM_CROSS_ATT=config.get('NUM_CROSS_ATT', self._NUM_CROSS_ATT),\n",
        "                        PYR_SCALES=[1.],\n",
        "                        feats_num=feats_num,\n",
        "                        hid_dim=int(config.get('fpn_channels', 64)),\n",
        "                        depths=depths,  # Use the list directly\n",
        "                        patch_size=config.get('patch_size', [2] * n),  # Fixed line\n",
        "                        mlp_ratio=int(config.get('mlp_ratio', 2)),\n",
        "                        num_heads=num_heads,  # Use the list directly\n",
        "                        mlp_type='basic',\n",
        "                        norm_type='BatchNorm2d',\n",
        "                        act_layer='gelu',\n",
        "                        drop_path_rate=config.get('drop_path_rate', 0.2),\n",
        "                        qkv_bias=config.get('qkv_bias', True),\n",
        "                        qk_scale=None,\n",
        "                        drop_rate=config.get('drop_rate', 0.),\n",
        "                        attn_drop_rate=config.get('attn_drop_rate', 0.),\n",
        "                        norm_layer=nn.LayerNorm,\n",
        "                        layer_scale=1e-5,\n",
        "                        img_size=img_size\n",
        "                    )\n",
        "                )\n",
        "        return out\n",
        "\n",
        "\n",
        "    def forward(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
        "        \"\"\"Forward pass of the Decoder.\"\"\"\n",
        "        lateral_out: List[Tensor] = [lateral(fmap) for lateral, fmap in zip(self._lateral, list(x.values())[-self._lateral_levels:])]\n",
        "\n",
        "        up_out: List[Tensor] = []\n",
        "        for idx, x in enumerate(reversed(lateral_out)):\n",
        "            if idx != 0:\n",
        "                x = x + up\n",
        "\n",
        "            if idx < self._lateral_levels - 1:\n",
        "                up = self._up[idx](x)\n",
        "\n",
        "            up_out.append(x)\n",
        "\n",
        "        cnn_outputs: Dict[int, Tensor] = {stage: self._out[idx](fmap) for idx, (fmap, stage) in enumerate(zip(reversed(up_out), self._required_stages))}\n",
        "        return self._forward_hierarchical(cnn_outputs)\n",
        "\n",
        "    def _forward_hierarchical(self, cnn_outputs: Dict[int, Tensor]) -> Dict[str, Tensor]:\n",
        "        \"\"\"Forward pass through the hierarchical decoder.\"\"\"\n",
        "        xs: List[Tensor] = [cnn_outputs[key].clone() for key in range(max(cnn_outputs.keys()), min(cnn_outputs.keys())-1, -1)]\n",
        "\n",
        "        out_dict: Dict[str, Tensor] = {}\n",
        "        QK: List[Tensor] = []\n",
        "        for i, key in enumerate(range(max(cnn_outputs.keys()), min(cnn_outputs.keys())-1, -1)):\n",
        "            QK = [xs[i]] + QK\n",
        "            if i == 0:\n",
        "                Pi = QK[0]\n",
        "            else:\n",
        "                Pi = self.hierarchical_dec[i](QK)\n",
        "            QK[0] = Pi\n",
        "            out_dict[f'P{key}'] = Pi\n",
        "\n",
        "            if self.use_seg:\n",
        "                Pi_seg = self._seg_head[i](Pi)\n",
        "                out_dict[f'S{key}'] = Pi_seg\n",
        "\n",
        "        return out_dict\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZezDxFGgGaKT",
        "outputId": "daaf60ee-1bdd-4fa1-fc20-b74d72c5722e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters: 7.24813 million\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Total trainable parameters: {total_params / 1e6:.5f} million\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VSKtTvh6Fvb"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcYhAE/gw0VsRby4Da8Dd2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0be89667db1242aa919ba5f1c6dccb2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "493f538365344f71b6c38c58678bc885": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "53e9603b385a4bc2875a241c49c07401": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e73b138c5c4747fca16ee4dfdada39a1",
              "IPY_MODEL_5f64cd12fe834904b2ebe458e80311d0",
              "IPY_MODEL_edc0facd34734af0a04c73e5e1e5e01d"
            ],
            "layout": "IPY_MODEL_493f538365344f71b6c38c58678bc885"
          }
        },
        "5f64cd12fe834904b2ebe458e80311d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba25410ac20e410a8d18b0d8e6618be3",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b1460be627844e7af5e5ce1d5a7fcc0",
            "value": 4
          }
        },
        "8b1460be627844e7af5e5ce1d5a7fcc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96e87d0fb39d4dc481d9f69d3b85f1bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba25410ac20e410a8d18b0d8e6618be3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c35ba06c29c8401fa9480a05834d7df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8f8da0fc85e4b92a9c98c2a450eef7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e73b138c5c4747fca16ee4dfdada39a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96e87d0fb39d4dc481d9f69d3b85f1bb",
            "placeholder": "",
            "style": "IPY_MODEL_c35ba06c29c8401fa9480a05834d7df1",
            "value": "Epoch4:80%"
          }
        },
        "edc0facd34734af0a04c73e5e1e5e01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0be89667db1242aa919ba5f1c6dccb2d",
            "placeholder": "",
            "style": "IPY_MODEL_c8f8da0fc85e4b92a9c98c2a450eef7d",
            "value": "4/5[00:44&lt;00:11,0.09it/s,v_num=un3g]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}