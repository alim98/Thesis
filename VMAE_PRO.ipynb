{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/Thesis/blob/main/VMAE_PRO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ZO-F0OqDpfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t21Fu_cr5LUu",
        "outputId": "2ecc6291-070c-47be-f29c-396c3d1b2489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
            "Collecting lightning\n",
            "  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Collecting monai\n",
            "  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.44)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.5.1)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.10.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.10/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.12)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.11)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Downloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monai-1.4.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, monai, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.5.0.post0 lightning-utilities-0.11.9 monai-1.4.0 pytorch-lightning-2.5.0.post0 torchmetrics-1.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install einops timm lightning wandb monai gitpython\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download DS"
      ],
      "metadata": {
        "id": "DKLXlFBGSoJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cloud.imi.uni-luebeck.de/s/xcZrLSQYtK68em8/download/OASIS.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZInA0cASqCf",
        "outputId": "4e2306f1-6316-457d-830b-d5bd9360e69b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-10 21:18:15--  https://cloud.imi.uni-luebeck.de/s/xcZrLSQYtK68em8/download/OASIS.zip\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1444971437 (1.3G) [application/zip]\n",
            "Saving to: ‘OASIS.zip’\n",
            "\n",
            "OASIS.zip           100%[===================>]   1.35G  15.3MB/s    in 91s     \n",
            "\n",
            "2025-01-10 21:19:47 (15.2 MB/s) - ‘OASIS.zip’ saved [1444971437/1444971437]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip -q OASIS.zip\n"
      ],
      "metadata": {
        "id": "FxuexOk3S4H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0937108c-4cae-4469-dba1-bc179530d5cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace OASIS/imagesTr/OASIS_0043_0000.nii.gz? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/OASIS/OASIS_dataset.json /content/"
      ],
      "metadata": {
        "id": "EMAbSVZHL2tK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "json_path = \"OASIS/OASIS_dataset.json\"\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Update paths dynamically\n",
        "base_path = \"OASIS\"\n",
        "for entry in data[\"training\"]:\n",
        "    entry[\"image\"] = os.path.join(base_path, entry[\"image\"].lstrip(\"./\"))\n",
        "    entry[\"label\"] = os.path.join(base_path, entry[\"label\"].lstrip(\"./\"))\n",
        "    entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "for entry in data[\"test\"]:\n",
        "    entry[\"image\"] = os.path.join(base_path, entry[\"image\"].lstrip(\"./\"))\n",
        "    entry[\"label\"] = os.path.join(base_path, entry[\"label\"].lstrip(\"./\"))\n",
        "    entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "for entry in data[\"registration_test\"]:\n",
        "    entry[\"fixed\"] = os.path.join(base_path, entry[\"fixed\"].lstrip(\"./\"))\n",
        "    entry[\"moving\"] = os.path.join(base_path, entry[\"moving\"].lstrip(\"./\"))\n",
        "    # entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "for entry in data[\"registration_val\"]:\n",
        "    entry[\"fixed\"] = os.path.join(base_path, entry[\"fixed\"].lstrip(\"./\"))\n",
        "    entry[\"moving\"] = os.path.join(base_path, entry[\"moving\"].lstrip(\"./\"))\n",
        "    # entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "\n",
        "# Save updated JSON\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(data, f, indent=4)"
      ],
      "metadata": {
        "id": "M7F7Nwa_d_gI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## new oasis dataset test"
      ],
      "metadata": {
        "id": "gKW-tdiFr7r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from monai.transforms import Resize\n",
        "\n",
        "class OASIS_Dataset(Dataset):\n",
        "    def __init__(self, json_path, mode=\"training\", input_dim=(128, 128, 128), is_pair=False):\n",
        "        self.input_dim = input_dim\n",
        "        self.is_pair = is_pair\n",
        "\n",
        "        # Load JSON data\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if mode == \"training\":\n",
        "            self.samples = data[\"training\"]\n",
        "        elif mode == \"test\":\n",
        "            self.samples = data[\"test\"]\n",
        "        elif mode == \"registration_val\":\n",
        "            self.samples = data[\"registration_val\"]\n",
        "        elif mode == \"registration_test\":\n",
        "            self.samples = data[\"registration_test\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "        self.transforms_image = Resize(spatial_size=input_dim)\n",
        "        self.transforms_mask = Resize(spatial_size=input_dim, mode=\"nearest\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_pair:\n",
        "            # Load fixed and moving images for registration\n",
        "            sample = self.samples[index]\n",
        "            fixed_path = sample[\"fixed\"]\n",
        "            moving_path = sample[\"moving\"]\n",
        "\n",
        "            fixed = nib.load(fixed_path).get_fdata()\n",
        "            moving = nib.load(moving_path).get_fdata()\n",
        "\n",
        "            fixed = self.transforms_image(torch.from_numpy(fixed).unsqueeze(0).float())\n",
        "            moving = self.transforms_image(torch.from_numpy(moving).unsqueeze(0).float())\n",
        "\n",
        "            # Load segmentation masks if available\n",
        "            # Assuming mask paths are provided; adjust accordingly\n",
        "            fixed_mask_path = sample.get(\"fixed_mask\", None)\n",
        "            moving_mask_path = sample.get(\"moving_mask\", None)\n",
        "\n",
        "            if fixed_mask_path and moving_mask_path:\n",
        "                fixed_mask = nib.load(fixed_mask_path).get_fdata()\n",
        "                moving_mask = nib.load(moving_mask_path).get_fdata()\n",
        "\n",
        "                fixed_mask = self.transforms_mask(torch.from_numpy(fixed_mask).unsqueeze(0).long())\n",
        "                moving_mask = self.transforms_mask(torch.from_numpy(moving_mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                # If masks are not available, return dummy masks\n",
        "                fixed_mask = torch.zeros_like(fixed)\n",
        "                moving_mask = torch.zeros_like(moving)\n",
        "\n",
        "            return fixed, moving, fixed_mask, moving_mask\n",
        "        else:\n",
        "            # Load unpaired data (image, label, mask)\n",
        "            sample = self.samples[index]\n",
        "            image_path = sample[\"image\"]\n",
        "            label_path = sample[\"label\"]\n",
        "            mask_path = sample.get(\"mask\", None)\n",
        "\n",
        "            image = nib.load(image_path).get_fdata()\n",
        "            label = nib.load(label_path).get_fdata()\n",
        "\n",
        "            image = self.transforms_image(torch.from_numpy(image).unsqueeze(0).float())\n",
        "            label = self.transforms_mask(torch.from_numpy(label).unsqueeze(0).long())\n",
        "\n",
        "            if mask_path:\n",
        "                mask = nib.load(mask_path).get_fdata()\n",
        "                mask = self.transforms_mask(torch.from_numpy(mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                mask = torch.zeros_like(label)\n",
        "\n",
        "            return image, label, mask\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# For training (paired data)\n",
        "train_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"training\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=True  # Set to True for paired data\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=4  # Adjust based on your system\n",
        ")\n",
        "\n",
        "# For validation\n",
        "val_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"registration_val\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=True  # Set to True for paired data\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n"
      ],
      "metadata": {
        "id": "OXUhE5U1ldo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: summerize json attributes\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "json_path = \"OASIS/OASIS_dataset.json\"\n",
        "with open(json_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def summarize_json(data):\n",
        "    summary = {}\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, list):\n",
        "            summary[key] = {\n",
        "                \"count\": len(value),\n",
        "                \"example\": value[0] if value else None  # Example item\n",
        "            }\n",
        "        elif isinstance(value, dict):\n",
        "            summary[key] = summarize_json(value) # Recursive call for nested dicts\n",
        "        else:\n",
        "            summary[key] = value\n",
        "    return summary\n",
        "\n",
        "summary = summarize_json(data)\n",
        "print(json.dumps(summary, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E07T0bLrl9Tv",
        "outputId": "03842a0b-61ac-4309-ad12-4d224241efdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"OASIS\",\n",
            "  \"release\": \"1.1\",\n",
            "  \"description\": \"OASIS task of Learn2Reg Dataset. Please see https://learn2reg.grand-challenge.org/ for more information. These data were prepared by Andrew Hoopes and Adrian V. Dalca for the following HyperMorph paper. If you use this collection please cite the following and refer to the OASIS Data Use Agreement. \",\n",
            "  \"licence\": \"Open Access Series of Imaging Studies (OASIS): Cross-Sectional MRI Data in Young, Middle Aged, Nondemented, and Demented Older Adults. Marcus DS, Wang TH, Parker J, Csernansky JG, Morris JC, Buckner RL. Journal of Cognitive Neuroscience, 19, 1498-1507.\",\n",
            "  \"reference\": \"\",\n",
            "  \"pairings\": \"unpaired\",\n",
            "  \"provided_data\": {\n",
            "    \"0\": {\n",
            "      \"count\": 3,\n",
            "      \"example\": \"image\"\n",
            "    }\n",
            "  },\n",
            "  \"registration_direction\": {\n",
            "    \"fixed\": 0,\n",
            "    \"moving\": 0\n",
            "  },\n",
            "  \"modality\": {\n",
            "    \"0\": \"MR\"\n",
            "  },\n",
            "  \"img_shift\": {\n",
            "    \"fixed\": \"Patient A\",\n",
            "    \"moving\": \"Patient B\"\n",
            "  },\n",
            "  \"labels\": {\n",
            "    \"0\": {}\n",
            "  },\n",
            "  \"tensorImageSize\": {\n",
            "    \"0\": \"3D\"\n",
            "  },\n",
            "  \"tensorImageShape\": {\n",
            "    \"0\": {\n",
            "      \"count\": 3,\n",
            "      \"example\": 160\n",
            "    }\n",
            "  },\n",
            "  \"numTraining\": 414,\n",
            "  \"numTest\": 39,\n",
            "  \"training\": {\n",
            "    \"count\": 414,\n",
            "    \"example\": {\n",
            "      \"image\": \"OASIS/imagesTr/OASIS_0001_0000.nii.gz\",\n",
            "      \"label\": \"OASIS/labelsTr/OASIS_0001_0000.nii.gz\",\n",
            "      \"mask\": \"OASIS/masksTr/OASIS_0001_0000.nii.gz\"\n",
            "    }\n",
            "  },\n",
            "  \"test\": {\n",
            "    \"count\": 39,\n",
            "    \"example\": {\n",
            "      \"image\": \"OASIS/imagesTs/OASIS_0415_0000.csv\",\n",
            "      \"label\": \"OASIS/labelsTs/OASIS_0415_0000.csv\",\n",
            "      \"mask\": \"OASIS/masksTs/OASIS_0415_0000.csv\"\n",
            "    }\n",
            "  },\n",
            "  \"numPairedTraining\": 0,\n",
            "  \"training_paired_images\": {\n",
            "    \"count\": 0,\n",
            "    \"example\": null\n",
            "  },\n",
            "  \"numPairedTest\": 0,\n",
            "  \"test_paired_images\": {\n",
            "    \"count\": 0,\n",
            "    \"example\": null\n",
            "  },\n",
            "  \"numRegistration_val\": 19,\n",
            "  \"registration_val\": {\n",
            "    \"count\": 19,\n",
            "    \"example\": {\n",
            "      \"fixed\": \"OASIS/imagesTr/OASIS_0395_0000.nii.gz\",\n",
            "      \"moving\": \"OASIS/imagesTr/OASIS_0396_0000.nii.gz\"\n",
            "    }\n",
            "  },\n",
            "  \"numRegistration_test\": 38,\n",
            "  \"registration_test\": {\n",
            "    \"count\": 38,\n",
            "    \"example\": {\n",
            "      \"fixed\": \"OASIS/imagesTs/OASIS_0415_0000.nii.gz\",\n",
            "      \"moving\": \"OASIS/imagesTs/OASIS_0416_0000.nii.gz\"\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mzSHxEK5y7Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Import Necessary Modules\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import yaml\n",
        "import glob\n",
        "import pickle\n",
        "import random\n",
        "import json\n",
        "from functools import reduce\n",
        "from typing import Tuple, Dict, Any, List, Set, Optional, Union, Callable, Type\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import lightning as L\n",
        "from lightning import LightningModule, Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "from einops import rearrange\n",
        "import timm\n",
        "import wandb\n",
        "import monai\n",
        "import nibabel as nib\n",
        "import warnings\n",
        "import monai.transforms as transforms\n",
        "import logging\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "wandb.login()\n",
        "wandb_logger = WandbLogger(project=\"hvit_test2\")  # Replace with your project name\n",
        "\n",
        "# 3. Define Utility Classes and Functions\n",
        "# 3.1. Logger Class (Ensure only this definition exists)\n",
        "import logging\n",
        "import os\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        # Create handlers\n",
        "        console_handler = logging.StreamHandler()\n",
        "\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        file_handler = logging.FileHandler(os.path.join(save_dir, \"logfile.log\"))\n",
        "\n",
        "        # Create formatters and add to handlers\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        console_handler.setFormatter(formatter)\n",
        "        file_handler.setFormatter(formatter)\n",
        "\n",
        "        # Add handlers to the logger\n",
        "        self.logger.addHandler(console_handler)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def info(self, message):\n",
        "        self.logger.info(message)\n",
        "\n",
        "    def warning(self, message):\n",
        "        self.logger.warning(message)\n",
        "\n",
        "    def error(self, message):\n",
        "        self.logger.error(message)\n",
        "\n",
        "    def debug(self, message):\n",
        "        self.logger.debug(message)\n",
        "\n",
        "# 3.2. Utility Functions\n",
        "def read_yaml_file(file_path):\n",
        "    \"\"\"\n",
        "    Reads a YAML file and returns the content as a dictionary.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        try:\n",
        "            content = yaml.safe_load(file)\n",
        "            return content\n",
        "        except yaml.YAMLError as e:\n",
        "            print(f\"Error reading YAML file: {e}\")\n",
        "            return None\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def get_one_hot(inp_seg, num_labels):\n",
        "    B, C, H, W, D = inp_seg.shape\n",
        "    inp_onehot = nn.functional.one_hot(inp_seg.long(), num_classes=num_labels)\n",
        "    inp_onehot = inp_onehot.squeeze(dim=1)\n",
        "    inp_onehot = inp_onehot.permute(0, 4, 1, 2, 3).contiguous()\n",
        "    return inp_onehot\n",
        "\n",
        "def DiceScore(y_pred, y_true, num_class):\n",
        "    y_true = nn.functional.one_hot(y_true, num_classes=num_class)\n",
        "    y_true = torch.squeeze(y_true, 1)\n",
        "    y_true = y_true.permute(0, 4, 1, 2, 3).contiguous()\n",
        "    intersection = y_pred * y_true\n",
        "    intersection = intersection.sum(dim=[2, 3, 4])\n",
        "    union = torch.pow(y_pred, 2).sum(dim=[2, 3, 4]) + torch.pow(y_true, 2).sum(dim=[2, 3, 4])\n",
        "    dsc = (2.*intersection) / (union + 1e-5)\n",
        "    return dsc\n",
        "\n",
        "# 3.3. Loss Functions\n",
        "class Grad3D(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    N-D gradient loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, penalty='l1', loss_mult=None):\n",
        "        super().__init__()\n",
        "        self.penalty = penalty\n",
        "        self.loss_mult = loss_mult\n",
        "\n",
        "    def forward(self, y_pred):\n",
        "        dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])\n",
        "        dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])\n",
        "        dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])\n",
        "\n",
        "        if self.penalty == 'l2':\n",
        "            dy = dy * dy\n",
        "            dx = dx * dx\n",
        "            dz = dz * dz\n",
        "\n",
        "        d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)\n",
        "        grad = d / 3.0\n",
        "\n",
        "        if self.loss_mult is not None:\n",
        "            grad *= self.loss_mult\n",
        "        return grad\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"Dice loss\"\"\"\n",
        "    def __init__(self, num_class=36):\n",
        "        super().__init__()\n",
        "        self.num_class = num_class\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_true = nn.functional.one_hot(y_true, num_classes=self.num_class)\n",
        "        y_true = torch.squeeze(y_true, 1)\n",
        "        y_true = y_true.permute(0, 4, 1, 2, 3).contiguous()\n",
        "        intersection = y_pred * y_true\n",
        "        intersection = intersection.sum(dim=[2, 3, 4])\n",
        "        union = torch.pow(y_pred, 2).sum(dim=[2, 3, 4]) + torch.pow(y_true, 2).sum(dim=[2, 3, 4])\n",
        "        dsc = (2.*intersection) / (union + 1e-5)\n",
        "        dsc_loss = (1-torch.mean(dsc))\n",
        "        return dsc_loss\n",
        "\n",
        "loss_functions = {\n",
        "    \"mse\": nn.MSELoss(),\n",
        "    \"dice\": DiceLoss(num_class=36),\n",
        "    \"grad\": Grad3D(penalty='l2')\n",
        "}\n",
        "\n",
        "# 4. Define the Dataset Class\n",
        "class OASIS_Dataset(Dataset):\n",
        "    def __init__(self, json_path, mode=\"training\", input_dim=(128, 128, 128), is_pair=False):\n",
        "        self.input_dim = input_dim\n",
        "        self.is_pair = is_pair\n",
        "\n",
        "        # Load JSON data\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if mode == \"training\":\n",
        "            self.samples = data[\"training\"]\n",
        "        elif mode == \"test\":\n",
        "            self.samples = data[\"test\"]\n",
        "        elif mode == \"registration_val\":\n",
        "            self.samples = data[\"registration_val\"]\n",
        "        elif mode == \"registration_test\":\n",
        "            self.samples = data[\"registration_test\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "        self.transforms_image = transforms.Resize(spatial_size=input_dim)\n",
        "        self.transforms_mask = transforms.Resize(spatial_size=input_dim, mode=\"nearest\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_pair:\n",
        "            # Load fixed and moving images for registration\n",
        "            sample = self.samples[index]\n",
        "            fixed_path = sample[\"fixed\"]\n",
        "            moving_path = sample[\"moving\"]\n",
        "\n",
        "            # Load images\n",
        "            fixed = nib.load(fixed_path).get_fdata()\n",
        "            moving = nib.load(moving_path).get_fdata()\n",
        "\n",
        "            # Apply transforms\n",
        "            fixed = self.transforms_image(torch.from_numpy(fixed).unsqueeze(0).float())\n",
        "            moving = self.transforms_image(torch.from_numpy(moving).unsqueeze(0).float())\n",
        "\n",
        "            # Load segmentation masks if available\n",
        "            fixed_mask_path = sample.get(\"fixed_mask\", None)\n",
        "            moving_mask_path = sample.get(\"moving_mask\", None)\n",
        "\n",
        "            if fixed_mask_path and moving_mask_path:\n",
        "                fixed_mask = nib.load(fixed_mask_path).get_fdata()\n",
        "                moving_mask = nib.load(moving_mask_path).get_fdata()\n",
        "\n",
        "                fixed_mask = self.transforms_mask(torch.from_numpy(fixed_mask).unsqueeze(0).long())\n",
        "                moving_mask = self.transforms_mask(torch.from_numpy(moving_mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                # If masks are not available, return dummy masks\n",
        "                fixed_mask = torch.zeros_like(fixed)\n",
        "                moving_mask = torch.zeros_like(moving)\n",
        "\n",
        "            return fixed, moving, fixed_mask, moving_mask\n",
        "        else:\n",
        "            # Load two random images for unpaired registration\n",
        "            selected_samples = random.sample(self.samples, 2)\n",
        "\n",
        "            # Load source image\n",
        "            src_sample = selected_samples[0]\n",
        "            src_path = src_sample[\"image\"]\n",
        "            src = nib.load(src_path).get_fdata()\n",
        "            src = self.transforms_image(torch.from_numpy(src).unsqueeze(0).float())\n",
        "\n",
        "            # Load source mask if available\n",
        "            src_mask_path = src_sample.get(\"mask\", None)\n",
        "            if src_mask_path:\n",
        "                src_mask = nib.load(src_mask_path).get_fdata()\n",
        "                src_mask = self.transforms_mask(torch.from_numpy(src_mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                src_mask = torch.zeros_like(src)\n",
        "\n",
        "            # Load target image\n",
        "            tgt_sample = selected_samples[1]\n",
        "            tgt_path = tgt_sample[\"image\"]\n",
        "            tgt = nib.load(tgt_path).get_fdata()\n",
        "            tgt = self.transforms_image(torch.from_numpy(tgt).unsqueeze(0).float())\n",
        "\n",
        "            # Load target mask if available\n",
        "            tgt_mask_path = tgt_sample.get(\"mask\", None)\n",
        "            if tgt_mask_path:\n",
        "                tgt_mask = nib.load(tgt_mask_path).get_fdata()\n",
        "                tgt_mask = self.transforms_mask(torch.from_numpy(tgt_mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                tgt_mask = torch.zeros_like(tgt)\n",
        "\n",
        "            return src, tgt, src_mask, tgt_mask\n",
        "\n",
        "# 5. Define the Model Components\n",
        "# (Assuming all model classes are defined correctly as per your initial code)\n",
        "\n",
        "# 6. Define the LightningModule\n",
        "class LiTHViT(LightningModule):\n",
        "    def __init__(self, args, config, wandb_logger=None, save_model_every_n_epochs=10):\n",
        "        super().__init__()\n",
        "        self.automatic_optimization = False\n",
        "        self.args = args\n",
        "        self.config = config\n",
        "        self.best_val_loss = 1e8\n",
        "        self.save_model_every_n_epochs = save_model_every_n_epochs\n",
        "        self.lr = args.lr\n",
        "        self.last_epoch = 0\n",
        "        self.tgt2src_reg = args.tgt2src_reg\n",
        "        self.hvit_light = args.hvit_light\n",
        "        self.precision = args.precision\n",
        "\n",
        "        # Initialize logger\n",
        "        self.custom_logger = Logger(save_dir=\"./logs\")\n",
        "\n",
        "        self.hvit = HierarchicalViT_Light(config) if self.hvit_light else HierarchicalViT(config)\n",
        "\n",
        "        self.loss_weights = {\n",
        "            \"mse\": self.args.mse_weights,\n",
        "            \"dice\": self.args.dice_weights,\n",
        "            \"grad\": self.args.grad_weights\n",
        "        }\n",
        "        self.wandb_logger = wandb_logger\n",
        "        self.test_step_outputs = []\n",
        "\n",
        "    def _forward(self, batch, calc_score: bool = False, tgt2src_reg: bool = False):\n",
        "        _loss = {}\n",
        "        _score = 0.\n",
        "\n",
        "        dtype_map = {\n",
        "            'bf16': torch.bfloat16,\n",
        "            'fp32': torch.float32,\n",
        "            'fp16': torch.float16\n",
        "        }\n",
        "        dtype_ = dtype_map.get(self.precision, torch.float32)\n",
        "\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=dtype_):\n",
        "            if tgt2src_reg:\n",
        "                target, source = batch[0].to(dtype=dtype_), batch[1].to(dtype=dtype_)\n",
        "                tgt_seg, src_seg = batch[2], batch[3]\n",
        "            else:\n",
        "                source, target = batch[0].to(dtype=dtype_), batch[1].to(dtype=dtype_)\n",
        "                src_seg, tgt_seg = batch[2], batch[3]\n",
        "\n",
        "            moved, flow = self.hvit(source, target)\n",
        "\n",
        "            if calc_score:\n",
        "                moved_seg = self._get_one_hot_from_src(src_seg, flow, self.args.num_labels)\n",
        "                _score = DiceScore(moved_seg, tgt_seg.long(), self.args.num_labels)\n",
        "\n",
        "            _loss = {}\n",
        "            for key, weight in self.loss_weights.items():\n",
        "                if key == \"mse\":\n",
        "                    _loss[key] = weight * loss_functions[key](moved, target)\n",
        "                elif key == \"dice\":\n",
        "                    moved_seg = self._get_one_hot_from_src(src_seg, flow, self.args.num_labels)\n",
        "                    _loss[key] = weight * loss_functions[key](moved_seg, tgt_seg.long())\n",
        "                elif key == \"grad\":\n",
        "                    _loss[key] = weight * loss_functions[key](flow)\n",
        "\n",
        "            _loss[\"avg_loss\"] = sum(_loss.values()) / len(_loss)\n",
        "        return _loss, _score\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.hvit.train()\n",
        "        opt = self.optimizers()\n",
        "\n",
        "        loss1, _ = self._forward(batch, calc_score=False)\n",
        "        self.manual_backward(loss1[\"avg_loss\"])\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if self.tgt2src_reg:\n",
        "            loss2, _ = self._forward(batch, tgt2src_reg=True, calc_score=False)\n",
        "            self.manual_backward(loss2[\"avg_loss\"])\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        total_loss = {\n",
        "            key: (loss1[key].item() + loss2[key].item()) / 2 if self.tgt2src_reg and key in loss2 else loss1[key].item()\n",
        "            for key in loss1.keys()\n",
        "        }\n",
        "\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics(total_loss, step=self.global_step)\n",
        "        self.custom_logger.info(f\"Batch {batch_idx} - Loss: {total_loss}\")\n",
        "        return total_loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        if self.current_epoch % self.save_model_every_n_epochs == 0:\n",
        "            checkpoints_dir = f\"./checkpoints/{self.current_epoch}\"\n",
        "            os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "            checkpoint_path = f\"{checkpoints_dir}/model_epoch_{self.current_epoch}.ckpt\"\n",
        "            self.trainer.save_checkpoint(checkpoint_path)\n",
        "            self.custom_logger.info(f\"Saved model at epoch {self.current_epoch}\")  # Use custom_logger\n",
        "\n",
        "        current_lr = self.optimizers().param_groups[0]['lr']\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"learning_rate\": current_lr}, step=self.global_step)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        with torch.no_grad():\n",
        "            self.hvit.eval()\n",
        "            _loss, _score = self._forward(batch, calc_score=True)\n",
        "\n",
        "        # Log each component of the validation loss\n",
        "        for loss_name, loss_value in _loss.items():\n",
        "            self.log(f\"val_{loss_name}\", loss_value, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        # Log the mean validation score if available\n",
        "        if _score is not None:\n",
        "            self.log(\"val_score\", _score.mean(), on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        # Log to wandb\n",
        "        if self.wandb_logger:\n",
        "            log_dict = {f\"val_{k}\": v.item() for k, v in _loss.items()}\n",
        "            log_dict.update({\n",
        "                \"val_score_mean\": _score.mean().item() if _score is not None else None,\n",
        "            })\n",
        "            self.wandb_logger.log_metrics({k: v for k, v in log_dict.items() if v is not None}, step=self.global_step)\n",
        "\n",
        "        return {\"val_loss\": _loss[\"avg_loss\"], \"val_score\": _score.mean().item()}\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Callback method called at the end of the validation epoch.\n",
        "        Saves the best model based on validation loss and logs metrics.\n",
        "        \"\"\"\n",
        "        val_loss = self.trainer.callback_metrics.get(\"val_avg_loss\")\n",
        "        if val_loss is not None and self.current_epoch > 0:\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                checkpoints_dir = f\"./checkpoints/{self.current_epoch}\"\n",
        "                os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "                best_model_path = f\"{checkpoints_dir}/best_model.ckpt\"\n",
        "                self.trainer.save_checkpoint(best_model_path)\n",
        "                if self.wandb_logger:\n",
        "                    self.wandb_logger.experiment.log({\n",
        "                        \"best_model_saved\": best_model_path,\n",
        "                        \"best_val_loss\": self.best_val_loss.item()\n",
        "                    })\n",
        "                self.custom_logger.info(f\"New best model saved with validation loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Performs a single test step on a batch of data.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.hvit.eval()\n",
        "            _, _score = self._forward(batch, calc_score=True)\n",
        "\n",
        "        _score = _score.mean() if isinstance(_score, torch.Tensor) else torch.tensor(_score).mean()\n",
        "\n",
        "        self.test_step_outputs.append(_score)\n",
        "\n",
        "        # Log to wandb only if the logger is available\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"test_dice\": _score.item()}, step=self.global_step)\n",
        "\n",
        "        # Return as a dict with tensor values\n",
        "        return {\"test_dice\": _score}\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Callback method called at the end of the test epoch.\n",
        "        Computes and logs the average test Dice score.\n",
        "        \"\"\"\n",
        "        # Calculate the average Dice score across all test steps\n",
        "        avg_test_dice = torch.stack(self.test_step_outputs).mean()\n",
        "\n",
        "        # Log the average test Dice score\n",
        "        self.log(\"avg_test_dice\", avg_test_dice, prog_bar=True)\n",
        "\n",
        "        # Log to wandb if available\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"total_test_dice_avg\": avg_test_dice.item()})\n",
        "\n",
        "        # Clear the test step outputs list for the next test epoch\n",
        "        self.test_step_outputs.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configures the optimizer and learning rate scheduler for the model.\n",
        "        \"\"\"\n",
        "        optimizer = torch.optim.Adam(self.hvit.parameters(), lr=self.lr, weight_decay=0, amsgrad=True)\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=self.lr_lambda)\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"epoch\",\n",
        "                \"frequency\": 1,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def lr_lambda(self, epoch):\n",
        "        \"\"\"\n",
        "        Defines the learning rate schedule.\n",
        "        \"\"\"\n",
        "        return math.pow(1 - epoch / self.trainer.max_epochs, 0.9)\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_checkpoint(cls, checkpoint_path, args=None, wandb_logger=None):\n",
        "        \"\"\"\n",
        "        Loads a model from a checkpoint file.\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "        args = args or checkpoint.get('hyper_parameters', {}).get('args')\n",
        "        config = checkpoint.get('hyper_parameters', {}).get('config')\n",
        "\n",
        "        model = cls(args, config, wandb_logger)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "        if 'hyper_parameters' in checkpoint:\n",
        "            hyper_params = checkpoint['hyper_parameters']\n",
        "            for attr in ['lr', 'best_val_loss', 'last_epoch']:\n",
        "                setattr(model, attr, hyper_params.get(attr, getattr(model, attr)))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        \"\"\"\n",
        "        Callback to save additional information in the checkpoint.\n",
        "        \"\"\"\n",
        "        checkpoint['hyper_parameters'] = {\n",
        "            'config': self.config,\n",
        "            'lr': self.lr,\n",
        "            'best_val_loss': self.best_val_loss,\n",
        "            'last_epoch': self.current_epoch\n",
        "        }\n",
        "\n",
        "    def _get_one_hot_from_src(self, src_seg, flow, num_labels):\n",
        "        \"\"\"\n",
        "        Converts source segmentation to one-hot encoding and applies deformation.\n",
        "        \"\"\"\n",
        "        src_seg_onehot = get_one_hot(src_seg, self.args.num_labels)\n",
        "        deformed_segs = [\n",
        "            self.hvit.spatial_trans(src_seg_onehot[:, i:i+1, ...].float(), flow.float())\n",
        "            for i in range(num_labels)\n",
        "        ]\n",
        "        return torch.cat(deformed_segs, dim=1)\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize the dataset and dataloaders\n",
        "\n",
        "# For training (unpaired data, paired on-the-fly)\n",
        "train_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"training\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=False  # Set to False to enable on-the-fly pairing\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    num_workers=1  # Adjust based on your system\n",
        ")\n",
        "\n",
        "# For validation\n",
        "val_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"registration_val\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=True  # Set to True for validation paired data\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=1\n",
        ")\n",
        "# Define training arguments\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.lr = 0.001\n",
        "        self.mse_weights = 1.0\n",
        "        self.dice_weights = 1.0\n",
        "        self.grad_weights = 1.0\n",
        "        self.tgt2src_reg = False\n",
        "        self.hvit_light = True\n",
        "        self.precision = 'fp32'  # Training precision (e.g., 'bf16', 'fp16', 'fp32')\n",
        "        self.num_labels = 36  # Update based on your dataset\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# 3. Define configuration\n",
        "config = {\n",
        "    'WO_SELF_ATT': False,\n",
        "    '_NUM_CROSS_ATT': -1,\n",
        "    'out_fmaps': ['P4', 'P3', 'P2', 'P1'],  # Number of levels = 4\n",
        "    'scale_level_df': 'P1',\n",
        "    'upsample_df': True,\n",
        "    'upsample_scale_factor': 2,\n",
        "    'fpn_channels': 64,\n",
        "    'start_channels': 32,\n",
        "    'patch_size': [2, 2, 2, 2],  # Matches number of levels\n",
        "    'backbone_net': 'fpn',\n",
        "    'in_channels': 1,\n",
        "\n",
        "    # **Debugged Lines: Update 'data_size' and add 'img_size' to match input_dim**\n",
        "    'data_size': [128, 128, 128],  # Updated from [40, 48, 56]\n",
        "    'img_size': [128, 128, 128],   # Added to align with input_dim\n",
        "\n",
        "    'bias': True,\n",
        "    'norm_type': 'instance',\n",
        "    'kernel_size': 3,\n",
        "    'depths': [1, 1, 1, 1],  # Matches number of levels\n",
        "    'mlp_ratio': 2,\n",
        "    'num_heads': [4, 8, 16, 32],  # Matches number of levels\n",
        "    'drop_path_rate': 0.,\n",
        "    'qkv_bias': True,\n",
        "    'drop_rate': 0.,\n",
        "    'attn_drop_rate': 0.,\n",
        "    'use_seg_loss': False,\n",
        "    'use_seg_proxy_loss': False,\n",
        "    'num_organs': 36,  # Updated to match DiceLoss\n",
        "}\n",
        "# Initialize WandB logger (optional, replace with None if not using WandB)\n",
        "wandb_logger = WandbLogger(project=\"hvit_test\")  # Replace with your project name\n",
        "\n",
        "# Instantiate the Lightning module\n",
        "lit_model = LiTHViT(args, config, wandb_logger=wandb_logger)\n",
        "\n",
        "# Define the PyTorch Lightning Trainer\n",
        "trainer = Trainer(\n",
        "    max_epochs=5,  # Number of epochs\n",
        "    logger=wandb_logger,  # Log training metrics\n",
        "    enable_checkpointing=False,  # Disable checkpointing for testing\n",
        "    devices=1,  # Number of GPUs (set to 0 for CPU)\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",  # Use GPU if available\n",
        "    precision=16 if args.precision == 'fp16' else 32  # Set precision\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.fit(lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2cb50298ddeb4710829e023d04b50ea9",
            "29c70c47e19b4bfea1bdf943f87cb7c3",
            "68e5ef4e452b4ccea5eecc9faa5d2dea",
            "0ceeccb5a742467a97e68573aeef2920",
            "2b4efe4b0d1d4e4c95de7b5bf6867177",
            "8c20936cd9574d598c29a0846a370881",
            "6b2610c39ecd41a7ba25ebe9b42816d6",
            "a7a09b287180453984154055e3f07bba",
            "24b67787b1c7409f8a000600ed2569a7",
            "21d9bdfdd3194b2b83684081afadd8d1",
            "40ce786947e4477b9bb806e10cb7378e",
            "0d19bc550b60450abb51084eb372fda7",
            "a3d921539a6f4c9f8afe53c8580b32b7",
            "c205ff7890f346079fbc8d70c0af07c8",
            "dd9b6de80b6349e782ef345456001b61",
            "3a0c2fd0d9784bcd8dd669e4e2f15727",
            "7ab87cf7b9ce4e44b7444bebbd901d73",
            "0e21705b16734601a0e8f4be95b4f556",
            "b53b7d5516ee4842a1405455e5865e39",
            "20a7d6d275574306af18f6a3c1f71a0b",
            "bf9094e614f7426dbec4e419ceb40a32",
            "53207ea3465042c6a96b82d4d6dca0ef"
          ]
        },
        "id": "x47GIP0GJwFY",
        "outputId": "adfe3e68-1d0a-44d0-d8d2-4e35766ef338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malim98barnet\u001b[0m (\u001b[33malim98barnet-university-of-tehran\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20250110_130647-ug937y0f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_test/runs/ug937y0f' target=\"_blank\">giddy-durian-1</a></strong> to <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_test' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/hvit_test</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_test/runs/ug937y0f' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/hvit_test/runs/ug937y0f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name | Type                  | Params | Mode \n",
            "-------------------------------------------------------\n",
            "0 | hvit | HierarchicalViT_Light | 17.9 M | train\n",
            "-------------------------------------------------------\n",
            "17.9 M    Trainable params\n",
            "0         Non-trainable params\n",
            "17.9 M    Total params\n",
            "71.731    Total estimated model params size (MB)\n",
            "244       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name | Type                  | Params | Mode \n",
            "-------------------------------------------------------\n",
            "0 | hvit | HierarchicalViT_Light | 17.9 M | train\n",
            "-------------------------------------------------------\n",
            "17.9 M    Trainable params\n",
            "0         Non-trainable params\n",
            "17.9 M    Total params\n",
            "71.731    Total estimated model params size (MB)\n",
            "244       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cb50298ddeb4710829e023d04b50ea9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d19bc550b60450abb51084eb372fda7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-10 13:06:59,021 - __main__ - INFO - Batch 0 - Loss: {'mse': 0.003875497728586197, 'dice': 0.9468063712120056, 'grad': 1.6333691732484112e-08, 'avg_loss': 0.31689396500587463}\n",
            "INFO:__main__:Batch 0 - Loss: {'mse': 0.003875497728586197, 'dice': 0.9468063712120056, 'grad': 1.6333691732484112e-08, 'avg_loss': 0.31689396500587463}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-10 13:07:01,549 - __main__ - INFO - Batch 1 - Loss: {'mse': 0.0029408661648631096, 'dice': 0.9466022253036499, 'grad': 0.0011872303439304233, 'avg_loss': 0.31691011786460876}\n",
            "INFO:__main__:Batch 1 - Loss: {'mse': 0.0029408661648631096, 'dice': 0.9466022253036499, 'grad': 0.0011872303439304233, 'avg_loss': 0.31691011786460876}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-10 13:07:04,097 - __main__ - INFO - Batch 2 - Loss: {'mse': 0.004398035351186991, 'dice': 0.9476045370101929, 'grad': 0.00596154248341918, 'avg_loss': 0.3193213939666748}\n",
            "INFO:__main__:Batch 2 - Loss: {'mse': 0.004398035351186991, 'dice': 0.9476045370101929, 'grad': 0.00596154248341918, 'avg_loss': 0.3193213939666748}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-10 13:07:06,634 - __main__ - INFO - Batch 3 - Loss: {'mse': 0.004532233811914921, 'dice': 0.9477837681770325, 'grad': 0.006855541840195656, 'avg_loss': 0.31972384452819824}\n",
            "INFO:__main__:Batch 3 - Loss: {'mse': 0.004532233811914921, 'dice': 0.9477837681770325, 'grad': 0.006855541840195656, 'avg_loss': 0.31972384452819824}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 16, 16, 16])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 32, 32, 32])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n",
            "MLP input shape after permute: torch.Size([1, 64, 64, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: \n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
            "INFO:lightning.pytorch.utilities.rank_zero:\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/manual.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# no loop to break at this level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/manual.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# manually capture logged metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m  \u001b[0;31m# release the batch from memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-d4937ae2b2fd>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         total_loss = {\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt2src_reg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-d4937ae2b2fd>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    335\u001b[0m         total_loss = {\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt2src_reg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/data/meta_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;31m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_default_nowrap_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d4937ae2b2fd>\u001b[0m in \u001b[0;36m<cell line: 593>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full"
      ],
      "metadata": {
        "id": "h9Cht5z3_bWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import json\n",
        "# import os\n",
        "\n",
        "# json_path = \"OASIS/OASIS_dataset.json\"\n",
        "# with open(json_path, \"r\") as f:\n",
        "#     data = json.load(f)\n",
        "\n",
        "# # Update paths dynamically\n",
        "# base_path = \"OASIS\"\n",
        "# for entry in data[\"training\"]:\n",
        "#     entry[\"image\"] = os.path.join(base_path, entry[\"image\"].lstrip(\"./\"))\n",
        "#     entry[\"label\"] = os.path.join(base_path, entry[\"label\"].lstrip(\"./\"))\n",
        "#     entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "# for entry in data[\"test\"]:\n",
        "#     entry[\"image\"] = os.path.join(base_path, entry[\"image\"].lstrip(\"./\"))\n",
        "#     entry[\"label\"] = os.path.join(base_path, entry[\"label\"].lstrip(\"./\"))\n",
        "#     entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "# for entry in data[\"registration_test\"]:\n",
        "#     entry[\"fixed\"] = os.path.join(base_path, entry[\"fixed\"].lstrip(\"./\"))\n",
        "#     entry[\"moving\"] = os.path.join(base_path, entry[\"moving\"].lstrip(\"./\"))\n",
        "#     # entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "# for entry in data[\"registration_val\"]:\n",
        "#     entry[\"fixed\"] = os.path.join(base_path, entry[\"fixed\"].lstrip(\"./\"))\n",
        "#     entry[\"moving\"] = os.path.join(base_path, entry[\"moving\"].lstrip(\"./\"))\n",
        "#     # entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "\n",
        "# # Save updated JSON\n",
        "# with open(json_path, \"w\") as f:\n",
        "#     json.dump(data, f, indent=4)"
      ],
      "metadata": {
        "id": "0O1Yj0slBxp4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## new oasis dataset test"
      ],
      "metadata": {
        "id": "-tjK_Go1Bxp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from monai.transforms import Resize\n",
        "\n",
        "class OASIS_Dataset(Dataset):\n",
        "    def __init__(self, json_path, mode=\"training\", input_dim=(128, 128, 128), is_pair=False):\n",
        "        self.input_dim = input_dim\n",
        "        self.is_pair = is_pair\n",
        "\n",
        "        # Load JSON data\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if mode == \"training\":\n",
        "            self.samples = data[\"training\"]\n",
        "        elif mode == \"test\":\n",
        "            self.samples = data[\"test\"]\n",
        "        elif mode == \"registration_val\":\n",
        "            self.samples = data[\"registration_val\"]\n",
        "        elif mode == \"registration_test\":\n",
        "            self.samples = data[\"registration_test\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "        self.transforms_image = Resize(spatial_size=input_dim)\n",
        "        self.transforms_mask = Resize(spatial_size=input_dim, mode=\"nearest\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_pair:\n",
        "            # Load fixed and moving images for registration\n",
        "            sample = self.samples[index]\n",
        "            fixed_path = sample[\"fixed\"]\n",
        "            moving_path = sample[\"moving\"]\n",
        "\n",
        "            fixed = nib.load(fixed_path).get_fdata()\n",
        "            moving = nib.load(moving_path).get_fdata()\n",
        "\n",
        "            fixed = self.transforms_image(torch.from_numpy(fixed).unsqueeze(0).float())\n",
        "            moving = self.transforms_image(torch.from_numpy(moving).unsqueeze(0).float())\n",
        "\n",
        "            # Load segmentation masks if available\n",
        "            # Assuming mask paths are provided; adjust accordingly\n",
        "            fixed_mask_path = sample.get(\"fixed_mask\", None)\n",
        "            moving_mask_path = sample.get(\"moving_mask\", None)\n",
        "\n",
        "            if fixed_mask_path and moving_mask_path:\n",
        "                fixed_mask = nib.load(fixed_mask_path).get_fdata()\n",
        "                moving_mask = nib.load(moving_mask_path).get_fdata()\n",
        "\n",
        "                fixed_mask = self.transforms_mask(torch.from_numpy(fixed_mask).unsqueeze(0).long())\n",
        "                moving_mask = self.transforms_mask(torch.from_numpy(moving_mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                # If masks are not available, return dummy masks\n",
        "                fixed_mask = torch.zeros_like(fixed)\n",
        "                moving_mask = torch.zeros_like(moving)\n",
        "\n",
        "            return fixed, moving, fixed_mask, moving_mask\n",
        "        else:\n",
        "            # Load unpaired data (image, label, mask)\n",
        "            sample = self.samples[index]\n",
        "            image_path = sample[\"image\"]\n",
        "            label_path = sample[\"label\"]\n",
        "            mask_path = sample.get(\"mask\", None)\n",
        "\n",
        "            image = nib.load(image_path).get_fdata()\n",
        "            label = nib.load(label_path).get_fdata()\n",
        "\n",
        "            image = self.transforms_image(torch.from_numpy(image).unsqueeze(0).float())\n",
        "            label = self.transforms_mask(torch.from_numpy(label).unsqueeze(0).long())\n",
        "\n",
        "            if mask_path:\n",
        "                mask = nib.load(mask_path).get_fdata()\n",
        "                mask = self.transforms_mask(torch.from_numpy(mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                mask = torch.zeros_like(label)\n",
        "\n",
        "            return image, label, mask\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# For training (paired data)\n",
        "train_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"training\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=True  # Set to True for paired data\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    num_workers=4  # Adjust based on your system\n",
        ")\n",
        "\n",
        "# For validation\n",
        "val_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"registration_val\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=True  # Set to True for paired data\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n"
      ],
      "metadata": {
        "id": "JsDtw7VcBxp5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class OASIS_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for OASIS Registration Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, json_path, mode=\"training\", input_dim=(128, 128, 128), is_pair=False):\n",
        "        self.input_dim = input_dim\n",
        "        self.is_pair = is_pair\n",
        "\n",
        "        # Load JSON data\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if mode == \"training\":\n",
        "            self.samples = data[\"training\"]\n",
        "        elif mode == \"test\":\n",
        "            self.samples = data[\"test\"]\n",
        "        elif mode == \"registration_val\":\n",
        "            self.samples = data[\"registration_val\"]\n",
        "        elif mode == \"registration_test\":\n",
        "            self.samples = data[\"registration_test\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "        self.transforms_image = Resize(spatial_size=input_dim)\n",
        "        self.transforms_mask = Resize(spatial_size=input_dim, mode=\"nearest\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_pair:\n",
        "            # Load fixed and moving images for registration\n",
        "            sample = self.samples[index]\n",
        "            fixed_path = sample[\"fixed\"]\n",
        "            moving_path = sample[\"moving\"]\n",
        "\n",
        "            fixed = nib.load(fixed_path).get_fdata()\n",
        "            moving = nib.load(moving_path).get_fdata()\n",
        "\n",
        "            fixed = self.transforms_image(torch.from_numpy(fixed).unsqueeze(0).float())\n",
        "            moving = self.transforms_image(torch.from_numpy(moving).unsqueeze(0).float())\n",
        "\n",
        "            # Load segmentation masks if available\n",
        "            fixed_mask_path = sample.get(\"fixed_mask\", None)\n",
        "            moving_mask_path = sample.get(\"moving_mask\", None)\n",
        "\n",
        "            if fixed_mask_path and moving_mask_path:\n",
        "                fixed_mask = nib.load(fixed_mask_path).get_fdata()\n",
        "                moving_mask = nib.load(moving_mask_path).get_fdata()\n",
        "\n",
        "                fixed_mask = self.transforms_mask(torch.from_numpy(fixed_mask).unsqueeze(0).long())\n",
        "                moving_mask = self.transforms_mask(torch.from_numpy(moving_mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                # If masks are not available, return dummy masks\n",
        "                fixed_mask = torch.zeros_like(fixed)\n",
        "                moving_mask = torch.zeros_like(moving)\n",
        "\n",
        "            return fixed, moving, fixed_mask, moving_mask\n",
        "        else:\n",
        "            # Load unpaired data (image, label, mask)\n",
        "            sample = self.samples[index]\n",
        "            image_path = sample[\"image\"]\n",
        "            label_path = sample[\"label\"]\n",
        "            mask_path = sample.get(\"mask\", None)\n",
        "\n",
        "            image = nib.load(image_path).get_fdata()\n",
        "            label = nib.load(label_path).get_fdata()\n",
        "\n",
        "            image = self.transforms_image(torch.from_numpy(image).unsqueeze(0).float())\n",
        "            label = self.transforms_mask(torch.from_numpy(label).unsqueeze(0).long())\n",
        "\n",
        "            if mask_path:\n",
        "                mask = nib.load(mask_path).get_fdata()\n",
        "                mask = self.transforms_mask(torch.from_numpy(mask).unsqueeze(0).long())\n",
        "            else:\n",
        "                mask = torch.zeros_like(label)\n",
        "\n",
        "            return image, label, mask\n",
        "\n",
        "# =======================\n",
        "# 8. Define Training Arguments and Configuration\n",
        "# =======================\n",
        "\n",
        "class Args:\n",
        "    \"\"\"\n",
        "    Training Arguments.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.lr = 0.001\n",
        "        self.mse_weights = 1.0\n",
        "        self.dice_weights = 1.0\n",
        "        self.grad_weights = 1.0\n",
        "        self.tgt2src_reg = False\n",
        "        self.hvit_light = True\n",
        "        self.precision = 'fp32'  # Options: 'bf16', 'fp16', 'fp32'\n",
        "        self.num_labels = 36  # Update based on your dataset\n",
        "\n",
        "args = Args()\n",
        "\n",
        "\n",
        "# =======================\n",
        "# 9. Initialize the Dataset and DataLoaders\n",
        "# =======================\n",
        "\n",
        "# For training (paired data)\n",
        "train_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"training\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=False  # Set to True for paired data\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    num_workers=4  # Adjust based on your system\n",
        ")\n",
        "\n",
        "# For validation\n",
        "val_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"registration_val\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=True  # Set to True for validation paired data\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# For testing (optional)\n",
        "test_dataset = OASIS_Dataset(\n",
        "    json_path=\"OASIS/OASIS_dataset.json\",\n",
        "    mode=\"registration_test\",\n",
        "    input_dim=(128, 128, 128),\n",
        "    is_pair=True  # Set to True for test paired data\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n"
      ],
      "metadata": {
        "id": "QZYcwaE6MuLJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# 1. Import Necessary Libraries\n",
        "# =======================\n",
        "\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import logging\n",
        "from typing import Dict, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import monai\n",
        "from monai.transforms import Resize\n",
        "\n",
        "import nibabel as nib\n",
        "\n",
        "import lightning as L\n",
        "from lightning import LightningModule, Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "from einops import rearrange\n",
        "import timm\n",
        "import wandb\n",
        "\n",
        "# =======================\n",
        "# 2. Setup Logging and WandB\n",
        "# =======================\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.login()\n",
        "wandb_logger = WandbLogger(project=\"hvit_mae_integration\")  # Replace with your project name\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# json_path = \"OASIS/OASIS_dataset.json\"\n",
        "# with open(json_path, \"r\") as f:\n",
        "#     data = json.load(f)\n",
        "\n",
        "# # Update paths dynamically\n",
        "# base_path = \"OASIS\"\n",
        "# for entry in data[\"training\"]:\n",
        "#     entry[\"image\"] = os.path.join(base_path, entry[\"image\"].lstrip(\"./\"))\n",
        "#     entry[\"label\"] = os.path.join(base_path, entry[\"label\"].lstrip(\"./\"))\n",
        "#     entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "# for entry in data[\"test\"]:\n",
        "#     entry[\"image\"] = os.path.join(base_path, entry[\"image\"].lstrip(\"./\"))\n",
        "#     entry[\"label\"] = os.path.join(base_path, entry[\"label\"].lstrip(\"./\"))\n",
        "#     entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "# for entry in data[\"registration_test\"]:\n",
        "#     entry[\"fixed\"] = os.path.join(base_path, entry[\"fixed\"].lstrip(\"./\"))\n",
        "#     entry[\"moving\"] = os.path.join(base_path, entry[\"moving\"].lstrip(\"./\"))\n",
        "#     # entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "# for entry in data[\"registration_val\"]:\n",
        "#     entry[\"fixed\"] = os.path.join(base_path, entry[\"fixed\"].lstrip(\"./\"))\n",
        "#     entry[\"moving\"] = os.path.join(base_path, entry[\"moving\"].lstrip(\"./\"))\n",
        "#     # entry[\"mask\"] = os.path.join(base_path, entry[\"mask\"].lstrip(\"./\"))\n",
        "\n",
        "# # Save updated JSON\n",
        "# with open(json_path, \"w\") as f:\n",
        "#     json.dump(data, f, indent=4)\n",
        "# Custom Logger Class\n",
        "class Logger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        # Create handlers\n",
        "        console_handler = logging.StreamHandler()\n",
        "\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        file_handler = logging.FileHandler(os.path.join(save_dir, \"logfile.log\"))\n",
        "\n",
        "        # Create formatters and add to handlers\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        console_handler.setFormatter(formatter)\n",
        "        file_handler.setFormatter(formatter)\n",
        "\n",
        "        # Add handlers to the logger\n",
        "        self.logger.addHandler(console_handler)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def info(self, message):\n",
        "        self.logger.info(message)\n",
        "\n",
        "    def warning(self, message):\n",
        "        self.logger.warning(message)\n",
        "\n",
        "    def error(self, message):\n",
        "        self.logger.error(message)\n",
        "\n",
        "    def debug(self, message):\n",
        "        self.logger.debug(message)\n",
        "\n",
        "# Initialize Custom Logger\n",
        "custom_logger = Logger(save_dir=\"./logs\")\n",
        "\n",
        "# =======================\n",
        "# 3. Define Utility Functions and Losses\n",
        "# =======================\n",
        "\n",
        "def get_one_hot(inp_seg, num_labels):\n",
        "    \"\"\"\n",
        "    Converts segmentation labels to one-hot encoding.\n",
        "    \"\"\"\n",
        "    inp_onehot = nn.functional.one_hot(inp_seg.long(), num_classes=num_labels)  # (B, C, H, W, D, num_labels)\n",
        "    inp_onehot = torch.squeeze(inp_onehot, 1)  # Remove channel dimension if present\n",
        "    inp_onehot = inp_onehot.permute(0, 5, 1, 2, 3).contiguous()  # (B, num_labels, H, W, D)\n",
        "    return inp_onehot\n",
        "\n",
        "def DiceScore(y_pred, y_true, num_class):\n",
        "    \"\"\"\n",
        "    Computes the Dice Score for multi-class segmentation.\n",
        "    \"\"\"\n",
        "    y_true = get_one_hot(y_true, num_class)  # (B, num_labels, H, W, D)\n",
        "    intersection = y_pred * y_true\n",
        "    intersection = intersection.sum(dim=[2, 3, 4])\n",
        "    union = torch.pow(y_pred, 2).sum(dim=[2, 3, 4]) + torch.pow(y_true, 2).sum(dim=[2, 3, 4])\n",
        "    dsc = (2. * intersection) / (union + 1e-5)\n",
        "    return dsc\n",
        "\n",
        "# Gradient Loss for Regularization\n",
        "class Grad3D(nn.Module):\n",
        "    \"\"\"\n",
        "    N-D gradient loss for smoothness regularization.\n",
        "    \"\"\"\n",
        "    def __init__(self, penalty='l1', loss_mult=None):\n",
        "        super().__init__()\n",
        "        self.penalty = penalty\n",
        "        self.loss_mult = loss_mult\n",
        "\n",
        "    def forward(self, y_pred):\n",
        "        dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])\n",
        "        dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])\n",
        "        dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])\n",
        "\n",
        "        if self.penalty == 'l2':\n",
        "            dy = dy ** 2\n",
        "            dx = dx ** 2\n",
        "            dz = dz ** 2\n",
        "\n",
        "        grad = (dx.mean() + dy.mean() + dz.mean()) / 3.0\n",
        "\n",
        "        if self.loss_mult is not None:\n",
        "            grad *= self.loss_mult\n",
        "        return grad\n",
        "\n",
        "# Dice Loss for Segmentation\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Dice loss for multi-class segmentation.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_class=36):\n",
        "        super().__init__()\n",
        "        self.num_class = num_class\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_true = get_one_hot(y_true, self.num_class)  # (B, num_labels, H, W, D)\n",
        "        intersection = y_pred * y_true\n",
        "        intersection = intersection.sum(dim=[2, 3, 4])\n",
        "        union = torch.pow(y_pred, 2).sum(dim=[2, 3, 4]) + torch.pow(y_true, 2).sum(dim=[2, 3, 4])\n",
        "        dsc = (2. * intersection) / (union + 1e-5)\n",
        "        dsc_loss = 1 - torch.mean(dsc)\n",
        "        return dsc_loss\n",
        "\n",
        "# Define Loss Functions\n",
        "loss_functions = {\n",
        "    \"mse\": nn.MSELoss(),\n",
        "    \"dice\": DiceLoss(num_class=36),\n",
        "    \"grad\": Grad3D(penalty='l2')\n",
        "}\n",
        "\n",
        "# =======================\n",
        "# 4. Define the Vision Transformer (ViT) MAE Encoder for 3D Data\n",
        "# =======================\n",
        "\n",
        "# Patch Embedding for 3D ViT\n",
        "class PatchEmbed3D(nn.Module):\n",
        "    \"\"\"\n",
        "    3D Patch Embedding Layer for Vision Transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=1, embed_dim=768, patch_size=(16, 16, 16)):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, D, H, W)\n",
        "        x = self.proj(x)  # (B, embed_dim, D', H', W')\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "# 3D Vision Transformer Encoder\n",
        "class ViTMAEEncoder3D(nn.Module):\n",
        "    \"\"\"\n",
        "    3D Vision Transformer Masked Autoencoder Encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=(128, 128, 128), patch_size=(16, 16, 16), in_channels=2, embed_dim=768,\n",
        "                 depth=12, num_heads=12, mlp_ratio=4.0, drop_rate=0.0, attn_drop_rate=0.0):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed3D(in_channels=in_channels, embed_dim=embed_dim, patch_size=patch_size)\n",
        "        num_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1]) * (img_size[2] // patch_size[2])\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
        "                                       dim_feedforward=int(embed_dim * mlp_ratio),\n",
        "                                       dropout=drop_rate, activation='gelu')\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, D, H, W)\n",
        "        x = self.patch_embed(x)  # (B, num_patches, embed_dim)\n",
        "        x = x + self.pos_embed  # Add positional embedding\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)  # (B, num_patches, embed_dim)\n",
        "        return x  # Feature representation\n",
        "\n",
        "# =======================\n",
        "# 5. Define Registration Head and Spatial Transformer\n",
        "# =======================\n",
        "\n",
        "# Registration Head\n",
        "class RegistrationHead(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Registration head for generating displacement fields.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
        "        super().__init__()\n",
        "        conv3d = nn.Conv3d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2\n",
        "        )\n",
        "        # Initialize weights with small random values\n",
        "        nn.init.normal_(conv3d.weight, mean=0.0, std=1e-5)\n",
        "        nn.init.constant_(conv3d.bias, 0)\n",
        "        self.add_module('conv3d', conv3d)\n",
        "\n",
        "# Spatial Transformer\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    N-D Spatial Transformer for applying displacement fields.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, mode='bilinear'):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "\n",
        "        # Create sampling grid\n",
        "        vectors = [torch.arange(0, s) for s in size]\n",
        "        grids = torch.meshgrid(*vectors, indexing='ij')  # For 3D, use 'ij' indexing\n",
        "        grid = torch.stack(grids)\n",
        "        grid = torch.unsqueeze(grid, 0)\n",
        "        grid = grid.type(torch.FloatTensor)\n",
        "\n",
        "        # Register the grid as a buffer\n",
        "        self.register_buffer('grid', grid)\n",
        "\n",
        "    def forward(self, src, flow):\n",
        "        # src: (B, C, D, H, W)\n",
        "        # flow: (B, 3, D, H, W)\n",
        "        new_locs = self.grid + flow\n",
        "        shape = flow.shape[2:]\n",
        "\n",
        "        # Normalize grid values to [-1, 1]\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n",
        "\n",
        "        # Permute to (B, D, H, W, C) as required by grid_sample\n",
        "        new_locs = new_locs.permute(0, 2, 3, 4, 1).contiguous()\n",
        "\n",
        "        return F.grid_sample(src, new_locs, align_corners=False, mode=self.mode)\n",
        "\n",
        "# =======================\n",
        "# 6. Define the HierarchicalViT_Module\n",
        "# =======================\n",
        "\n",
        "class HierarchicalViT_Module(nn.Module):\n",
        "    \"\"\"\n",
        "    Hierarchical Vision Transformer (HViT) with MAE Backbone for Image Registration.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super(HierarchicalViT_Module, self).__init__()\n",
        "        self.config = config  # Initialize the config attribute\n",
        "\n",
        "        # Initialize the 3D ViT MAE encoder\n",
        "        self.encoder = ViTMAEEncoder3D(\n",
        "            img_size=config['data_size'],\n",
        "            patch_size=(16, 16, 16),\n",
        "            in_channels=2,  # source + target\n",
        "            embed_dim=config['fpn_channels'],  # Match FPN channels\n",
        "            depth=config['vit_depth'],\n",
        "            num_heads=config['vit_num_heads'],\n",
        "            mlp_ratio=config['vit_mlp_ratio'],\n",
        "            drop_rate=config['vit_drop_rate'],\n",
        "            attn_drop_rate=config['vit_attn_drop_rate']\n",
        "        )\n",
        "\n",
        "        # Decoder: Simple linear layers to map transformer features to desired output\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(config['fpn_channels'], config['fpn_channels'] * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config['fpn_channels'] * 2, config['fpn_channels']),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Reshape and Upsample to match spatial dimensions\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.ConvTranspose3d(config['fpn_channels'], config['fpn_channels'], kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(config['fpn_channels'], config['fpn_channels'], kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(config['fpn_channels'], config['fpn_channels'], kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(config['fpn_channels'], config['fpn_channels'], kernel_size=2, stride=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # This upsampling sequence doubles the spatial dimensions each step:\n",
        "        # From 8x8x8 -> 16x16x16 -> 32x32x32 -> 64x64x64 -> 128x128x128\n",
        "\n",
        "        # Registration head to generate displacement fields\n",
        "        self.reg_head = RegistrationHead(\n",
        "            in_channels=config['fpn_channels'],\n",
        "            out_channels=config['ndims'],\n",
        "            kernel_size=config['kernel_size']\n",
        "        )\n",
        "\n",
        "        # Spatial Transformer for applying the flow\n",
        "        self.spatial_trans = SpatialTransformer(size=config['data_size'], mode='bilinear')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the HierarchicalViT with MAE backbone.\n",
        "        Args:\n",
        "            x: Concatenated source and target images (B, 2, D, H, W)\n",
        "        Returns:\n",
        "            moved: Source image moved according to the predicted flow\n",
        "            flow: Predicted displacement field\n",
        "        \"\"\"\n",
        "        features = self.encoder(x)  # (B, num_patches, embed_dim)\n",
        "        decoded = self.decoder(features)  # (B, num_patches, embed_dim)\n",
        "\n",
        "        # Reshape decoded features to spatial dimensions\n",
        "        # num_patches = (128/16)^3 = 8^3 = 512\n",
        "        # decoded: (B,512,768) -> (B,768,8,8,8)\n",
        "        decoded_spatial = decoded.permute(0, 2, 1).reshape(-1, self.config['fpn_channels'], 8, 8, 8)\n",
        "\n",
        "        # Upsample to match original spatial dimensions\n",
        "        upsampled = self.upsample(decoded_spatial)  # (B,768,128,128,128)\n",
        "\n",
        "        # Generate displacement field\n",
        "        flow = self.reg_head(upsampled)  # (B,3,128,128,128)\n",
        "\n",
        "        # Apply displacement field to source image\n",
        "        moved = self.spatial_trans(x[:, :1, ...], flow)  # Apply flow to source image\n",
        "\n",
        "        return moved, flow\n",
        "\n",
        "# =======================\n",
        "# 7. Define the OASIS Dataset Class\n",
        "# =======================\n",
        "# Define Configuration Dictionary\n",
        "config = {\n",
        "    'WO_SELF_ATT': False,\n",
        "    '_NUM_CROSS_ATT': -1,\n",
        "    'out_fmaps': ['P4', 'P3', 'P2', 'P1'],  # Number of levels = 4\n",
        "    'scale_level_df': 'P1',\n",
        "    'upsample_df': True,\n",
        "    'upsample_scale_factor': 2,\n",
        "    'fpn_channels': 768,  # Match ViT MAE embed_dim\n",
        "    'start_channels': 32,\n",
        "    'patch_size': [16, 16, 16, 16],  # Matches number of levels\n",
        "    'backbone_net': 'mae_vit',\n",
        "    'in_channels': 1,\n",
        "    'data_size': [128, 128, 128],  # 3D data size\n",
        "    'img_size': [128, 128, 128],   # Align with input_dim\n",
        "    'bias': True,\n",
        "    'norm_type': 'instance',\n",
        "    'kernel_size': 3,\n",
        "    'vit_depth': 12,\n",
        "    'vit_num_heads': 12,\n",
        "    'vit_mlp_ratio': 4.0,\n",
        "    'vit_drop_rate': 0.0,\n",
        "    'vit_attn_drop_rate': 0.0,\n",
        "    'num_organs': 36,  # Updated to match DiceLoss\n",
        "    'ndims': 3,  # Number of spatial dimensions\n",
        "}\n",
        "# =======================\n",
        "# 10. Define the LightningModule\n",
        "# =======================\n",
        "\n",
        "class LiTHViT(LightningModule):\n",
        "    \"\"\"\n",
        "    LightningModule for Hierarchical Vision Transformer with MAE Backbone.\n",
        "    \"\"\"\n",
        "    def __init__(self, args, config, wandb_logger=None, save_model_every_n_epochs=10):\n",
        "        super(LiTHViT, self).__init__()\n",
        "        self.automatic_optimization = False  # Manual optimization\n",
        "        self.args = args\n",
        "        self.config = config\n",
        "        self.best_val_loss = 1e8\n",
        "        self.save_model_every_n_epochs = save_model_every_n_epochs\n",
        "        self.lr = args.lr\n",
        "        self.tgt2src_reg = args.tgt2src_reg\n",
        "        self.hvit_light = args.hvit_light\n",
        "        self.precision = args.precision\n",
        "\n",
        "        # Initialize the HierarchicalViT_Module\n",
        "        self.hvit = HierarchicalViT_Module(config)\n",
        "\n",
        "        # Define loss weights\n",
        "        self.loss_weights = {\n",
        "            \"mse\": self.args.mse_weights,\n",
        "            \"dice\": self.args.dice_weights,\n",
        "            \"grad\": self.args.grad_weights\n",
        "        }\n",
        "\n",
        "        # Initialize loggers\n",
        "        self.wandb_logger = wandb_logger\n",
        "        self.custom_logger = custom_logger\n",
        "        self.test_step_outputs = []\n",
        "\n",
        "    def _forward(self, batch, calc_score: bool = False, tgt2src_reg: bool = False):\n",
        "        \"\"\"\n",
        "        Forward pass to compute losses and optionally scores.\n",
        "        \"\"\"\n",
        "        _loss = {}\n",
        "        _score = 0.\n",
        "\n",
        "        dtype_map = {\n",
        "            'bf16': torch.bfloat16,\n",
        "            'fp32': torch.float32,\n",
        "            'fp16': torch.float16\n",
        "        }\n",
        "        dtype_ = dtype_map.get(self.precision, torch.float32)\n",
        "\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=dtype_):\n",
        "            if tgt2src_reg:\n",
        "                target, source = batch[1].to(dtype=dtype_), batch[0].to(dtype=dtype_)\n",
        "                tgt_seg, src_seg = batch[3], batch[2]\n",
        "            else:\n",
        "                source, target = batch[0].to(dtype=dtype_), batch[1].to(dtype=dtype_)\n",
        "                src_seg, tgt_seg = batch[2], batch[3]\n",
        "\n",
        "            # Concatenate source and target along channel dimension\n",
        "            concatenated_input = torch.cat([source, target], dim=1)  # (B, 2, D, H, W)\n",
        "\n",
        "            moved, flow = self.hvit(concatenated_input)\n",
        "\n",
        "            if calc_score:\n",
        "                moved_seg = self._get_one_hot_from_src(src_seg, flow, self.args.num_labels)\n",
        "                _score = DiceScore(moved_seg, tgt_seg.long(), self.args.num_labels)\n",
        "\n",
        "            _loss = {}\n",
        "            for key, weight in self.loss_weights.items():\n",
        "                if key == \"mse\":\n",
        "                    _loss[key] = weight * loss_functions[key](moved, target)\n",
        "                elif key == \"dice\":\n",
        "                    moved_seg = self._get_one_hot_from_src(src_seg, flow, self.args.num_labels)\n",
        "                    _loss[key] = weight * loss_functions[key](moved_seg, tgt_seg.long())\n",
        "                elif key == \"grad\":\n",
        "                    _loss[key] = weight * loss_functions[key](flow)\n",
        "\n",
        "            _loss[\"avg_loss\"] = sum(_loss.values()) / len(_loss)\n",
        "        return _loss, _score\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Training step.\n",
        "        \"\"\"\n",
        "        opt = self.optimizers()\n",
        "\n",
        "        # Forward pass and compute losses\n",
        "        loss_dict, _ = self._forward(batch, calc_score=False)\n",
        "\n",
        "        # Backward pass\n",
        "        self.manual_backward(loss_dict[\"avg_loss\"])\n",
        "\n",
        "        # Optimizer step\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Logging\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics(loss_dict, step=self.global_step)\n",
        "        self.custom_logger.info(f\"Batch {batch_idx} - Loss: {loss_dict}\")\n",
        "        return loss_dict\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Actions to perform at the end of each training epoch.\n",
        "        \"\"\"\n",
        "        # Save model checkpoint every N epochs\n",
        "        if self.current_epoch % self.save_model_every_n_epochs == 0:\n",
        "            checkpoints_dir = f\"./checkpoints/epoch_{self.current_epoch}\"\n",
        "            os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "            checkpoint_path = f\"{checkpoints_dir}/model_epoch_{self.current_epoch}.ckpt\"\n",
        "            self.trainer.save_checkpoint(checkpoint_path)\n",
        "            self.custom_logger.info(f\"Saved model at epoch {self.current_epoch}\")\n",
        "\n",
        "        # Log learning rate\n",
        "        current_lr = self.optimizers().param_groups[0]['lr']\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"learning_rate\": current_lr}, step=self.global_step)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Validation step.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            _loss, _score = self._forward(batch, calc_score=True)\n",
        "\n",
        "        # Log each component of the validation loss\n",
        "        for loss_name, loss_value in _loss.items():\n",
        "            self.log(f\"val_{loss_name}\", loss_value, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        # Log the mean validation score if available\n",
        "        if _score is not None:\n",
        "            self.log(\"val_score\", _score.mean(), on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        # Log to WandB\n",
        "        if self.wandb_logger:\n",
        "            log_dict = {f\"val_{k}\": v.item() for k, v in _loss.items()}\n",
        "            if _score is not None:\n",
        "                log_dict[\"val_score_mean\"] = _score.mean().item()\n",
        "            self.wandb_logger.log_metrics(log_dict, step=self.global_step)\n",
        "\n",
        "        return {\"val_loss\": _loss[\"avg_loss\"], \"val_score\": _score.mean().item() if _score is not None else 0.0}\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Actions to perform at the end of the validation epoch.\n",
        "        \"\"\"\n",
        "        val_loss = self.trainer.callback_metrics.get(\"val_avg_loss\")\n",
        "        if val_loss is not None and self.current_epoch > 0:\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                checkpoints_dir = f\"./checkpoints/best\"\n",
        "                os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "                best_model_path = f\"{checkpoints_dir}/best_model.ckpt\"\n",
        "                self.trainer.save_checkpoint(best_model_path)\n",
        "                if self.wandb_logger:\n",
        "                    self.wandb_logger.experiment.log({\n",
        "                        \"best_model_saved\": best_model_path,\n",
        "                        \"best_val_loss\": self.best_val_loss.item()\n",
        "                    })\n",
        "                self.custom_logger.info(f\"New best model saved with validation loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Test step.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            _loss, _score = self._forward(batch, calc_score=True)\n",
        "\n",
        "        _score_mean = _score.mean().item() if _score is not None else 0.0\n",
        "        self.test_step_outputs.append(_score_mean)\n",
        "\n",
        "        # Log to WandB\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"test_dice\": _score_mean}, step=self.global_step)\n",
        "\n",
        "        return {\"test_dice\": _score_mean}\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Actions to perform at the end of the test epoch.\n",
        "        \"\"\"\n",
        "        # Calculate the average Dice score across all test steps\n",
        "        avg_test_dice = sum(self.test_step_outputs) / len(self.test_step_outputs) if self.test_step_outputs else 0.0\n",
        "\n",
        "        # Log the average test Dice score\n",
        "        self.log(\"avg_test_dice\", avg_test_dice, prog_bar=True)\n",
        "\n",
        "        # Log to WandB\n",
        "        if self.wandb_logger:\n",
        "            self.wandb_logger.log_metrics({\"total_test_dice_avg\": avg_test_dice})\n",
        "\n",
        "        # Clear the test step outputs list for the next test epoch\n",
        "        self.test_step_outputs.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configures the optimizer and learning rate scheduler for the model.\n",
        "        \"\"\"\n",
        "        optimizer = torch.optim.Adam(self.hvit.parameters(), lr=self.lr, weight_decay=0, amsgrad=True)\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=self.lr_lambda)\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"epoch\",\n",
        "                \"frequency\": 1,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def lr_lambda(self, epoch):\n",
        "        \"\"\"\n",
        "        Defines the learning rate schedule.\n",
        "        \"\"\"\n",
        "        return math.pow(1 - epoch / self.trainer.max_epochs, 0.9)\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_checkpoint(cls, checkpoint_path, args=None, wandb_logger=None):\n",
        "        \"\"\"\n",
        "        Loads a model from a checkpoint file.\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "        args = args or checkpoint.get('hyper_parameters', {}).get('args')\n",
        "        config = checkpoint.get('hyper_parameters', {}).get('config')\n",
        "\n",
        "        model = cls(args, config, wandb_logger)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "        if 'hyper_parameters' in checkpoint:\n",
        "            hyper_params = checkpoint['hyper_parameters']\n",
        "            for attr in ['lr', 'best_val_loss', 'last_epoch']:\n",
        "                setattr(model, attr, hyper_params.get(attr, getattr(model, attr)))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        \"\"\"\n",
        "        Callback to save additional information in the checkpoint.\n",
        "        \"\"\"\n",
        "        checkpoint['hyper_parameters'] = {\n",
        "            'config': self.config,\n",
        "            'lr': self.lr,\n",
        "            'best_val_loss': self.best_val_loss,\n",
        "            'last_epoch': self.current_epoch\n",
        "        }\n",
        "    def get_one_hot(inp_seg, num_labels):\n",
        "        \"\"\"\n",
        "        Converts segmentation labels to one-hot encoding.\n",
        "        Args:\n",
        "            inp_seg: Input segmentation tensor of shape (B, D, H, W)\n",
        "            num_labels: Number of segmentation classes\n",
        "        Returns:\n",
        "            One-hot encoded tensor of shape (B, num_labels, D, H, W)\n",
        "        \"\"\"\n",
        "        # Debugging: Print input shape\n",
        "        print(f\"Input segmentation shape: {inp_seg.shape}\")\n",
        "\n",
        "        # Apply one-hot encoding\n",
        "        inp_onehot = nn.functional.one_hot(inp_seg.long(), num_classes=num_labels)  # Expected output shape: (B, D, H, W, num_labels)\n",
        "        print(f\"One-hot encoded shape before permute: {inp_onehot.shape}\")\n",
        "\n",
        "        # Check if the shape matches expectations\n",
        "        if inp_onehot.dim() == 5:  # (B, D, H, W, num_labels)\n",
        "            inp_onehot = inp_onehot.permute(0, 4, 1, 2, 3).contiguous()  # Reorder to (B, num_labels, D, H, W)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected one-hot encoded shape: {inp_onehot.shape}\")\n",
        "\n",
        "        print(f\"One-hot encoded shape after permute: {inp_onehot.shape}\")\n",
        "        return inp_onehot\n",
        "\n",
        "    def _get_one_hot_from_src(self, src_seg, flow, num_labels):\n",
        "        \"\"\"\n",
        "        Converts source segmentation to one-hot encoding and applies deformation.\n",
        "        Args:\n",
        "            src_seg: Source segmentation tensor\n",
        "            flow: Deformation field\n",
        "            num_labels: Number of segmentation classes\n",
        "        Returns:\n",
        "            Deformed one-hot encoded segmentation\n",
        "        \"\"\"\n",
        "        src_seg_onehot = get_one_hot(src_seg, num_labels)  # (B, num_labels, D, H, W)\n",
        "        deformed_segs = []\n",
        "\n",
        "        for i in range(num_labels):\n",
        "            # Extract one class at a time and apply deformation\n",
        "            class_seg = src_seg_onehot[:, i:i+1, ...]  # Keep the channel dimension\n",
        "            deformed_class = self.hvit.spatial_trans(class_seg.float(), flow.float())\n",
        "            deformed_segs.append(deformed_class)\n",
        "\n",
        "        # Concatenate along the channel (class) dimension\n",
        "        return torch.cat(deformed_segs, dim=1)  # (B, num_labels, D, H, W)\n",
        "\n",
        "# =======================\n",
        "# 11. Initialize the LightningModule and Trainer\n",
        "# =======================\n",
        "\n",
        "# Initialize the LightningModule\n",
        "lit_model = LiTHViT(args, config, wandb_logger=wandb_logger)\n",
        "\n",
        "# Define the PyTorch Lightning Trainer\n",
        "trainer = Trainer(\n",
        "    max_epochs=50,  # Number of epochs\n",
        "    logger=wandb_logger,  # Log training metrics\n",
        "    enable_checkpointing=False,  # Disable automatic checkpointing\n",
        "    devices=1,  # Number of GPUs (set to 0 for CPU)\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",  # Use GPU if available\n",
        "    precision=16 if args.precision == 'fp16' else 32  # Set precision\n",
        ")\n",
        "\n",
        "# =======================\n",
        "# 12. Start Training and Testing\n",
        "# =======================\n",
        "\n",
        "# Start Training\n",
        "trainer.fit(lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
        "\n",
        "# Start Testing (Optional)\n",
        "trainer.test(lit_model, dataloaders=test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2d5a3872f398447ba64e7dedca154112",
            "1d51ea8d67f4424ab20e9a1525eaf775",
            "55bf9d4284a24b80bec444438bae69d9",
            "56e051e8c2f34c53bee49bb36e2192e4",
            "7fc44c7a72fc4710804b7e59acb1d3f1",
            "1f5aa81db1d94f969135c88d87ee7851",
            "37b7a73bd06841769ea2b42becb5ff6e",
            "41e8f061ce39402a877543b1ea20f48b",
            "2106b59e205b4560abf4e893083ac111",
            "ba81859fe2d54f719e53128d11994230",
            "c4f25274e0e345f4af74152e583b610c"
          ]
        },
        "id": "oHB_l-3C_Z9h",
        "outputId": "e8045078-03ec-4a92-95e9-a2805a0794ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20250110_221951-6apgn6fb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_mae_integration/runs/6apgn6fb' target=\"_blank\">exalted-puddle-9</a></strong> to <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_mae_integration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_mae_integration' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/hvit_mae_integration</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alim98barnet-university-of-tehran/hvit_mae_integration/runs/6apgn6fb' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/hvit_mae_integration/runs/6apgn6fb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name | Type                   | Params | Mode \n",
            "--------------------------------------------------------\n",
            "0 | hvit | HierarchicalViT_Module | 113 M  | train\n",
            "--------------------------------------------------------\n",
            "113 M     Trainable params\n",
            "0         Non-trainable params\n",
            "113 M     Total params\n",
            "452.177   Total estimated model params size (MB)\n",
            "144       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name | Type                   | Params | Mode \n",
            "--------------------------------------------------------\n",
            "0 | hvit | HierarchicalViT_Module | 113 M  | train\n",
            "--------------------------------------------------------\n",
            "113 M     Trainable params\n",
            "0         Non-trainable params\n",
            "113 M     Total params\n",
            "452.177   Total estimated model params size (MB)\n",
            "144       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d5a3872f398447ba64e7dedca154112"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 14.75 GiB of which 2.31 GiB is free. Process 182254 has 12.44 GiB memory in use. Of the allocated memory 12.25 GiB is allocated by PyTorch, and 66.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2a96466fa350>\u001b[0m in \u001b[0;36m<cell line: 714>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;31m# Start Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;31m# Start Testing (Optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m             \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         )\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-2a96466fa350>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \"\"\"\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0m_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalc_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;31m# Log each component of the validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-2a96466fa350>\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, batch, calc_score, tgt2src_reg)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mconcatenated_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, 2, D, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0mmoved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhvit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcalc_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-2a96466fa350>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;31m# Upsample to match original spatial dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mupsampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_spatial\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B,768,128,128,128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;31m# Generate displacement field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         )\n\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         return F.conv_transpose3d(\n\u001b[0m\u001b[1;32m   1348\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/data/meta_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;31m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# if \"out\" in kwargs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_default_nowrap_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 14.75 GiB of which 2.31 GiB is free. Process 182254 has 12.44 GiB memory in use. Of the allocated memory 12.25 GiB is allocated by PyTorch, and 66.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "# Initialize the model\n",
        "hvit_module = HierarchicalViT_Module(config)\n",
        "\n",
        "# Print summary\n",
        "summary(hvit_module, input_size=(1, 2, 128, 128, 128), col_names=[\"input_size\", \"output_size\", \"num_params\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "ApQ3EDNPPgME",
        "outputId": "9ef699ce-8fae-4407-ad59-f2acbc83a798"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ViTMAEEncoder3D: 1, PatchEmbed3D: 2, Conv3d: 3, LayerNorm: 3, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, LayerNorm: 2, Sequential: 1, Linear: 2, ReLU: 2, Linear: 2, ReLU: 2, ConvTranspose3d: 2, ReLU: 2, ConvTranspose3d: 2, ReLU: 2, ConvTranspose3d: 2, ReLU: 2]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-2a96466fa350>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;31m# Upsample to match original spatial dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mupsampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_spatial\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B,768,128,128,128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         return F.conv_transpose3d(\n\u001b[0m\u001b[1;32m   1348\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1.54 GiB is free. Process 182254 has 13.21 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 65.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-50d4476bf3f3>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Print summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhvit_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0;31m     summary_list = forward_pass(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mexecuted_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;34mf\"Executed layers up to: {executed_layers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ViTMAEEncoder3D: 1, PatchEmbed3D: 2, Conv3d: 3, LayerNorm: 3, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4, Dropout: 4, Linear: 4, Dropout: 4, LayerNorm: 4, TransformerEncoderLayer: 3, MultiheadAttention: 4, Dropout: 4, LayerNorm: 4, Linear: 4,..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Initialize the model and TensorBoard writer\n",
        "hvit_module = HierarchicalViT_Module(config)\n",
        "writer = SummaryWriter(\"runs/HierarchicalViT\")\n",
        "\n",
        "# Add the model to TensorBoard\n",
        "dummy_input = torch.randn(1, 2, 128, 128, 128)  # Batch size 1, 2 channels (source+target)\n",
        "writer.add_graph(hvit_module, dummy_input)\n",
        "writer.close()\n",
        "\n",
        "# Launch TensorBoard\n",
        "# In terminal:\n",
        "# tensorboard --logdir=runs/HierarchicalViT\n"
      ],
      "metadata": {
        "id": "QpudWi2xNvxk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6Oh0LiLSCxQ",
        "outputId": "74c1f813-096c-455d-9dcc-81d9460f7ee5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.5.1+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "# Initialize the model\n",
        "hvit_module = HierarchicalViT_Module(config)\n",
        "\n",
        "# Generate dummy input\n",
        "dummy_input = torch.randn(1, 2, 128, 128, 128)  # Batch size 1, 2 channels (source+target)\n",
        "\n",
        "# Perform a forward pass\n",
        "moved, flow = hvit_module(dummy_input)\n",
        "\n",
        "# Visualize the model\n",
        "dot = make_dot((moved, flow), params=dict(hvit_module.named_parameters()))\n",
        "dot.render(\"HierarchicalViT_Model\", format=\"pdf\")  # Saves to PDF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r82zgq1LMC2s",
        "outputId": "de5e450b-7d03-4857-fb96-88098699c03f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HierarchicalViT_Model.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/OASIS_dataset.json /content/OASIS"
      ],
      "metadata": {
        "id": "3F6Bp22sJnfF"
      },
      "execution_count": 4,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gKW-tdiFr7r-"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2cb50298ddeb4710829e023d04b50ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29c70c47e19b4bfea1bdf943f87cb7c3",
              "IPY_MODEL_68e5ef4e452b4ccea5eecc9faa5d2dea",
              "IPY_MODEL_0ceeccb5a742467a97e68573aeef2920"
            ],
            "layout": "IPY_MODEL_2b4efe4b0d1d4e4c95de7b5bf6867177"
          }
        },
        "29c70c47e19b4bfea1bdf943f87cb7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c20936cd9574d598c29a0846a370881",
            "placeholder": "​",
            "style": "IPY_MODEL_6b2610c39ecd41a7ba25ebe9b42816d6",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "68e5ef4e452b4ccea5eecc9faa5d2dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7a09b287180453984154055e3f07bba",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24b67787b1c7409f8a000600ed2569a7",
            "value": 2
          }
        },
        "0ceeccb5a742467a97e68573aeef2920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21d9bdfdd3194b2b83684081afadd8d1",
            "placeholder": "​",
            "style": "IPY_MODEL_40ce786947e4477b9bb806e10cb7378e",
            "value": " 2/2 [00:03&lt;00:00,  0.63it/s]"
          }
        },
        "2b4efe4b0d1d4e4c95de7b5bf6867177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "8c20936cd9574d598c29a0846a370881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2610c39ecd41a7ba25ebe9b42816d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7a09b287180453984154055e3f07bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b67787b1c7409f8a000600ed2569a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21d9bdfdd3194b2b83684081afadd8d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40ce786947e4477b9bb806e10cb7378e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d19bc550b60450abb51084eb372fda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3d921539a6f4c9f8afe53c8580b32b7",
              "IPY_MODEL_c205ff7890f346079fbc8d70c0af07c8",
              "IPY_MODEL_dd9b6de80b6349e782ef345456001b61"
            ],
            "layout": "IPY_MODEL_3a0c2fd0d9784bcd8dd669e4e2f15727"
          }
        },
        "a3d921539a6f4c9f8afe53c8580b32b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ab87cf7b9ce4e44b7444bebbd901d73",
            "placeholder": "​",
            "style": "IPY_MODEL_0e21705b16734601a0e8f4be95b4f556",
            "value": "Epoch 0:   0%"
          }
        },
        "c205ff7890f346079fbc8d70c0af07c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b53b7d5516ee4842a1405455e5865e39",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20a7d6d275574306af18f6a3c1f71a0b",
            "value": 0
          }
        },
        "dd9b6de80b6349e782ef345456001b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf9094e614f7426dbec4e419ceb40a32",
            "placeholder": "​",
            "style": "IPY_MODEL_53207ea3465042c6a96b82d4d6dca0ef",
            "value": " 0/414 [00:00&lt;?, ?it/s]"
          }
        },
        "3a0c2fd0d9784bcd8dd669e4e2f15727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "7ab87cf7b9ce4e44b7444bebbd901d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e21705b16734601a0e8f4be95b4f556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b53b7d5516ee4842a1405455e5865e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20a7d6d275574306af18f6a3c1f71a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf9094e614f7426dbec4e419ceb40a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53207ea3465042c6a96b82d4d6dca0ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d5a3872f398447ba64e7dedca154112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d51ea8d67f4424ab20e9a1525eaf775",
              "IPY_MODEL_55bf9d4284a24b80bec444438bae69d9",
              "IPY_MODEL_56e051e8c2f34c53bee49bb36e2192e4"
            ],
            "layout": "IPY_MODEL_7fc44c7a72fc4710804b7e59acb1d3f1"
          }
        },
        "1d51ea8d67f4424ab20e9a1525eaf775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f5aa81db1d94f969135c88d87ee7851",
            "placeholder": "​",
            "style": "IPY_MODEL_37b7a73bd06841769ea2b42becb5ff6e",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "55bf9d4284a24b80bec444438bae69d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41e8f061ce39402a877543b1ea20f48b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2106b59e205b4560abf4e893083ac111",
            "value": 0
          }
        },
        "56e051e8c2f34c53bee49bb36e2192e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba81859fe2d54f719e53128d11994230",
            "placeholder": "​",
            "style": "IPY_MODEL_c4f25274e0e345f4af74152e583b610c",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "7fc44c7a72fc4710804b7e59acb1d3f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "1f5aa81db1d94f969135c88d87ee7851": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37b7a73bd06841769ea2b42becb5ff6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41e8f061ce39402a877543b1ea20f48b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2106b59e205b4560abf4e893083ac111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba81859fe2d54f719e53128d11994230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4f25274e0e345f4af74152e583b610c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}